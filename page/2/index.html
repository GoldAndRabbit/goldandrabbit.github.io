<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Gold and Rabbit&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="having fun coding">
<meta property="og:type" content="website">
<meta property="og:title" content="Gold and Rabbit&#39;s Blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Gold and Rabbit&#39;s Blog">
<meta property="og:description" content="having fun coding">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Gold and Rabbit">
<meta property="article:tag" content="Recommending System">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Gold and Rabbit&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Gold and Rabbit&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Git Tutorial" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/05/Git%20Tutorial/" class="article-date">
  <time datetime="2020-04-05T14:56:00.000Z" itemprop="datePublished">2020-04-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Programming-Data/">Programming & Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/05/Git%20Tutorial/">Git Tutorial</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;你自己的github对应的邮箱地址&quot;</span><br></pre></td></tr></table></figure>
<p>一路回车<br>（通过ls查看～/.ssh下面的所有内容查看）</p>
<p>将刚刚创建的ssh keys添加到github中</p>
<ol>
<li>利用gedit/cat命令，查看id_rsa.pub的内容</li>
<li>在GitHub中，依次点击Settings -&gt; SSH Keys -&gt; Add SSH Key，将id_rsa.pub文件中的字符串复制进去，注意字符串中没有换行和空格。</li>
</ol>
<p>再次检查SSH连接情况(在～/.ssh目录下):<br>输入如下命令</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure>

<p>看到如下所示，则表示添加成功</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hi 你的用户名! You’ve successfully authenticated, but GitHub does <span class="keyword">not</span> provide shell access.</span><br></pre></td></tr></table></figure>

<p>即利用自己的用户名和email地址配置git</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --<span class="keyword">global</span> user.name <span class="string">&quot;你的github用户名&quot;</span></span><br><span class="line">git config --<span class="keyword">global</span> user.email <span class="string">&quot;你的github邮箱地址&quot;</span></span><br></pre></td></tr></table></figure>

<p>ps: 为什么配置好了提交代码到远程分之还是要输入密码? clone的时候选择http</p>
<h2 id="clone-pull-push"><a href="#clone-pull-push" class="headerlink" title="clone, pull, push"></a>clone, pull, push</h2><ul>
<li><p>clone: 克隆远程分支到本地</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone -b my_branch &lt;远程仓库地址&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>pull: 将远程代码pull[拉取]到本地 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin branch_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>push: 将本地代码push[推送]到远程</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add yourfilename</span><br><span class="line">git ci -m <span class="string">&quot;说明修改内容: 解决xx问题/增加xx逻辑/修复xxBug&quot;</span></span><br><span class="line">git push origin branch_name</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="merge-v-s-rebase"><a href="#merge-v-s-rebase" class="headerlink" title="merge v.s. rebase"></a>merge v.s. rebase</h2><ul>
<li>将分支my_branch合并到master分支<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">git merge my_branch</span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure>

</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/04/05/Git%20Tutorial/" data-id="ckjh3lsez000d19dlhkme5nge" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Faiss" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/04/Faiss/" class="article-date">
  <time datetime="2020-04-04T12:11:00.000Z" itemprop="datePublished">2020-04-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Programming-Data/">Programming & Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/04/Faiss/">Faiss</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul>
<li>A library for efficient similarity search and clustering of dense vectors.Faiss：高效稠密向量相似Top检索库;</li>
<li>Faiss contains several methods for similarity search. It assumes that the instances are represented as vectors and are identified by an integer, and that the vectors can be compared with L2 (Euclidean) distances or dot products. Vectors that are similar to a query vector are those that have the lowest L2 distance or the highest dot product with the query vector. It also supports cosine similarity, since this is a dot product on normalized vectors. Faiss支持多种相似度检索度量：L2距离，点积和余弦相似度。</li>
</ul>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 更新conda</span></span><br><span class="line">conda update conda</span><br><span class="line"><span class="meta">#</span><span class="bash"> 先安装mkl</span></span><br><span class="line">conda install mkl</span><br><span class="line"><span class="meta">#</span><span class="bash"> gpu版本 -- 记得根据自己安装的cuda版本安装对应的faiss版本，不然会出异常</span></span><br><span class="line">conda install faiss-gpu cudatoolkit=10.0 -c pytorch # For CUDA10</span><br></pre></td></tr></table></figure>

<h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line">d = <span class="number">64</span>                           <span class="comment"># dimension</span></span><br><span class="line">nb = <span class="number">100000</span>                      <span class="comment"># database size</span></span><br><span class="line">nq = <span class="number">10000</span>                       <span class="comment"># nb of queries</span></span><br><span class="line">np.random.seed(<span class="number">1234</span>)             <span class="comment"># make reproducible</span></span><br><span class="line">xb = np.random.random((nb, d)).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">xq = np.random.random((nq, d)).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">xb[:, <span class="number">0</span>] += np.arange(nb) / <span class="number">1000.</span></span><br><span class="line">xq[:, <span class="number">0</span>] += np.arange(nq) / <span class="number">1000.</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Faiss is built around the Index object.</span></span><br><span class="line"><span class="string">It encapsulates the set of database vectors, and optionally preprocesses them to make searching efficient.</span></span><br><span class="line"><span class="string">There are many types of indexes,</span></span><br><span class="line"><span class="string">we are going to use the simplest version that just performs brute-force L2 distance search on them: IndexFlatL2.</span></span><br><span class="line"><span class="string">All indexes need to know when they are built which is the dimensionality of the vectors they operate on, d in our case.</span></span><br><span class="line"><span class="string">Then, most of the indexes also require a training phase, to analyze the distribution of the vectors. For IndexFlatL2, we can skip this operation.</span></span><br><span class="line"><span class="string">When the index is built and trained, two operations can be performed on the index: add and search.</span></span><br><span class="line"><span class="string">To add elements to the index, we call add on xb.</span></span><br><span class="line"><span class="string">We can also display the two state variables of the index: is_trained, a boolean that indicates whether training is required and ntotal, the number of indexed vectors.</span></span><br><span class="line"><span class="string">Some indexes can also store integer IDs corresponding to each of the vectors (but not IndexFlatL2).</span></span><br><span class="line"><span class="string">If no IDs are provided, add just uses the vector ordinal as the id, ie. the first vector gets 0, the second 1, etc.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">index = faiss.IndexFlatL2(d)     <span class="comment"># build the index</span></span><br><span class="line">print(index.is_trained)          <span class="comment"># log: True</span></span><br><span class="line">index.add(xb)                    <span class="comment"># add vectors to the index</span></span><br><span class="line">print(index.ntotal)              <span class="comment"># log: 100000</span></span><br><span class="line">​</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">The basic search operation that can be performed on an index is the k-nearest-neighbor search,</span></span><br><span class="line"><span class="string">ie. for each query vector, find its k nearest neighbors in the database.</span></span><br><span class="line"><span class="string">The result of this operation can be conveniently stored in an integer matrix of size nq-by-k,</span></span><br><span class="line"><span class="string">where row i contains the IDs of the neighbors of query vector i, sorted by increasing distance.</span></span><br><span class="line"><span class="string">In addition to this matrix, the search operation returns a nq-by-k floating-point matrix with the corresponding squared distances.</span></span><br><span class="line"><span class="string">As a sanity check, we can first search a few database vectors, to make sure the nearest neighbor is indeed the vector itself.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">k = <span class="number">4</span>                          <span class="comment"># we want to see 4 nearest neighbors</span></span><br><span class="line">D, I = index.search(xb[:<span class="number">5</span>], k) <span class="comment"># sanity check</span></span><br><span class="line">print(I)</span><br><span class="line">print(D)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[  0 393 363  78]</span></span><br><span class="line"><span class="string"> [  1 555 277 364]</span></span><br><span class="line"><span class="string"> [  2 304 101  13]</span></span><br><span class="line"><span class="string"> [  3 173  18 182]</span></span><br><span class="line"><span class="string"> [  4 288 370 531]]</span></span><br><span class="line"><span class="string">[[0.        7.1751738 7.20763   7.2511625]</span></span><br><span class="line"><span class="string"> [0.        6.3235645 6.684581  6.799946 ]</span></span><br><span class="line"><span class="string"> [0.        5.7964087 6.391736  7.2815123]</span></span><br><span class="line"><span class="string"> [0.        7.2779055 7.527987  7.6628466]</span></span><br><span class="line"><span class="string"> [0.        6.7638035 7.2951202 7.3688145]]</span></span><br><span class="line"><span class="string">the nearest neighbor of each query is indeed the index of the vector,</span></span><br><span class="line"><span class="string">and the corresponding distance is 0. And within a row, distances are increasing.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">D, I = index.search(xq, k)     <span class="comment"># actual search</span></span><br><span class="line">print(I[:<span class="number">5</span>])                   <span class="comment"># neighbors of the 5 first queries</span></span><br><span class="line">print(I[<span class="number">-5</span>:])                  <span class="comment"># neighbors of the 5 last queries</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[ 381  207  210  477]</span></span><br><span class="line"><span class="string"> [ 526  911  142   72]</span></span><br><span class="line"><span class="string"> [ 838  527 1290  425]</span></span><br><span class="line"><span class="string"> [ 196  184  164  359]</span></span><br><span class="line"><span class="string"> [ 526  377  120  425]]</span></span><br><span class="line"><span class="string">[[ 9900 10500  9309  9831]</span></span><br><span class="line"><span class="string"> [11055 10895 10812 11321]</span></span><br><span class="line"><span class="string"> [11353 11103 10164  9787]</span></span><br><span class="line"><span class="string"> [10571 10664 10632  9638]</span></span><br><span class="line"><span class="string"> [ 9628  9554 10036  9582]]</span></span><br><span class="line"><span class="string">Because of the value added to the first component of the vectors,</span></span><br><span class="line"><span class="string">the dataset is smeared along the first axis in d-dim space. </span></span><br><span class="line"><span class="string">So the neighbors of the first few vectors are around the beginning of the dataset,</span></span><br><span class="line"><span class="string">and the ones of the vectors around ~10000 are also around index 10000 in the dataset.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">To speed up the search, it is possible to segment the dataset into pieces.</span></span><br><span class="line"><span class="string">We define Voronoi cells in the d-dimensional space, and each database vector falls in one of the cells.</span></span><br><span class="line"><span class="string">At search time, only the database vectors y contained in the cell the query x falls in and a few neighboring ones are compared against the query vector.</span></span><br><span class="line"><span class="string">This is done via the IndexIVFFlat index.</span></span><br><span class="line"><span class="string">This type of index requires a training stage, that can be performed on any collection of vectors that has the same distribution as the database vectors.</span></span><br><span class="line"><span class="string">In this case we just use the database vectors themselves.</span></span><br><span class="line"><span class="string">The IndexIVFFlat also requires another index, the quantizer, that assigns vectors to Voronoi cells.</span></span><br><span class="line"><span class="string">Each cell is defined by a centroid, and finding the Voronoi cell a vector falls in consists in finding the nearest neighbor of the vector in the set of centroids.</span></span><br><span class="line"><span class="string">This is the task of the other index, which is typically an IndexFlatL2.</span></span><br><span class="line"><span class="string">There are two parameters to the search method: nlist, the number of cells, and nprobe, the number of cells (out of nlist) that are visited to perform a search.</span></span><br><span class="line"><span class="string">The search time roughly increases linearly with the number of probes plus some constant due to the quantization.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">nlist = <span class="number">100</span>                       <span class="comment"># 聚类中心个数</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)  <span class="comment"># the other index</span></span><br><span class="line">index = faiss.IndexIVFFlat(quantizer, d, nlist)</span><br><span class="line"><span class="keyword">assert</span> <span class="keyword">not</span> index.is_trained</span><br><span class="line">index.train(xb)</span><br><span class="line"><span class="keyword">assert</span> index.is_trained</span><br><span class="line"></span><br><span class="line">index.add(xb)                  <span class="comment"># add may be a bit slower as well</span></span><br><span class="line">D, I = index.search(xq, k)     <span class="comment"># actual search</span></span><br><span class="line">print(I[<span class="number">-5</span>:])                  <span class="comment"># neighbors of the 5 last queries</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[ 9900  9309  9810 10048]</span></span><br><span class="line"><span class="string"> [11055 10895 10812 11321]</span></span><br><span class="line"><span class="string"> [11353 10164  9787 10719]</span></span><br><span class="line"><span class="string"> [10571 10664 10632 10203]</span></span><br><span class="line"><span class="string"> [ 9628  9554  9582 10304]]</span></span><br><span class="line"><span class="string">The values are similar, but not exactly the same as for the brute-force search (see above).</span></span><br><span class="line"><span class="string">This is because some of the results were not in the exact same Voronoi cell.</span></span><br><span class="line"><span class="string">Therefore, visiting a few more cells may prove useful. &#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">index.nprobe = <span class="number">10</span>              <span class="comment"># default nprobe is 1, try a few more 默认聚类中心个数1, 尝试调大中心数10</span></span><br><span class="line">D, I = index.search(xq, k)</span><br><span class="line">print(I[<span class="number">-5</span>:])                  <span class="comment"># neighbors of the 5 last queries</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[ 9900 10500  9309  9831]</span></span><br><span class="line"><span class="string"> [11055 10895 10812 11321]</span></span><br><span class="line"><span class="string"> [11353 11103 10164  9787]</span></span><br><span class="line"><span class="string"> [10571 10664 10632  9638]</span></span><br><span class="line"><span class="string"> [ 9628  9554 10036  9582]]</span></span><br><span class="line"><span class="string">which is the correct result.</span></span><br><span class="line"><span class="string">Note that getting a perfect result in this case is merely an artifact of the data distribution,</span></span><br><span class="line"><span class="string">as it is has a strong component on the x-axis which makes it easier to handle.</span></span><br><span class="line"><span class="string">The nprobe parameter is always a way of adjusting the tradeoff between speed and accuracy of the result.</span></span><br><span class="line"><span class="string">Setting nprobe = nlist gives the same result as the brute-force search (but slower).</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ngpus = faiss.get_num_gpus()</span><br><span class="line">print(<span class="string">&#x27;number of GPUS:&#x27;</span>, ngpus)</span><br><span class="line">res = faiss.StandardGpuResources()</span><br><span class="line"><span class="comment"># build a flat (CPU) index</span></span><br><span class="line">index_flat = faiss.IndexFlatL2(d)</span><br><span class="line"><span class="comment"># make it into a gpu index</span></span><br><span class="line">gpu_index_flat = faiss.index_cpu_to_gpu(res, <span class="number">0</span>, index_flat)</span><br><span class="line">gpu_index_flat.add(xb)</span><br><span class="line">print(gpu_index_flat.ntotal)</span><br><span class="line">D, I = gpu_index_flat.search(xq, k)     <span class="comment"># actual search</span></span><br><span class="line">print(I[:<span class="number">5</span>])                            <span class="comment"># neighbors of the 5 first queries</span></span><br><span class="line">print(I[<span class="number">-5</span>:])                           <span class="comment"># neighbors of the 5 last queries</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># add self defined index</span></span><br><span class="line">index = faiss.IndexFlatL2(xb.shape[<span class="number">1</span>])</span><br><span class="line">ids = np.arange(xb.shape[<span class="number">0</span>]) + <span class="number">231</span></span><br><span class="line">index2 = faiss.IndexIDMap(index)</span><br><span class="line">index2.add_with_ids(xb, ids)</span><br><span class="line">k = <span class="number">4</span>                                   <span class="comment"># we want to see 4 nearest neighbors</span></span><br><span class="line">D, I = index2.search(xq, k)             <span class="comment"># actual search</span></span><br><span class="line">print(I[:<span class="number">5</span>])                            <span class="comment"># neighbors of the 5 first queries</span></span><br><span class="line">print(I[<span class="number">-5</span>:])                           <span class="comment"># neighbors of the 5 last queries</span></span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/04/04/Faiss/" data-id="ckjh3lsey000c19dlb07dd0qa" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-The Economics of Money, Banking, and Financial Markets" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/04/The%20Economics%20of%20Money,%20Banking,%20and%20Financial%20Markets/" class="article-date">
  <time datetime="2020-04-04T12:11:00.000Z" itemprop="datePublished">2020-04-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Reading/">Reading</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/04/The%20Economics%20of%20Money,%20Banking,%20and%20Financial%20Markets/">The Economics of Money, Banking, and Financial Markets</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="理解利率"><a href="#理解利率" class="headerlink" title="理解利率"></a>理解利率</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/04/04/The%20Economics%20of%20Money,%20Banking,%20and%20Financial%20Markets/" data-id="ckjh3lsf6000t19dl1pfh9uvv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-狼人杀如何致胜" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/22/%E7%8B%BC%E4%BA%BA%E6%9D%80%E5%A6%82%E4%BD%95%E8%87%B4%E8%83%9C/" class="article-date">
  <time datetime="2020-03-21T18:12:57.000Z" itemprop="datePublished">2020-03-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Always-Review/">Always Review</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/22/%E7%8B%BC%E4%BA%BA%E6%9D%80%E5%A6%82%E4%BD%95%E8%87%B4%E8%83%9C/">狼人杀如何致胜</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul>
<li>到底是一个重点是靠盘逻辑的游戏？还是一个缺少逻辑、更多的是表演/发言/说谎的游戏？逻辑在获胜上占到多少比重？</li>
<li>表演是这个游戏里面重要的部分。如何正确的从自己的角色表演，达到获胜的目的。</li>
<li>作为游戏，均存在玩物丧志属性，总结游戏玩法也是符合让这个游戏对于我的边际效益最小化的目的，鼓励投入到新领域中去。</li>
<li>本文重点是9人国标局。3民3狼1预1巫1猎。12人局存在玩家质量参差不齐的局面。</li>
</ul>
<h2 id="获胜之道"><a href="#获胜之道" class="headerlink" title="获胜之道"></a>获胜之道</h2><ul>
<li>总结一下玩到现在如何获得胜利的方法，理论不一定严谨，整体目的是避免一些错误的发言和节奏，梳理一些关键性的要点，在整体上最大化胜率。重点解析预言家和狼人这两个角色。</li>
</ul>
<h5 id="预言家"><a href="#预言家" class="headerlink" title="预言家"></a>预言家</h5><ul>
<li>预言家是9人局胜率最低的角色，这是由于规则决定的（大概率双神起跳）。</li>
<li>作为预言家必须非常坦然接受你死的很快或者你死的很冤的事实。</li>
<li>发言的时候必须快速清晰的报出来查验，以免还没有说出来任何查验只透露自己身份然后被狼人自爆。预言家在遗言的时候。一定要多说。即使是非常冤死。但是真预言家的求生欲一定不能过低。</li>
</ul>
<h5 id="狼人"><a href="#狼人" class="headerlink" title="狼人"></a>狼人</h5><ul>
<li>狼人的胜利关键在于“顺势而为”，三只狼人需要各自独立地根据场上的形势因时制宜的选择悍跳/划水/引导/站边/跟票/冲票。</li>
<li>同时三狼要有一定程度的分工，整体态势太趋同（都怂或者都悍跳）风险的。</li>
</ul>
<h2 id="案例复盘"><a href="#案例复盘" class="headerlink" title="案例复盘"></a>案例复盘</h2><h4 id="失败的一局复盘"><a href="#失败的一局复盘" class="headerlink" title="失败的一局复盘"></a>失败的一局复盘</h4><div style="text-align: center;">
<img alt="" src="https://i.loli.net/2020/09/14/QYTr83RcIsd4qWh.jpg" style="display: inline-block;" width="200"/>
</div>


<ul>
<li>图上这局是我玩的比较失败的一局。结果：作为好人阵营（猎人）我们输的非常彻底，被一只悍跳狼完全带了节奏，狼人预言家身份坐实，民整体没有视角做得出的选择也都是错误的，神职女巫和猎人完全齐齐的走向了错误的做法。</li>
</ul>
<p>这局的特殊在于 </p>
<ol>
<li>一局下来好人之间几乎没有形成提供有效的通讯（统计来看9人国标局好人输概率大于50%，但是好人们几乎完全没有相互之间的信息认可，这种是绝对不算多的案例）。</li>
<li>悍跳狼第一天就被投死，其余怂狼从头到尾也没有也没有太多行动。但是好人掉进圈套里完全出不来，后续决策是一连串错误，而且是走向那种温水煮青蛙的错误节奏。</li>
<li>整局游戏来看，好人绝对不是一定输，是肯定机会逆转的，但是整体上我认为好人如果想赢非常艰难，故做一下复盘。</li>
</ol>
<h4 id="重点历程"><a href="#重点历程" class="headerlink" title="重点历程"></a>重点历程</h4><ol start="0">
<li><p>狼人刀3号女巫，3女巫自救。9预言家8金水。</p>
</li>
<li><p>第一天9号第一个发言，说自己预言家给8号的民发了金水，发言过程中没有明显语气或者逻辑上的问题。然后说由于7、8、9位置学，轻踩一下7号。接着顺次12345发言均没有有效信息，其中女巫跳出来身份说明是自救的。<br>8号是归票位，给8发出去金水，然后8又没有身份，我当时认为信服力不一定。<br>但是8不是神职，如果9是狼人发到8民身上，可能性也是存在的，发言上没有明显问题，当时我没有完全不信或者不信。</p>
</li>
<li><p>然后6号发言，给7号发了查杀。<br>从位置上来看，9预言家8号金水则7号位置有些尴尬。<br>6给7查杀，当时我是不太信的，然后7号是民，但7的角度而言，6和9对7都有一定攻击性。但6是查杀，他从自身身份出发只能从发言打死6。</p>
</li>
<li><p>第一天投票：我上票给了7号，6整体发言语气和逻辑都还好（6查验的是后位置的7有合理性，感觉9的力度不太好说，其实我这里操作有一定的赌博性，我是猎人从容错的角度出发可以这么投）。但是结果4，7，8，9投6，结果6出局。<br>这里狼人有个破绽，2和5选择了弃票。但当时在69发言都没有明显破绽的情况下，我对这个弃票没有太怀疑，思考的重点还是6/9谁是真预言家上。</p>
</li>
<li><p>6号发表遗言：<br>6发言整体比较激动，语气没问题像真预言家，用了4张加时卡，逻辑上看整体发言就表达一个他的逻辑：9预8金，则7是位置及其尴尬的一张牌。如果6是一只狼人，为什么不隐藏自己的身份，大概率把7打出去呢。为什么一定要站出来把7打死，铁铁的要打死这张7呢。<br>这里无非两种情况（当时我没有把这两种情况盘的非常清楚）：</p>
<ol>
<li>6是真预言家，验出来7是狼人，然后从到尾踩死这个7。 </li>
<li>6是狼人，这个狼人选择很冒险却收益很小的一步，冒着自己被投出去的风险杀死7这个民。他的风险在于大家可能不信他是真预言家（事实上第一轮投票大家真没相信）。因为9首位发言给归票位金水预言家面还是有一些的。<br>然后这里1我、3巫、还有4民这三个人无一例外心路历程都是上述第1种，相信了6是真预言家。当时怎么看觉得呢这个6非要踩死这个7，从他的狼人视角来看真的收益好小。</li>
</ol>
</li>
<li><p>第二天晚上9真预查验的是1猎（就是我）是好人，女巫因为是认定6真预言家，所以必然是要开毒，选择毒7。狼人杀了3女巫。<br>然后第二天我是最后发言的归票位。然后第二天白天发言2号没什么信息，3女巫被杀，4民5狼发言如下：<br>4民说了自己觉得信了6真预言家，理由就是感觉上一局6狼的话收益太小。且现在9还活着。表示想上票给9。<br>5狼发言表示和4的逻辑一样，同理觉得6狼的话收益太小。同想上票9。这时候我的心路历程和他俩一样一样的。<br>9真预言家发言，说查验了我是好人。但是9没有怎么盘6的问题。我没有信他的金水。 我觉得9的验人这个力度从9是好人说没问题，因为上一局我给他上了一票，是正确的验人选择。但是反过来从9狼人说也是存在可能性的，拉拢一个好人进来。另外我觉得2如果是真预言家在2天没死的情况下又2天验出来都是好人，内心当时确实存疑。其实从复盘看，他的验人选择真的就是单纯的真预言家好人的视角的做法没什么多想的，但是当时我和其他好人4、8真的没有认下来他的真预言家身份。<br>8民对于9第一天给他的金水不敢接。且他认为狼人第二天没杀9他心存疑惑。8表示要投票给9。<br>1猎人我，我逻辑和4和5一样也走在相信6的道路上。一方面是信了6，另一方面9的发言似乎没怎么针对过6号。这个我心存疑惑。然后我就balabala一直分析6如果是狼人第一天收益太小的事实，希望大家上票给9。</p>
</li>
<li><p>结果：大家上票给9。9出局，9没有太深入的盘逻辑。</p>
</li>
<li><p>第三天夜晚，狼人选择了杀8号。场上剩下四个人：1猎2狼4民5狼。双狼控场游戏结束。</p>
</li>
</ol>
<h4 id="要点总结"><a href="#要点总结" class="headerlink" title="要点总结"></a>要点总结</h4><ol>
<li><p>狼怎么玩的：<br>6狼（胜利方mvp）给7踩死的查杀。这种其实是根据位置学的反逻辑。用冒着自己的危险踩死7的视角，尝试自己的死换来的是强有力的预言家，表面看坐实的程度不低。<br>2号怂狼没有透出任何信息。<br>5狼略微引导了一下6是预言家的逻辑，但是1猎3巫4民确实是想到一块了（我当时确实从6悍跳预言家收益晓得角度认了6预言家，且在我之前发言的4民也是想到了且发言的），并不是因为跟了5的逻辑。</p>
</li>
<li><p>好人怎么玩的：<br>1猎，3巫，4民：其中这三个都是认为6作为狼人可能的收益很小的角度上，认定6可能是真预言家。后面女巫顺势开毒。其他均给9上票。<br>8民，第一天金水没有接也合理。因为6狼发言没有太大问题，9预言家的金水给8也不一定能坐实9。</p>
</li>
<li><p>这一局的独特性，为什么我总觉得这局好人难赢，为什么说这一局不容易太逆转。</p>
<ol>
<li>首先女巫自救没有银水。少验一个人。</li>
<li>预言家是首置位发言，给8民一个金水。8民是金水正好也是归票位的金水。在8是民的情况下。金水可接可不接。预言家可认可不认。（我觉得这里是在赌概率）。且第二天白天预言家这个发言我觉得有不足。</li>
<li>7号民牌，开不了视角。只能反打6，9号一开始根据位置学轻踩7，能说的大致就那么多。</li>
<li>2和5第一天的弃票是狼人一个重要的破绽，回过头来想想第一天好人正常逻辑大概率是不会弃票，肯定是6和7的局。弃票则说明真的狼面很大。但是为什么这个看起来暴露的弃票行为没有获得理论上足够的重视呢？我理解因为6最后遗言这一手。基本上把所有的思路都引入在真预言家身上=&gt;所有好人的当务之急，或者急需抿身份的思路都是【谁是真预言家】这个问题。如果真是6预9狼。9是要马上要投出去的牌。或者要干掉这个被6验出来的7号或者从位置学上被尴尬的7号。总之好人视角来看，7/9两张牌都有争议。无论是7/9出1狼或者7和9狼踩狼，【排7/9的身份或者狼坑】整体执行优先级都高于对于【好人对弃票的反馈】。</li>
</ol>
</li>
</ol>
<h4 id="为什么这局难以逆转"><a href="#为什么这局难以逆转" class="headerlink" title="为什么这局难以逆转"></a>为什么这局难以逆转</h4><p>如果发生以下条件，那么6号预言家难坐实，局势也不会这么发展。</p>
<ol>
<li>第一天狼人没有刀女巫。换来一个银水。</li>
<li>7号8号两个里面存在一个神。真实情况是7、8两位置连民，都整体缺少视角，7被毒冤死，另8缺少足够信息。</li>
<li>第一天9号真预言家不是第一个发言。且给的不是归票位验人。</li>
<li>真预言家在2天内至少验出来一匹狼。且最好第二天验出来狼。1/2/4/5的身份从头到尾巴除了2/5第一天弃票这个信息。基本上发言没有透露太多信息或者破绽。好人从头到位都有点缺乏信息。</li>
</ol>
<h4 id="这局如果好人想赢，怎么赢："><a href="#这局如果好人想赢，怎么赢：" class="headerlink" title="这局如果好人想赢，怎么赢："></a>这局如果好人想赢，怎么赢：</h4><ol>
<li><p>真预言家9，作为场上唯一有视野的人：</p>
<ol>
<li>努力去向全场好人去仔仔细细盘6号这匹狼的狼人意图。语态上始终坚持狠狠的打死6这只狼不动摇。（我认为这个不太容易让好人相信，但是是必要要尝试去做的不能不这么做。）</li>
<li>努力推进打平衡局的玩法。暂时保住7号。表示自己可以扛出，将7当做反向金水。一定建议让女巫不要毒7，哪怕毒自己，打一个平衡局，自己扛出。走1预1狼平衡局，真预言家出局后续走向纯盘逻辑的局。</li>
</ol>
</li>
<li><p>第二天的金水我接住。认同9号的验人逻辑。就是因为我第一天给他上票他验了我。结果是金水。就是这么回事没什么好说的。然后我转过来思维从剩余的人里面出。</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/03/22/%E7%8B%BC%E4%BA%BA%E6%9D%80%E5%A6%82%E4%BD%95%E8%87%B4%E8%83%9C/" data-id="ckjh3lsfe001i19dl1b7v3zim" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-David Stern, who turned NBA into powerhouse" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/18/David%20Stern,%20who%20turned%20NBA%20into%20powerhouse/" class="article-date">
  <time datetime="2020-02-18T14:56:00.000Z" itemprop="datePublished">2020-02-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Reading/">Reading</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/18/David%20Stern,%20who%20turned%20NBA%20into%20powerhouse/">David Stern, who turned NBA into powerhouse</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="NBA，一个世界联盟"><a href="#NBA，一个世界联盟" class="headerlink" title="NBA，一个世界联盟"></a>NBA，一个世界联盟</h2><ul>
<li>从小学时期到现在（2020），NBA贯穿了我的整个成长过程。今天，NBA是一个强大的商业联盟，是一种文化现象，但在早期是一个美国非常边缘的联赛。</li>
<li>大卫斯特恩，已故NBA总裁在30年间做对了几个关键决策。本文思考NBA如何从一个美国边缘联赛，成为一种席卷全球的文化现象。</li>
</ul>
<hr>
<h2 id="大卫·斯特恩留给NBA的遗产"><a href="#大卫·斯特恩留给NBA的遗产" class="headerlink" title="大卫·斯特恩留给NBA的遗产"></a>大卫·斯特恩留给NBA的遗产</h2><ul>
<li>一句话总结：斯特恩把NBA从”职业篮球比赛”变成了”现代体育秀”。NBA不是单纯的一种体育竞技，而是成了一种强大制星能力、强娱乐性的大众文化。有明星、有恩怨、有表演。</li>
<li>改变NBA的规则，让观众觉得不虚此行、看的过瘾。</li>
</ul>
<hr>
<h5 id="竞技平衡性的政策"><a href="#竞技平衡性的政策" class="headerlink" title="竞技平衡性的政策"></a>竞技平衡性的政策</h5><p>Why平衡性?</p>
<ul>
<li>NBA每支球队处于美国不同的城市，经济水平方差很大。大城市薪水高，小城市开不出薪水。实力越强的球员大概率就去大城市。所以强队越强，弱队越弱。是一种不够健康的发展态势。</li>
<li>从观众的角度来看，强队虐弱队，缺乏看点。水平旗鼓相当的对抗，最吸引人。因此问题转化为，如何让整体联盟每只球队实力差距不要太大。</li>
</ul>
<h5 id="出台”工资帽”制度。"><a href="#出台”工资帽”制度。" class="headerlink" title="出台”工资帽”制度。"></a>出台”工资帽”制度。</h5><ul>
<li>每个球队工资总和有个硬性上限，大幅度在整体上实力差距很小。（不是没有差距，是让这种差距在一种可以良性发展的范围内，可以接受的范围内）</li>
</ul>
<h5 id="调整选秀政策"><a href="#调整选秀政策" class="headerlink" title="调整选秀政策"></a>调整选秀政策</h5><ul>
<li>改变早期选秀制度的弊端。成绩越差的球队，下一年会有更好的顺位。因此有的球队在第一年肉眼可见拿不到好成绩的时候，就会选择”来年再战，破罐子破摔”这种策略。因此战绩惨不忍睹，观赏性下降。</li>
<li>排名垫底的若干只球队，都有概率获得状元签，而不是最差的那一只球队。个人感觉就是引入了一些噪声，使得最垫底的球队，自己的想法实现起来有巨大困难，迫使整体而言还是需要把名次向上提升。</li>
</ul>
<h5 id="政策鼓励球员得分"><a href="#政策鼓励球员得分" class="headerlink" title="政策鼓励球员得分"></a>政策鼓励球员得分</h5><ul>
<li>2004引入No Handcheck政策，防守者对于进攻者的身体接触，不能太大。说白了就是让球员的阻碍作用不能太狠，大大增加了更多上篮得分机会。典型的例子就是哈登这种，对于类似的规则相当收益。大大提升了外线球员爆炸性得分的可能性。</li>
<li>这种政策的效果就是对于造星的效果提升十分明显。有更多的球员有个人突出的表现。明星效应来的比之前强很多。</li>
</ul>
<h5 id="改变NBA的产品形式：从”比赛”到”球星”"><a href="#改变NBA的产品形式：从”比赛”到”球星”" class="headerlink" title="改变NBA的产品形式：从”比赛”到”球星”"></a>改变NBA的产品形式：从”比赛”到”球星”</h5><ul>
<li><p>相比于打造比赛，斯特恩更专注与把一个个球员打造成有故事的产品。你想起来一些球员，可能不是他的名字，而是外号。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">约翰逊、拉里伯德、乔丹、詹姆斯、杜兰特、贾巴尔、加内特、邓肯、罗斯、霍华德</span><br></pre></td></tr></table></figure>
<p>对比</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;魔术师&quot;、&quot;大鸟&quot;、&quot;飞人&quot;、&quot;小皇帝&quot;、&quot;死神&quot;、&quot;天沟&quot;、&quot;狼王&quot;、&quot;石佛&quot;、&quot;风城玫瑰&quot;、&quot;魔兽&quot;</span><br></pre></td></tr></table></figure>
<p>下面一行称号显然比上面那行更有吸引力、有故事感。</p>
</li>
<li><p>塑造豪门对决和恩怨情仇。例如，拉里伯德 v.s. 魔术师这两个时代巨星。凯尔特人 v.s. 湖人两支历史豪门对决</p>
</li>
<li><p>热火三巨头：巅峰老詹、巅峰韦德、龙王波什。三个当之无愧的球队老大放在一支球队，能否无敌？</p>
</li>
<li><p>各种”反叛”、”背叛”的文化。每段时间都会塑造一些争议人物。杜兰特投敌是对是错？给我的角度我认为追求总冠军是是没错的，同时对于俄克拉荷马也不欠什么，但是大多数球迷可能认为杜兰特背叛了俄克拉荷马。</p>
</li>
<li><p>造星推进手段1：全明星赛。NBA全明星赛是造星思想的最好体现。开场前的主持人依次介绍每个球员。还会请明星来演唱。绝对不仅仅是一场单纯的经济体育。在全明星赛上面基本上球员不怎么加强防守（多数只有比赛最后一节才认真），以各种夸张的扣篮、空接为主，即使是不爱看篮球的，看完都会很过瘾。</p>
</li>
</ul>
<p>拉文罚球线扣篮</p>
<ul>
<li>造星推进手段2：”圣诞大战” 。安排最强的几支队伍在圣诞节期间强强对话，几乎是额外增加一次重演季后赛决赛或者东西部半决赛的看点。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/18/David%20Stern,%20who%20turned%20NBA%20into%20powerhouse/" data-id="ckjh3lsex000819dl6gfkhx9n" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Linux Commands" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/18/Linux%20Commands/" class="article-date">
  <time datetime="2020-02-18T14:56:00.000Z" itemprop="datePublished">2020-02-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Programming-Data/">Programming & Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/18/Linux%20Commands/">Linux Commands</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="du"><a href="#du" class="headerlink" title="du"></a>du</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">du (disk usage) : 查看磁盘使用空间；</span><br><span class="line">du -sh filename/dirname</span><br></pre></td></tr></table></figure>

<h2 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h2><ul>
<li>When you execute a Unix job in the background ( using &amp;, bg command), and logout from the session, your process will get killed. You can avoid this using several methods — executing the job with nohup, or making it as batch job using at, batch or cron command. Nohup stands for no hang up, which can be executed as shown below. nohup syntax:<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup command &amp;</span><br></pre></td></tr></table></figure></li>
<li>Nohup is very helpful when you have to execute a shell-script or command that take a long time to finish. In that case, you don’t want to be connected to the shell and waiting for the command to complete. Instead, execute it with nohup, exit the shell and continue with your other work.<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -aux | grep charm</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/18/Linux%20Commands/" data-id="ckjh3lsf3000k19dl9zz5hnra" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Recurrent Neural Network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/13/Recurrent%20Neural%20Network/" class="article-date">
  <time datetime="2020-02-13T14:56:00.000Z" itemprop="datePublished">2020-02-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/13/Recurrent%20Neural%20Network/">Recurrent Neural Network</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/13/Recurrent%20Neural%20Network/" data-id="ckjh3lsf3000m19dl0y4z1wur" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Tensorflow API" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/13/Tensorflow%20API/" class="article-date">
  <time datetime="2020-02-13T14:56:00.000Z" itemprop="datePublished">2020-02-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Programming-Data/">Programming & Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/13/Tensorflow%20API/">Tensorflow API</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="tf-nn-embedding-lookup"><a href="#tf-nn-embedding-lookup" class="headerlink" title="tf.nn.embedding_lookup()"></a>tf.nn.embedding_lookup()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup(</span><br><span class="line">    params,</span><br><span class="line">    ids,</span><br><span class="line">    partition_strategy=<span class="string">&#x27;mod&#x27;</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    validate_indices=<span class="literal">True</span>,</span><br><span class="line">    max_norm=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Looks up ids in a list of embedding tensors. This function is used to perform parallel lookups on the list of tensors in params. It is a generalization of tf.gather, where params is interpreted as a partitioning of a large embedding tensor. params may be a PartitionedVariable as returned by using tf.get_variable() with a partitioner. 按照ids顺序返回params中的第ids行。</p>
<ul>
<li>If len(params) &gt; 1, each element id of ids is partitioned between the elements of params according to the partition_strategy. In all strategies, if the id space does not evenly divide the number of partitions, each of the first (max_id + 1) % len(params) partitions will be assigned one more id.</li>
<li>If partition_strategy is “mod”, we assign each id to partition p = id % len(params). For instance, 13 ids are split across 5 partitions as: [[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]</li>
<li>If partition_strategy is “div”, we assign ids to partitions in a contiguous manner. In this case, 13 ids are split across 5 partitions as: [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]</li>
</ul>
<p>The results of the lookup are concatenated into a dense tensor. The returned tensor has shape shape(ids) + shape(params)[1:].</p>
<p>demo</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">a = np.array(</span><br><span class="line">    [[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>],</span><br><span class="line">     [<span class="number">1.1</span>, <span class="number">1.2</span>, <span class="number">1.3</span>],</span><br><span class="line">     [<span class="number">2.1</span>, <span class="number">2.2</span>, <span class="number">2.3</span>],</span><br><span class="line">     [<span class="number">3.1</span>, <span class="number">3.2</span>, <span class="number">3.3</span>],</span><br><span class="line">     [<span class="number">4.1</span>, <span class="number">4.2</span>, <span class="number">4.3</span>]])</span><br><span class="line">idx1 = tf.Variable([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>], tf.int32)</span><br><span class="line">idx2 = tf.Variable([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>]], tf.int32)</span><br><span class="line">out1 = tf.nn.embedding_lookup(a, idx1)</span><br><span class="line">out2 = tf.nn.embedding_lookup(a, idx2)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(a)</span><br><span class="line">    print(<span class="string">&#x27;=========&#x27;</span>)</span><br><span class="line">    print(sess.run(out1))</span><br><span class="line">    print(<span class="string">&#x27;=========&#x27;</span>)</span><br><span class="line">    print(sess.run(out2))</span><br></pre></td></tr></table></figure>
<p>result</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line"> [<span class="number">1.1</span> <span class="number">1.2</span> <span class="number">1.3</span>]</span><br><span class="line"> [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line"> [<span class="number">3.1</span> <span class="number">3.2</span> <span class="number">3.3</span>]</span><br><span class="line"> [<span class="number">4.1</span> <span class="number">4.2</span> <span class="number">4.3</span>]]</span><br><span class="line">=========</span><br><span class="line">[[<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line"> [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line"> [<span class="number">3.1</span> <span class="number">3.2</span> <span class="number">3.3</span>]</span><br><span class="line"> [<span class="number">1.1</span> <span class="number">1.2</span> <span class="number">1.3</span>]]</span><br><span class="line">=========</span><br><span class="line">[[[<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line">  [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line">  [<span class="number">3.1</span> <span class="number">3.2</span> <span class="number">3.3</span>]</span><br><span class="line">  [<span class="number">1.1</span> <span class="number">1.2</span> <span class="number">1.3</span>]]</span><br><span class="line"> [[<span class="number">4.1</span> <span class="number">4.2</span> <span class="number">4.3</span>]</span><br><span class="line">  [<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line">  [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line">  [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]]]</span><br></pre></td></tr></table></figure>


<h2 id="tf-sequence-mask"><a href="#tf-sequence-mask" class="headerlink" title="tf.sequence_mask"></a>tf.sequence_mask</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.sequence_mask(</span><br><span class="line">    lengths,</span><br><span class="line">    maxlen=<span class="literal">None</span>,</span><br><span class="line">    dtype=tf.bool,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>Returns a mask tensor representing the first N positions of each cell. If lengths has shape [d_1, d_2, …, d_n] the resulting tensor mask has dtype dtype and shape [d_1, d_2, …, d_n, maxlen], with<br>mask[i_1, i_2, …, i_n, j] = (j &lt; lengths[i_1, i_2, …, i_n])</p>
<p>Examples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.sequence_mask([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>], <span class="number">5</span>) </span><br><span class="line"><span class="comment"># [[True, False, False, False, False], </span></span><br><span class="line"><span class="comment"># [True, True, True, False, False], </span></span><br><span class="line"><span class="comment"># [True, True, False, False, False]] </span></span><br></pre></td></tr></table></figure>
<p>另外</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.sequence_mask([[<span class="number">1</span>, <span class="number">3</span>],[<span class="number">2</span>,<span class="number">0</span>]]) </span><br><span class="line"><span class="comment"># [[[True, False, False], </span></span><br><span class="line"><span class="comment"># [True, True, True]], </span></span><br><span class="line"><span class="comment"># [[True, True, False], </span></span><br><span class="line"><span class="comment"># [False, False, False]]]</span></span><br></pre></td></tr></table></figure>

<h2 id="tf-expand-dims"><a href="#tf-expand-dims" class="headerlink" title="tf.expand_dims"></a>tf.expand_dims</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims( </span><br><span class="line">    input, </span><br><span class="line">    axis=<span class="literal">None</span>, </span><br><span class="line">    name=<span class="literal">None</span>, </span><br><span class="line">    dim=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>See the guide: Tensor Transformations &gt; Shapes and Shaping</p>
<p>Inserts a dimension of 1 into a tensor’s shape.</p>
<p>Given a tensor input, this operation inserts a dimension of 1 at the dimension index axis of input’s shape. The dimension index axis starts at zero; if you specify a negative number for axis it is counted backward from the end.</p>
<p>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape [height, width, channels], you can make it a batch of 1 image with expand_dims(image, 0), which will make the shape [1, height, width, channels].</p>
<p>Other examples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &#x27;t&#x27; is a tensor of shape [2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">0</span>))  <span class="comment"># [1, 2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">-1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># &#x27;t2&#x27; is a tensor of shape [2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">0</span>))  <span class="comment"># [1, 2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">2</span>))  <span class="comment"># [2, 3, 1, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">3</span>))  <span class="comment"># [2, 3, 5, 1]</span></span><br></pre></td></tr></table></figure>
<p>This operation requires that:</p>
<p>-1-input.dims() &lt;= dim &lt;= input.dims()</p>
<p>This operation is related to squeeze(), which removes dimensions of size 1.</p>
<h2 id="tf-gather"><a href="#tf-gather" class="headerlink" title="tf.gather()"></a>tf.gather()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.gather( </span><br><span class="line">    params, </span><br><span class="line">    indices, </span><br><span class="line">    validate_indices=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>, </span><br><span class="line">    axis=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>indices must be an integer tensor of any dimension (usually 0-D or 1-D). Produces an output tensor with shape params.shape[:axis] + indices.shape + params.shape[axis + 1:] where:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.range(<span class="number">0</span>, <span class="number">10</span>)*<span class="number">10</span> + tf.constant(<span class="number">1</span>, shape=[<span class="number">10</span>])</span><br><span class="line">y = tf.gather(x, [<span class="number">1</span>, <span class="number">5</span>, <span class="number">9</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(x))</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>

<p>result</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ <span class="number">1</span> <span class="number">11</span> <span class="number">21</span> <span class="number">31</span> <span class="number">41</span> <span class="number">51</span> <span class="number">61</span> <span class="number">71</span> <span class="number">81</span> <span class="number">91</span>]</span><br><span class="line">[<span class="number">11</span> <span class="number">51</span> <span class="number">91</span>]</span><br></pre></td></tr></table></figure>


<h2 id="tf-reshape"><a href="#tf-reshape" class="headerlink" title="tf.reshape()"></a>tf.reshape()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.reshape(</span><br><span class="line">    tensor,</span><br><span class="line">    shape,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Given tensor, this operation returns a tensor that has the same values as tensor with shape shape.</p>
<p>If one component of shape is the special value -1, the size of that dimension is computed so that the total size remains constant. In particular, a shape of [-1] flattens into 1-D. At most one component of shape can be -1.　If shape is 1-D or higher, then the operation returns a tensor with shape shape filled with the values of tensor. In this case, the number of elements implied by shape must be the same as the number of elements in tensor.</p>
<p>demo</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(np.array(list(range(<span class="number">1</span>, <span class="number">9</span>+<span class="number">1</span>, <span class="number">1</span>))))</span><br><span class="line">b = tf.reshape(a, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># t: a (3 x 2 x 3) tensor</span></span><br><span class="line">t = tf.constant(</span><br><span class="line">    [[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">      [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]],</span><br><span class="line">     [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">      [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]],</span><br><span class="line">     [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">      [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]]</span><br><span class="line">)</span><br><span class="line">c = tf.reshape(t, [<span class="number">2</span>, <span class="number">-1</span>])</span><br><span class="line">d = tf.reshape(t, [<span class="number">-1</span>, <span class="number">9</span>])</span><br><span class="line">e = tf.reshape(t, [<span class="number">2</span>, <span class="number">-1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(b.get_shape().as_list())</span><br><span class="line">    print(sess.run(t), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(t.get_shape().as_list())</span><br><span class="line">    print(sess.run(c), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(c.get_shape().as_list())</span><br><span class="line">    print(sess.run(d), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(d.get_shape().as_list())</span><br><span class="line">    print(sess.run(e), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(e.get_shape().as_list())</span><br></pre></td></tr></table></figure>
<p>result</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[[ <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line">  [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span>]</span><br><span class="line">  [<span class="number">10</span> <span class="number">11</span> <span class="number">12</span>]]]</span><br><span class="line"></span><br><span class="line">[<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">[[[ <span class="number">1</span>  <span class="number">4</span>]</span><br><span class="line">  [ <span class="number">2</span>  <span class="number">5</span>]</span><br><span class="line">  [ <span class="number">3</span>  <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7</span> <span class="number">10</span>]</span><br><span class="line">  [ <span class="number">8</span> <span class="number">11</span>]</span><br><span class="line">  [ <span class="number">9</span> <span class="number">12</span>]]]</span><br><span class="line"></span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<h2 id="tf-split"><a href="#tf-split" class="headerlink" title="tf.split()"></a>tf.split()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.split(</span><br><span class="line">    value,</span><br><span class="line">    num_or_size_splits,</span><br><span class="line">    axis=<span class="number">0</span>,</span><br><span class="line">    num=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">&#x27;split&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Splits a tensor into sub tensors.</p>
<p>If num_or_size_splits is an integer type, num_split, then splits value along dimension axis into num_split smaller tensors. Requires that num_split evenly divides value.shape[axis]. If num_or_size_splits is not an integer type, it is presumed to be a Tensor size_splits, then splits value into len(size_splits) pieces. The shape of the i-th piece has the same size as the value except along dimension axis where the size is size_splits[i].</p>
<p>demo</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(np.reshape(list(range(<span class="number">1</span>, <span class="number">6</span>+<span class="number">1</span>, <span class="number">1</span>)), (<span class="number">2</span>, <span class="number">3</span>)))</span><br><span class="line">b1, b2, b3 = tf.split(a, <span class="number">3</span>, axis=<span class="number">1</span>)</span><br><span class="line">d1, d2, d3 = tf.split(a, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], axis=<span class="number">1</span>)</span><br><span class="line">c1, c2 = tf.split(a, [<span class="number">1</span>, <span class="number">2</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b1), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b2), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b3), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(d1), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(d2), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(d3), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(c1), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(c2), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">result</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>]</span><br><span class="line"> [<span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span>]</span><br><span class="line"> [<span class="number">5</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">3</span>]</span><br><span class="line"> [<span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>]</span><br><span class="line"> [<span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span>]</span><br><span class="line"> [<span class="number">5</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">3</span>]</span><br><span class="line"> [<span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>]</span><br><span class="line"> [<span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">6</span>]]</span><br></pre></td></tr></table></figure>

<h2 id="tf-transpose"><a href="#tf-transpose" class="headerlink" title="tf.transpose()"></a>tf.transpose()</h2><p>demo</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">     [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">)</span><br><span class="line">b = tf.transpose(a)</span><br><span class="line"><span class="comment"># equivaltently</span></span><br><span class="line">c = tf.transpose(a, [<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># x : 2 x 2 x 3</span></span><br><span class="line">x = tf.constant(</span><br><span class="line">    [[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">      [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]],</span><br><span class="line">     [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">      [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]]</span><br><span class="line">)</span><br><span class="line"><span class="comment"># perm is more useful for n-dimensional tersors, for n &gt; 2</span></span><br><span class="line"><span class="comment"># y : 2 x 3 x 2</span></span><br><span class="line">y = tf.transpose(x, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(c), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    print(sess.run(x), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(x.get_shape().as_list(), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(y), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(y.get_shape().as_list(), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>result</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[[ <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line">  [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span>]</span><br><span class="line">  [<span class="number">10</span> <span class="number">11</span> <span class="number">12</span>]]]</span><br><span class="line"></span><br><span class="line">[<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/13/Tensorflow%20API/" data-id="ckjh3lsf5000r19dl3zbj47yo" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Graph Neural Network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/03/Graph%20Neural%20Network/" class="article-date">
  <time datetime="2020-02-03T03:56:00.000Z" itemprop="datePublished">2020-02-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/03/Graph%20Neural%20Network/">Graph Neural Network</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h2><p>Graph neural network/Graph embedding code.  </p>
<ul>
<li>GNN: gcn, graphSAGE</li>
<li>GE: node2vec</li>
</ul>
<h2 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h2><div style="text-align: center;">
<img alt="" src="https://s3.ax1x.com/2020/11/15/DFYfqe.png" style="display: inline-block;" width="666"/>
</div>

<p>GCN: learn a function of signals/features on a graph <em>G = (V, E)</em> which takes as input:  </p>
<ul>
<li>A feature description <em>x<sub>i</sub></em> for every node i summarized in a <em>N x D</em> feature matrix <em>X</em>. (<em>N</em> : number of nodes, <em>D</em> : number of input features)</li>
<li>A representative description of the graph structure in matrix form, eg: adjacency matrix <em>A</em>.</li>
</ul>
<p>and produces a node-level output <em>Z</em> (an <em>N x F</em> feature matrix, where <em>F</em> is the number of output features per node). Graph-level outputs can be modeled by producing some form of pooling operation.</p>
<p>Every neural network layer can then be written as a non-linear function, consider the following simple form of a layer-wise propagation rule:  </p>
<p><img src="http://latex.codecogs.com/png.latex?H%5E%7B(l+1)%7D=f%5Cleft(H%5E%7B(l)%7D,A%5Cright)=%5Csigma%5Cleft(AH%5E%7B(l)%7DW%5E%7B(l)%7D%5Cright)" alt="gnn">  </p>
<p>with <em>H<sup>(0)</sup>=X</em> and <em>H<sup>(L)</sup>=Z</em> (<em>z</em> for graph-level outputs), <em>L</em> being the number of layers. The specific models then differ only in how <img src="http://latex.codecogs.com/png.latex?f%5Cleft(%5Ccdot,%7B%5Ccdot%7D%5Cright)" alt="f(⋅,⋅)">  is chosen and parameterized.</p>
<h2 id="Preference"><a href="#Preference" class="headerlink" title="Preference"></a>Preference</h2><ul>
<li><p>Semi-Supervised Classification with Graph Convolutional Networks<br><a target="_blank" rel="noopener" href="https://github.com/tkipf/gcn/">https://github.com/tkipf/gcn/</a> </p>
</li>
<li><p>Inductive Representation Learning on Large Graphs<br><a target="_blank" rel="noopener" href="https://github.com/williamleif/graphsage-simple">https://github.com/williamleif/graphsage-simple</a></p>
</li>
<li><p>node2vec: Scalable Feature Learning for Networks<br><a target="_blank" rel="noopener" href="https://github.com/aditya-grover/node2vec">https://github.com/aditya-grover/node2vec</a></p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/03/Graph%20Neural%20Network/" data-id="ckjh3lsf1000g19dl8jjj29iu" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-The Illustrated Transformer" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/03/The%20Illustrated%20Transformer/" class="article-date">
  <time datetime="2020-02-03T03:56:00.000Z" itemprop="datePublished">2020-02-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/03/The%20Illustrated%20Transformer/">The Illustrated Transformer</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul>
<li>介绍博客链接关于Transformer以及论文《Attention is all you need》以加深对于transformer框架的理解；</li>
<li>Transformer在时序建模上效果在很多nlp任务上优于RNN，且不同于CNN卷积进行特征提取过程；</li>
</ul>
<h2 id="Transformer用于机器翻译任务"><a href="#Transformer用于机器翻译任务" class="headerlink" title="Transformer用于机器翻译任务"></a>Transformer用于机器翻译任务</h2><ul>
<li><p>Transformer的提出用于机器翻译任务；翻译句子时输入输出是两个句子，中间黑盒包括一个Encoder和一个Decoder，更一般的情况是多个Encoder和多个Decoder进行堆叠；</p>
</li>
<li><p>拆解开单个Encoder，其中是一个Self-Attention和一个Feed Forward层；拆解开单个Decoder，其中是一个Self-Attention、一个Enconder-Decoder Attention层和一个Feed Forward层；</p>
</li>
</ul>
<h2 id="Self-Attention-Layer-拆解"><a href="#Self-Attention-Layer-拆解" class="headerlink" title="Self-Attention Layer 拆解"></a>Self-Attention Layer 拆解</h2><ul>
<li><p>和常见NLP任务一样输入为单词的embedding，分别表示为；经过Self-Attention层后，分别输出为，再分别过各自的FFNN’</p>
</li>
<li><p>Attention过程首先对每个单词的embedding构造三个向量分别为Key、Query和Value；这三个向量是通过输入单词embedding分别乘以三个矩阵所得分别表示为，这三个矩阵在训练过程中得到；</p>
</li>
</ul>
<ul>
<li><p>Self-Attention过程如何实现？分解来看就是理解weight怎么得到，得到weight以后乘上去即可（Transformer里面是乘在value上），即理解如下过程；<br>  输入单词的embedding的query根据key点乘进行打分（维度归一化）</p>
<p>  =&gt; softmax之后得到一个weight</p>
<p>  =&gt; 对value乘以weight进行加权求和得到z</p>
</li>
</ul>
<ul>
<li>Self-Attention的计算过程可以用如下式子表示，其中理解Softmax(x)=weight</li>
</ul>
<p>上述过程描述了单个输入单词embedding生成self-attention后的表示的过程，在实际运算时通过矩阵运算来并行实现所有单词的self-attention过程；同时，上述实现了一个head的self-attention的过程，实际上可以设置多个head进行self-attention，再将多个head学习到的表示z进行concatenate，最后将这个宽表示乘以一个weight矩阵；</p>
<ul>
<li><p>截止目前Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子中词的顺序怎么打乱Transformer都会得到类似的结果，换句话说此时的Transformer只是一个功能更强大的词袋模型而已。为解决这个问题，Transformer编码词向量时引入了位置编码（Position Embedding）特征。那么如何编码位置信息呢？常见模式有：a. 根据数据学习；b. 自己设计编码规则。</p>
</li>
<li><p>另外Transformer在每个Encoder-Block里面，同时采用ResBlock的结构，另外采用了Layer Nomalization的方法进行配合使用；</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/03/The%20Illustrated%20Transformer/" data-id="ckjh3lsfc001c19dleu2vgti7" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/About-Us/">About Us</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Always-Review/">Always Review</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Data/">Programming & Data</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading/">Reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recsys-Ads-Tech/">Recsys & Ads Tech</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2099/01/">January 2099</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2099/01/01/About%20Us/">About Us</a>
          </li>
        
          <li>
            <a href="/2020/12/31/2020%20Annual%20Summary/">2020 Annual Summary</a>
          </li>
        
          <li>
            <a href="/2020/12/20/Shoe%20Dog/">Shoe Dog</a>
          </li>
        
          <li>
            <a href="/2020/12/16/Bert/">Bert</a>
          </li>
        
          <li>
            <a href="/2020/11/08/The%20Essence%20of%20RecSys/">The Essence of RecSys</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Gold and Rabbit<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>