<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Gold and Rabbit&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="having fun coding">
<meta property="og:type" content="website">
<meta property="og:title" content="Gold and Rabbit&#39;s Blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Gold and Rabbit&#39;s Blog">
<meta property="og:description" content="having fun coding">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Gold and Rabbit">
<meta property="article:tag" content="Recommending System">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Gold and Rabbit&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Gold and Rabbit&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-狼人杀如何致胜" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/22/%E7%8B%BC%E4%BA%BA%E6%9D%80%E5%A6%82%E4%BD%95%E8%87%B4%E8%83%9C/" class="article-date">
  <time datetime="2020-03-21T18:12:57.000Z" itemprop="datePublished">2020-03-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Always-Review/">Always Review</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/22/%E7%8B%BC%E4%BA%BA%E6%9D%80%E5%A6%82%E4%BD%95%E8%87%B4%E8%83%9C/">狼人杀如何致胜</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul>
<li>到底是一个重点是靠盘逻辑的游戏？还是一个缺少逻辑、更多的是表演/发言/说谎的游戏？逻辑在获胜上占到多少比重？</li>
<li>表演是这个游戏里面重要的部分。如何正确的从自己的角色表演，达到获胜的目的。</li>
<li>作为游戏，均存在玩物丧志属性，总结游戏玩法也是符合让这个游戏对于我的边际效益最小化的目的，鼓励投入到新领域中去。</li>
<li>本文重点是9人国标局。3民3狼1预1巫1猎。12人局存在玩家质量参差不齐的局面。</li>
</ul>
<h2 id="获胜之道"><a href="#获胜之道" class="headerlink" title="获胜之道"></a>获胜之道</h2><ul>
<li>总结一下玩到现在如何获得胜利的方法，理论不一定严谨，整体目的是避免一些错误的发言和节奏，梳理一些关键性的要点，在整体上最大化胜率。重点解析预言家和狼人这两个角色。</li>
</ul>
<h5 id="预言家"><a href="#预言家" class="headerlink" title="预言家"></a>预言家</h5><ul>
<li>预言家是9人局胜率最低的角色，这是由于规则决定的（大概率双神起跳）。</li>
<li>作为预言家必须非常坦然接受你死的很快或者你死的很冤的事实。</li>
<li>发言的时候必须快速清晰的报出来查验，以免还没有说出来任何查验只透露自己身份然后被狼人自爆。预言家在遗言的时候。一定要多说。即使是非常冤死。但是真预言家的求生欲一定不能过低。</li>
</ul>
<h5 id="狼人"><a href="#狼人" class="headerlink" title="狼人"></a>狼人</h5><ul>
<li>狼人的胜利关键在于“顺势而为”，三只狼人需要各自独立地根据场上的形势因时制宜的选择悍跳/划水/引导/站边/跟票/冲票。</li>
<li>同时三狼要有一定程度的分工，整体态势太趋同（都怂或者都悍跳）风险的。</li>
</ul>
<h2 id="案例复盘"><a href="#案例复盘" class="headerlink" title="案例复盘"></a>案例复盘</h2><h4 id="失败的一局复盘"><a href="#失败的一局复盘" class="headerlink" title="失败的一局复盘"></a>失败的一局复盘</h4><div style="text-align: center;">
<img alt="" src="https://i.loli.net/2020/09/14/QYTr83RcIsd4qWh.jpg" style="display: inline-block;" width="200"/>
</div>


<ul>
<li>图上这局是我玩的比较失败的一局。结果：作为好人阵营（猎人）我们输的非常彻底，被一只悍跳狼完全带了节奏，狼人预言家身份坐实，民整体没有视角做得出的选择也都是错误的，神职女巫和猎人完全齐齐的走向了错误的做法。</li>
</ul>
<p>这局的特殊在于 </p>
<ol>
<li>一局下来好人之间几乎没有形成提供有效的通讯（统计来看9人国标局好人输概率大于50%，但是好人们几乎完全没有相互之间的信息认可，这种是绝对不算多的案例）。</li>
<li>悍跳狼第一天就被投死，其余怂狼从头到尾也没有也没有太多行动。但是好人掉进圈套里完全出不来，后续决策是一连串错误，而且是走向那种温水煮青蛙的错误节奏。</li>
<li>整局游戏来看，好人绝对不是一定输，是肯定机会逆转的，但是整体上我认为好人如果想赢非常艰难，故做一下复盘。</li>
</ol>
<h4 id="重点历程"><a href="#重点历程" class="headerlink" title="重点历程"></a>重点历程</h4><ol start="0">
<li><p>狼人刀3号女巫，3女巫自救。9预言家8金水。</p>
</li>
<li><p>第一天9号第一个发言，说自己预言家给8号的民发了金水，发言过程中没有明显语气或者逻辑上的问题。然后说由于7、8、9位置学，轻踩一下7号。接着顺次12345发言均没有有效信息，其中女巫跳出来身份说明是自救的。<br>8号是归票位，给8发出去金水，然后8又没有身份，我当时认为信服力不一定。<br>但是8不是神职，如果9是狼人发到8民身上，可能性也是存在的，发言上没有明显问题，当时我没有完全不信或者不信。</p>
</li>
<li><p>然后6号发言，给7号发了查杀。<br>从位置上来看，9预言家8号金水则7号位置有些尴尬。<br>6给7查杀，当时我是不太信的，然后7号是民，但7的角度而言，6和9对7都有一定攻击性。但6是查杀，他从自身身份出发只能从发言打死6。</p>
</li>
<li><p>第一天投票：我上票给了7号，6整体发言语气和逻辑都还好（6查验的是后位置的7有合理性，感觉9的力度不太好说，其实我这里操作有一定的赌博性，我是猎人从容错的角度出发可以这么投）。但是结果4，7，8，9投6，结果6出局。<br>这里狼人有个破绽，2和5选择了弃票。但当时在69发言都没有明显破绽的情况下，我对这个弃票没有太怀疑，思考的重点还是6/9谁是真预言家上。</p>
</li>
<li><p>6号发表遗言：<br>6发言整体比较激动，语气没问题像真预言家，用了4张加时卡，逻辑上看整体发言就表达一个他的逻辑：9预8金，则7是位置及其尴尬的一张牌。如果6是一只狼人，为什么不隐藏自己的身份，大概率把7打出去呢。为什么一定要站出来把7打死，铁铁的要打死这张7呢。<br>这里无非两种情况（当时我没有把这两种情况盘的非常清楚）：</p>
<ol>
<li>6是真预言家，验出来7是狼人，然后从到尾踩死这个7。 </li>
<li>6是狼人，这个狼人选择很冒险却收益很小的一步，冒着自己被投出去的风险杀死7这个民。他的风险在于大家可能不信他是真预言家（事实上第一轮投票大家真没相信）。因为9首位发言给归票位金水预言家面还是有一些的。<br>然后这里1我、3巫、还有4民这三个人无一例外心路历程都是上述第1种，相信了6是真预言家。当时怎么看觉得呢这个6非要踩死这个7，从他的狼人视角来看真的收益好小。</li>
</ol>
</li>
<li><p>第二天晚上9真预查验的是1猎（就是我）是好人，女巫因为是认定6真预言家，所以必然是要开毒，选择毒7。狼人杀了3女巫。<br>然后第二天我是最后发言的归票位。然后第二天白天发言2号没什么信息，3女巫被杀，4民5狼发言如下：<br>4民说了自己觉得信了6真预言家，理由就是感觉上一局6狼的话收益太小。且现在9还活着。表示想上票给9。<br>5狼发言表示和4的逻辑一样，同理觉得6狼的话收益太小。同想上票9。这时候我的心路历程和他俩一样一样的。<br>9真预言家发言，说查验了我是好人。但是9没有怎么盘6的问题。我没有信他的金水。 我觉得9的验人这个力度从9是好人说没问题，因为上一局我给他上了一票，是正确的验人选择。但是反过来从9狼人说也是存在可能性的，拉拢一个好人进来。另外我觉得2如果是真预言家在2天没死的情况下又2天验出来都是好人，内心当时确实存疑。其实从复盘看，他的验人选择真的就是单纯的真预言家好人的视角的做法没什么多想的，但是当时我和其他好人4、8真的没有认下来他的真预言家身份。<br>8民对于9第一天给他的金水不敢接。且他认为狼人第二天没杀9他心存疑惑。8表示要投票给9。<br>1猎人我，我逻辑和4和5一样也走在相信6的道路上。一方面是信了6，另一方面9的发言似乎没怎么针对过6号。这个我心存疑惑。然后我就balabala一直分析6如果是狼人第一天收益太小的事实，希望大家上票给9。</p>
</li>
<li><p>结果：大家上票给9。9出局，9没有太深入的盘逻辑。</p>
</li>
<li><p>第三天夜晚，狼人选择了杀8号。场上剩下四个人：1猎2狼4民5狼。双狼控场游戏结束。</p>
</li>
</ol>
<h4 id="要点总结"><a href="#要点总结" class="headerlink" title="要点总结"></a>要点总结</h4><ol>
<li><p>狼怎么玩的：<br>6狼（胜利方mvp）给7踩死的查杀。这种其实是根据位置学的反逻辑。用冒着自己的危险踩死7的视角，尝试自己的死换来的是强有力的预言家，表面看坐实的程度不低。<br>2号怂狼没有透出任何信息。<br>5狼略微引导了一下6是预言家的逻辑，但是1猎3巫4民确实是想到一块了（我当时确实从6悍跳预言家收益晓得角度认了6预言家，且在我之前发言的4民也是想到了且发言的），并不是因为跟了5的逻辑。</p>
</li>
<li><p>好人怎么玩的：<br>1猎，3巫，4民：其中这三个都是认为6作为狼人可能的收益很小的角度上，认定6可能是真预言家。后面女巫顺势开毒。其他均给9上票。<br>8民，第一天金水没有接也合理。因为6狼发言没有太大问题，9预言家的金水给8也不一定能坐实9。</p>
</li>
<li><p>这一局的独特性，为什么我总觉得这局好人难赢，为什么说这一局不容易太逆转。</p>
<ol>
<li>首先女巫自救没有银水。少验一个人。</li>
<li>预言家是首置位发言，给8民一个金水。8民是金水正好也是归票位的金水。在8是民的情况下。金水可接可不接。预言家可认可不认。（我觉得这里是在赌概率）。且第二天白天预言家这个发言我觉得有不足。</li>
<li>7号民牌，开不了视角。只能反打6，9号一开始根据位置学轻踩7，能说的大致就那么多。</li>
<li>2和5第一天的弃票是狼人一个重要的破绽，回过头来想想第一天好人正常逻辑大概率是不会弃票，肯定是6和7的局。弃票则说明真的狼面很大。但是为什么这个看起来暴露的弃票行为没有获得理论上足够的重视呢？我理解因为6最后遗言这一手。基本上把所有的思路都引入在真预言家身上=&gt;所有好人的当务之急，或者急需抿身份的思路都是【谁是真预言家】这个问题。如果真是6预9狼。9是要马上要投出去的牌。或者要干掉这个被6验出来的7号或者从位置学上被尴尬的7号。总之好人视角来看，7/9两张牌都有争议。无论是7/9出1狼或者7和9狼踩狼，【排7/9的身份或者狼坑】整体执行优先级都高于对于【好人对弃票的反馈】。</li>
</ol>
</li>
</ol>
<h4 id="为什么这局难以逆转"><a href="#为什么这局难以逆转" class="headerlink" title="为什么这局难以逆转"></a>为什么这局难以逆转</h4><p>如果发生以下条件，那么6号预言家难坐实，局势也不会这么发展。</p>
<ol>
<li>第一天狼人没有刀女巫。换来一个银水。</li>
<li>7号8号两个里面存在一个神。真实情况是7、8两位置连民，都整体缺少视角，7被毒冤死，另8缺少足够信息。</li>
<li>第一天9号真预言家不是第一个发言。且给的不是归票位验人。</li>
<li>真预言家在2天内至少验出来一匹狼。且最好第二天验出来狼。1/2/4/5的身份从头到尾巴除了2/5第一天弃票这个信息。基本上发言没有透露太多信息或者破绽。好人从头到位都有点缺乏信息。</li>
</ol>
<h4 id="这局如果好人想赢，怎么赢："><a href="#这局如果好人想赢，怎么赢：" class="headerlink" title="这局如果好人想赢，怎么赢："></a>这局如果好人想赢，怎么赢：</h4><ol>
<li><p>真预言家9，作为场上唯一有视野的人：</p>
<ol>
<li>努力去向全场好人去仔仔细细盘6号这匹狼的狼人意图。语态上始终坚持狠狠的打死6这只狼不动摇。（我认为这个不太容易让好人相信，但是是必要要尝试去做的不能不这么做。）</li>
<li>努力推进打平衡局的玩法。暂时保住7号。表示自己可以扛出，将7当做反向金水。一定建议让女巫不要毒7，哪怕毒自己，打一个平衡局，自己扛出。走1预1狼平衡局，真预言家出局后续走向纯盘逻辑的局。</li>
</ol>
</li>
<li><p>第二天的金水我接住。认同9号的验人逻辑。就是因为我第一天给他上票他验了我。结果是金水。就是这么回事没什么好说的。然后我转过来思维从剩余的人里面出。</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/03/22/%E7%8B%BC%E4%BA%BA%E6%9D%80%E5%A6%82%E4%BD%95%E8%87%B4%E8%83%9C/" data-id="ckix721hy001buddl7i5wfhif" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Linux Commands" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/18/Linux%20Commands/" class="article-date">
  <time datetime="2020-02-18T14:56:00.000Z" itemprop="datePublished">2020-02-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Programming-Data/">Programming & Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/18/Linux%20Commands/">Linux Commands</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="du"><a href="#du" class="headerlink" title="du"></a>du</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">du (disk usage) : 查看磁盘使用空间；</span><br><span class="line">du -sh filename/dirname</span><br></pre></td></tr></table></figure>

<h2 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h2><ul>
<li>When you execute a Unix job in the background ( using &amp;, bg command), and logout from the session, your process will get killed. You can avoid this using several methods — executing the job with nohup, or making it as batch job using at, batch or cron command. Nohup stands for no hang up, which can be executed as shown below. nohup syntax:<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup command &amp;</span><br></pre></td></tr></table></figure></li>
<li>Nohup is very helpful when you have to execute a shell-script or command that take a long time to finish. In that case, you don’t want to be connected to the shell and waiting for the command to complete. Instead, execute it with nohup, exit the shell and continue with your other work.<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -aux | grep charm</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/18/Linux%20Commands/" data-id="ckix721hn000iuddl8z50e64z" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Recurrent Neural Network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/13/Recurrent%20Neural%20Network/" class="article-date">
  <time datetime="2020-02-13T14:56:00.000Z" itemprop="datePublished">2020-02-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/13/Recurrent%20Neural%20Network/">Recurrent Neural Network</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/13/Recurrent%20Neural%20Network/" data-id="ckix721ho000luddlc5736en2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Tensorflow API" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/13/Tensorflow%20API/" class="article-date">
  <time datetime="2020-02-13T14:56:00.000Z" itemprop="datePublished">2020-02-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Programming-Data/">Programming & Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/13/Tensorflow%20API/">Tensorflow API</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="tf-nn-embedding-lookup"><a href="#tf-nn-embedding-lookup" class="headerlink" title="tf.nn.embedding_lookup()"></a>tf.nn.embedding_lookup()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup(</span><br><span class="line">    params,</span><br><span class="line">    ids,</span><br><span class="line">    partition_strategy=<span class="string">&#x27;mod&#x27;</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    validate_indices=<span class="literal">True</span>,</span><br><span class="line">    max_norm=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Looks up ids in a list of embedding tensors. This function is used to perform parallel lookups on the list of tensors in params. It is a generalization of tf.gather, where params is interpreted as a partitioning of a large embedding tensor. params may be a PartitionedVariable as returned by using tf.get_variable() with a partitioner. 按照ids顺序返回params中的第ids行。</p>
<ul>
<li>If len(params) &gt; 1, each element id of ids is partitioned between the elements of params according to the partition_strategy. In all strategies, if the id space does not evenly divide the number of partitions, each of the first (max_id + 1) % len(params) partitions will be assigned one more id.</li>
<li>If partition_strategy is “mod”, we assign each id to partition p = id % len(params). For instance, 13 ids are split across 5 partitions as: [[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]</li>
<li>If partition_strategy is “div”, we assign ids to partitions in a contiguous manner. In this case, 13 ids are split across 5 partitions as: [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]</li>
</ul>
<p>The results of the lookup are concatenated into a dense tensor. The returned tensor has shape shape(ids) + shape(params)[1:].</p>
<p>demo</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">a = np.array(</span><br><span class="line">    [[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>],</span><br><span class="line">     [<span class="number">1.1</span>, <span class="number">1.2</span>, <span class="number">1.3</span>],</span><br><span class="line">     [<span class="number">2.1</span>, <span class="number">2.2</span>, <span class="number">2.3</span>],</span><br><span class="line">     [<span class="number">3.1</span>, <span class="number">3.2</span>, <span class="number">3.3</span>],</span><br><span class="line">     [<span class="number">4.1</span>, <span class="number">4.2</span>, <span class="number">4.3</span>]])</span><br><span class="line">idx1 = tf.Variable([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>], tf.int32)</span><br><span class="line">idx2 = tf.Variable([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>]], tf.int32)</span><br><span class="line">out1 = tf.nn.embedding_lookup(a, idx1)</span><br><span class="line">out2 = tf.nn.embedding_lookup(a, idx2)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(a)</span><br><span class="line">    print(<span class="string">&#x27;=========&#x27;</span>)</span><br><span class="line">    print(sess.run(out1))</span><br><span class="line">    print(<span class="string">&#x27;=========&#x27;</span>)</span><br><span class="line">    print(sess.run(out2))</span><br></pre></td></tr></table></figure>
<p>result</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line"> [<span class="number">1.1</span> <span class="number">1.2</span> <span class="number">1.3</span>]</span><br><span class="line"> [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line"> [<span class="number">3.1</span> <span class="number">3.2</span> <span class="number">3.3</span>]</span><br><span class="line"> [<span class="number">4.1</span> <span class="number">4.2</span> <span class="number">4.3</span>]]</span><br><span class="line">=========</span><br><span class="line">[[<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line"> [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line"> [<span class="number">3.1</span> <span class="number">3.2</span> <span class="number">3.3</span>]</span><br><span class="line"> [<span class="number">1.1</span> <span class="number">1.2</span> <span class="number">1.3</span>]]</span><br><span class="line">=========</span><br><span class="line">[[[<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line">  [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line">  [<span class="number">3.1</span> <span class="number">3.2</span> <span class="number">3.3</span>]</span><br><span class="line">  [<span class="number">1.1</span> <span class="number">1.2</span> <span class="number">1.3</span>]]</span><br><span class="line"> [[<span class="number">4.1</span> <span class="number">4.2</span> <span class="number">4.3</span>]</span><br><span class="line">  [<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line">  [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line">  [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]]]</span><br></pre></td></tr></table></figure>


<h2 id="tf-sequence-mask"><a href="#tf-sequence-mask" class="headerlink" title="tf.sequence_mask"></a>tf.sequence_mask</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.sequence_mask(</span><br><span class="line">    lengths,</span><br><span class="line">    maxlen=<span class="literal">None</span>,</span><br><span class="line">    dtype=tf.bool,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>Returns a mask tensor representing the first N positions of each cell. If lengths has shape [d_1, d_2, …, d_n] the resulting tensor mask has dtype dtype and shape [d_1, d_2, …, d_n, maxlen], with<br>mask[i_1, i_2, …, i_n, j] = (j &lt; lengths[i_1, i_2, …, i_n])</p>
<p>Examples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.sequence_mask([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>], <span class="number">5</span>) </span><br><span class="line"><span class="comment"># [[True, False, False, False, False], </span></span><br><span class="line"><span class="comment"># [True, True, True, False, False], </span></span><br><span class="line"><span class="comment"># [True, True, False, False, False]] </span></span><br></pre></td></tr></table></figure>
<p>另外</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.sequence_mask([[<span class="number">1</span>, <span class="number">3</span>],[<span class="number">2</span>,<span class="number">0</span>]]) </span><br><span class="line"><span class="comment"># [[[True, False, False], </span></span><br><span class="line"><span class="comment"># [True, True, True]], </span></span><br><span class="line"><span class="comment"># [[True, True, False], </span></span><br><span class="line"><span class="comment"># [False, False, False]]]</span></span><br></pre></td></tr></table></figure>

<h2 id="tf-expand-dims"><a href="#tf-expand-dims" class="headerlink" title="tf.expand_dims"></a>tf.expand_dims</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims( </span><br><span class="line">    input, </span><br><span class="line">    axis=<span class="literal">None</span>, </span><br><span class="line">    name=<span class="literal">None</span>, </span><br><span class="line">    dim=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>See the guide: Tensor Transformations &gt; Shapes and Shaping</p>
<p>Inserts a dimension of 1 into a tensor’s shape.</p>
<p>Given a tensor input, this operation inserts a dimension of 1 at the dimension index axis of input’s shape. The dimension index axis starts at zero; if you specify a negative number for axis it is counted backward from the end.</p>
<p>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape [height, width, channels], you can make it a batch of 1 image with expand_dims(image, 0), which will make the shape [1, height, width, channels].</p>
<p>Other examples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &#x27;t&#x27; is a tensor of shape [2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">0</span>))  <span class="comment"># [1, 2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">-1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># &#x27;t2&#x27; is a tensor of shape [2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">0</span>))  <span class="comment"># [1, 2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">2</span>))  <span class="comment"># [2, 3, 1, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">3</span>))  <span class="comment"># [2, 3, 5, 1]</span></span><br></pre></td></tr></table></figure>
<p>This operation requires that:</p>
<p>-1-input.dims() &lt;= dim &lt;= input.dims()</p>
<p>This operation is related to squeeze(), which removes dimensions of size 1.</p>
<h2 id="tf-gather"><a href="#tf-gather" class="headerlink" title="tf.gather()"></a>tf.gather()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.gather( </span><br><span class="line">    params, </span><br><span class="line">    indices, </span><br><span class="line">    validate_indices=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>, </span><br><span class="line">    axis=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>indices must be an integer tensor of any dimension (usually 0-D or 1-D). Produces an output tensor with shape params.shape[:axis] + indices.shape + params.shape[axis + 1:] where:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.range(<span class="number">0</span>, <span class="number">10</span>)*<span class="number">10</span> + tf.constant(<span class="number">1</span>, shape=[<span class="number">10</span>])</span><br><span class="line">y = tf.gather(x, [<span class="number">1</span>, <span class="number">5</span>, <span class="number">9</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(x))</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>

<p>result</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ <span class="number">1</span> <span class="number">11</span> <span class="number">21</span> <span class="number">31</span> <span class="number">41</span> <span class="number">51</span> <span class="number">61</span> <span class="number">71</span> <span class="number">81</span> <span class="number">91</span>]</span><br><span class="line">[<span class="number">11</span> <span class="number">51</span> <span class="number">91</span>]</span><br></pre></td></tr></table></figure>


<h2 id="tf-reshape"><a href="#tf-reshape" class="headerlink" title="tf.reshape()"></a>tf.reshape()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.reshape(</span><br><span class="line">    tensor,</span><br><span class="line">    shape,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Given tensor, this operation returns a tensor that has the same values as tensor with shape shape.</p>
<p>If one component of shape is the special value -1, the size of that dimension is computed so that the total size remains constant. In particular, a shape of [-1] flattens into 1-D. At most one component of shape can be -1.　If shape is 1-D or higher, then the operation returns a tensor with shape shape filled with the values of tensor. In this case, the number of elements implied by shape must be the same as the number of elements in tensor.</p>
<p>demo</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(np.array(list(range(<span class="number">1</span>, <span class="number">9</span>+<span class="number">1</span>, <span class="number">1</span>))))</span><br><span class="line">b = tf.reshape(a, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># t: a (3 x 2 x 3) tensor</span></span><br><span class="line">t = tf.constant(</span><br><span class="line">    [[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">      [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]],</span><br><span class="line">     [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">      [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]],</span><br><span class="line">     [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">      [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]]</span><br><span class="line">)</span><br><span class="line">c = tf.reshape(t, [<span class="number">2</span>, <span class="number">-1</span>])</span><br><span class="line">d = tf.reshape(t, [<span class="number">-1</span>, <span class="number">9</span>])</span><br><span class="line">e = tf.reshape(t, [<span class="number">2</span>, <span class="number">-1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(b.get_shape().as_list())</span><br><span class="line">    print(sess.run(t), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(t.get_shape().as_list())</span><br><span class="line">    print(sess.run(c), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(c.get_shape().as_list())</span><br><span class="line">    print(sess.run(d), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(d.get_shape().as_list())</span><br><span class="line">    print(sess.run(e), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(e.get_shape().as_list())</span><br></pre></td></tr></table></figure>
<p>result</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[[ <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line">  [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span>]</span><br><span class="line">  [<span class="number">10</span> <span class="number">11</span> <span class="number">12</span>]]]</span><br><span class="line"></span><br><span class="line">[<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">[[[ <span class="number">1</span>  <span class="number">4</span>]</span><br><span class="line">  [ <span class="number">2</span>  <span class="number">5</span>]</span><br><span class="line">  [ <span class="number">3</span>  <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7</span> <span class="number">10</span>]</span><br><span class="line">  [ <span class="number">8</span> <span class="number">11</span>]</span><br><span class="line">  [ <span class="number">9</span> <span class="number">12</span>]]]</span><br><span class="line"></span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<h2 id="tf-split"><a href="#tf-split" class="headerlink" title="tf.split()"></a>tf.split()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.split(</span><br><span class="line">    value,</span><br><span class="line">    num_or_size_splits,</span><br><span class="line">    axis=<span class="number">0</span>,</span><br><span class="line">    num=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">&#x27;split&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Splits a tensor into sub tensors.</p>
<p>If num_or_size_splits is an integer type, num_split, then splits value along dimension axis into num_split smaller tensors. Requires that num_split evenly divides value.shape[axis]. If num_or_size_splits is not an integer type, it is presumed to be a Tensor size_splits, then splits value into len(size_splits) pieces. The shape of the i-th piece has the same size as the value except along dimension axis where the size is size_splits[i].</p>
<p>demo</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(np.reshape(list(range(<span class="number">1</span>, <span class="number">6</span>+<span class="number">1</span>, <span class="number">1</span>)), (<span class="number">2</span>, <span class="number">3</span>)))</span><br><span class="line">b1, b2, b3 = tf.split(a, <span class="number">3</span>, axis=<span class="number">1</span>)</span><br><span class="line">d1, d2, d3 = tf.split(a, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], axis=<span class="number">1</span>)</span><br><span class="line">c1, c2 = tf.split(a, [<span class="number">1</span>, <span class="number">2</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b1), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b2), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b3), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(d1), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(d2), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(d3), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(c1), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(c2), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">result</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>]</span><br><span class="line"> [<span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span>]</span><br><span class="line"> [<span class="number">5</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">3</span>]</span><br><span class="line"> [<span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>]</span><br><span class="line"> [<span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span>]</span><br><span class="line"> [<span class="number">5</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">3</span>]</span><br><span class="line"> [<span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>]</span><br><span class="line"> [<span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">6</span>]]</span><br></pre></td></tr></table></figure>

<h2 id="tf-transpose"><a href="#tf-transpose" class="headerlink" title="tf.transpose()"></a>tf.transpose()</h2><p>demo</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">     [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">)</span><br><span class="line">b = tf.transpose(a)</span><br><span class="line"><span class="comment"># equivaltently</span></span><br><span class="line">c = tf.transpose(a, [<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># x : 2 x 2 x 3</span></span><br><span class="line">x = tf.constant(</span><br><span class="line">    [[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">      [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]],</span><br><span class="line">     [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">      [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]]</span><br><span class="line">)</span><br><span class="line"><span class="comment"># perm is more useful for n-dimensional tersors, for n &gt; 2</span></span><br><span class="line"><span class="comment"># y : 2 x 3 x 2</span></span><br><span class="line">y = tf.transpose(x, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(c), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    print(sess.run(x), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(x.get_shape().as_list(), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(y), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(y.get_shape().as_list(), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>result</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[[ <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line">  [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span>]</span><br><span class="line">  [<span class="number">10</span> <span class="number">11</span> <span class="number">12</span>]]]</span><br><span class="line"></span><br><span class="line">[<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/13/Tensorflow%20API/" data-id="ckix721hq000quddlep8j4g7c" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Graph Neural Network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/03/Graph%20Neural%20Network/" class="article-date">
  <time datetime="2020-02-03T03:56:00.000Z" itemprop="datePublished">2020-02-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/03/Graph%20Neural%20Network/">Graph Neural Network</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h2><p>Graph neural network/Graph embedding code.  </p>
<ul>
<li>GNN: gcn, graphSAGE</li>
<li>GE: node2vec</li>
</ul>
<h2 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h2><div style="text-align: center;">
<img alt="" src="https://s3.ax1x.com/2020/11/15/DFYfqe.png" style="display: inline-block;" width="666"/>
</div>

<p>GCN: learn a function of signals/features on a graph <em>G = (V, E)</em> which takes as input:  </p>
<ul>
<li>A feature description <em>x<sub>i</sub></em> for every node i summarized in a <em>N x D</em> feature matrix <em>X</em>. (<em>N</em> : number of nodes, <em>D</em> : number of input features)</li>
<li>A representative description of the graph structure in matrix form, eg: adjacency matrix <em>A</em>.</li>
</ul>
<p>and produces a node-level output <em>Z</em> (an <em>N x F</em> feature matrix, where <em>F</em> is the number of output features per node). Graph-level outputs can be modeled by producing some form of pooling operation.</p>
<p>Every neural network layer can then be written as a non-linear function, consider the following simple form of a layer-wise propagation rule:  </p>
<p><img src="http://latex.codecogs.com/png.latex?H%5E%7B(l+1)%7D=f%5Cleft(H%5E%7B(l)%7D,A%5Cright)=%5Csigma%5Cleft(AH%5E%7B(l)%7DW%5E%7B(l)%7D%5Cright)" alt="gnn">  </p>
<p>with <em>H<sup>(0)</sup>=X</em> and <em>H<sup>(L)</sup>=Z</em> (<em>z</em> for graph-level outputs), <em>L</em> being the number of layers. The specific models then differ only in how <img src="http://latex.codecogs.com/png.latex?f%5Cleft(%5Ccdot,%7B%5Ccdot%7D%5Cright)" alt="f(⋅,⋅)">  is chosen and parameterized.</p>
<h2 id="Preference"><a href="#Preference" class="headerlink" title="Preference"></a>Preference</h2><ul>
<li><p>Semi-Supervised Classification with Graph Convolutional Networks<br><a target="_blank" rel="noopener" href="https://github.com/tkipf/gcn/">https://github.com/tkipf/gcn/</a> </p>
</li>
<li><p>Inductive Representation Learning on Large Graphs<br><a target="_blank" rel="noopener" href="https://github.com/williamleif/graphsage-simple">https://github.com/williamleif/graphsage-simple</a></p>
</li>
<li><p>node2vec: Scalable Feature Learning for Networks<br><a target="_blank" rel="noopener" href="https://github.com/aditya-grover/node2vec">https://github.com/aditya-grover/node2vec</a></p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/03/Graph%20Neural%20Network/" data-id="ckix721hh0008uddlbcvybzk8" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-The Illustrated Transformer" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/03/The%20Illustrated%20Transformer/" class="article-date">
  <time datetime="2020-02-03T03:56:00.000Z" itemprop="datePublished">2020-02-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/03/The%20Illustrated%20Transformer/">The Illustrated Transformer</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul>
<li>介绍博客链接关于Transformer以及论文《Attention is all you need》以加深对于transformer框架的理解；</li>
<li>Transformer在时序建模上效果在很多nlp任务上优于RNN，且不同于CNN卷积进行特征提取过程；</li>
</ul>
<h2 id="Transformer用于机器翻译任务"><a href="#Transformer用于机器翻译任务" class="headerlink" title="Transformer用于机器翻译任务"></a>Transformer用于机器翻译任务</h2><ul>
<li><p>Transformer的提出用于机器翻译任务；翻译句子时输入输出是两个句子，中间黑盒包括一个Encoder和一个Decoder，更一般的情况是多个Encoder和多个Decoder进行堆叠；</p>
</li>
<li><p>拆解开单个Encoder，其中是一个Self-Attention和一个Feed Forward层；拆解开单个Decoder，其中是一个Self-Attention、一个Enconder-Decoder Attention层和一个Feed Forward层；</p>
</li>
</ul>
<h2 id="Self-Attention-Layer-拆解"><a href="#Self-Attention-Layer-拆解" class="headerlink" title="Self-Attention Layer 拆解"></a>Self-Attention Layer 拆解</h2><ul>
<li><p>和常见NLP任务一样输入为单词的embedding，分别表示为；经过Self-Attention层后，分别输出为，再分别过各自的FFNN’</p>
</li>
<li><p>Attention过程首先对每个单词的embedding构造三个向量分别为Key、Query和Value；这三个向量是通过输入单词embedding分别乘以三个矩阵所得分别表示为，这三个矩阵在训练过程中得到；</p>
</li>
</ul>
<ul>
<li><p>Self-Attention过程如何实现？分解来看就是理解weight怎么得到，得到weight以后乘上去即可（Transformer里面是乘在value上），即理解如下过程；<br>  输入单词的embedding的query根据key点乘进行打分（维度归一化）</p>
<p>  =&gt; softmax之后得到一个weight</p>
<p>  =&gt; 对value乘以weight进行加权求和得到z</p>
</li>
</ul>
<ul>
<li>Self-Attention的计算过程可以用如下式子表示，其中理解Softmax(x)=weight</li>
</ul>
<p>上述过程描述了单个输入单词embedding生成self-attention后的表示的过程，在实际运算时通过矩阵运算来并行实现所有单词的self-attention过程；同时，上述实现了一个head的self-attention的过程，实际上可以设置多个head进行self-attention，再将多个head学习到的表示z进行concatenate，最后将这个宽表示乘以一个weight矩阵；</p>
<ul>
<li><p>截止目前Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子中词的顺序怎么打乱Transformer都会得到类似的结果，换句话说此时的Transformer只是一个功能更强大的词袋模型而已。为解决这个问题，Transformer编码词向量时引入了位置编码（Position Embedding）特征。那么如何编码位置信息呢？常见模式有：a. 根据数据学习；b. 自己设计编码规则。</p>
</li>
<li><p>另外Transformer在每个Encoder-Block里面，同时采用ResBlock的结构，另外采用了Layer Nomalization的方法进行配合使用；</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/03/The%20Illustrated%20Transformer/" data-id="ckix721hp000nuddlhq3526et" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-pickle dump OverflowError" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/03/pickle%20dump%20OverflowError/" class="article-date">
  <time datetime="2020-02-03T03:56:00.000Z" itemprop="datePublished">2020-02-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Programming-Data/">Programming & Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/03/pickle%20dump%20OverflowError/">pickle dump OverflowError</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<p>报错如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OverflowError: cannot serialize a bytes object larger than <span class="number">4</span> GiB</span><br><span class="line">解决方法</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.dump(d, open(<span class="string">&quot;file&quot;</span>, <span class="string">&#x27;w&#x27;</span>), protocol=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>官方文档</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Protocol version <span class="number">0</span> <span class="keyword">is</span> the original “human-readable” protocol <span class="keyword">and</span> <span class="keyword">is</span> backwards compatible <span class="keyword">with</span> earlier versions of Python.</span><br><span class="line">Protocol version <span class="number">1</span> <span class="keyword">is</span> an old binary format which <span class="keyword">is</span> also compatible <span class="keyword">with</span> earlier versions of Python.</span><br><span class="line">Protocol version <span class="number">2</span> was introduced <span class="keyword">in</span> Python <span class="number">2.3</span>. It provides much more efficient pickling of new-style classes. Refer to PEP <span class="number">307</span> <span class="keyword">for</span> information about improvements brought by protocol <span class="number">2.</span></span><br><span class="line">Protocol version <span class="number">3</span> was added <span class="keyword">in</span> Python <span class="number">3.0</span>. It has explicit support <span class="keyword">for</span> bytes objects <span class="keyword">and</span> cannot be unpickled by Python <span class="number">2.</span>x. This <span class="keyword">is</span> the default protocol, <span class="keyword">and</span> the recommended protocol when compatibility <span class="keyword">with</span> other Python <span class="number">3</span> versions <span class="keyword">is</span> required.</span><br><span class="line">Protocol version <span class="number">4</span> was added <span class="keyword">in</span> Python <span class="number">3.4</span>. It adds support <span class="keyword">for</span> very large objects, pickling more kinds of objects, <span class="keyword">and</span> some data format optimizations. Refer to PEP <span class="number">3154</span> <span class="keyword">for</span> information about improvements brought by protocol <span class="number">4.</span></span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/02/03/pickle%20dump%20OverflowError/" data-id="ckix721hx0019uddlcxmjafrl" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Tricks in Data Mining Competitions" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/11/Tricks%20in%20Data%20Mining%20Competitions/" class="article-date">
  <time datetime="2020-01-11T12:11:00.000Z" itemprop="datePublished">2020-01-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/11/Tricks%20in%20Data%20Mining%20Competitions/">Tricks in Data Mining Competitions</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="Feature-Engineering-Basic"><a href="#Feature-Engineering-Basic" class="headerlink" title="Feature Engineering Basic"></a>Feature Engineering Basic</h2><h3 id="Discretizer-Bin-continuous-data-into-intervals"><a href="#Discretizer-Bin-continuous-data-into-intervals" class="headerlink" title="Discretizer, Bin continuous data into intervals"></a>Discretizer, Bin continuous data into intervals</h3><ul>
<li>Uniform – All bins in each feature have identical widths.</li>
<li>Quantitle – All bins in each feature have the same number of points.</li>
<li>Kmeans – Values in each bin have the same nearest center of a 1D k-means cluster.</li>
</ul>
<h3 id="Binazier-Encode-categorical-features"><a href="#Binazier-Encode-categorical-features" class="headerlink" title="Binazier, Encode categorical features"></a>Binazier, Encode categorical features</h3><ul>
<li>OneHotEncoder, OrdinalEncoder, HashMap, etc.</li>
</ul>
<h3 id="Scaler"><a href="#Scaler" class="headerlink" title="Scaler"></a>Scaler</h3><h5 id="Linear-Scaler"><a href="#Linear-Scaler" class="headerlink" title="Linear Scaler"></a>Linear Scaler</h5><ul>
<li>Min-Max, Maxabs, Nomalize, Robust（QR，clip with quantile range）</li>
</ul>
<h5 id="Non-Linear-Scaler"><a href="#Non-Linear-Scaler" class="headerlink" title="Non-Linear Scaler"></a>Non-Linear Scaler</h5><ul>
<li><p>Gaussian-like Scaler.<br>  Box-Cox/Yeo-Johnson<br>  RankGauss.</p>
</li>
<li><p>XX-like Scaler<br>  Use icdf of target distribution to transform (cdfX)</p>
</li>
<li><p>Non-linear transforms such as Box-Cox may be userful to transform y. Diversity for ensemble, sometimes with better performance.</p>
</li>
<li><p>Yeo-Johnson transform</p>
</li>
<li><p>Linear transform. Like standard normalized work better for X.</p>
</li>
</ul>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>Learn lookup table as category representation<br>Category Embedding/ Entity Embedding</p>
<h5 id="Numerical-Embedding"><a href="#Numerical-Embedding" class="headerlink" title="Numerical Embedding"></a>Numerical Embedding</h5><ul>
<li>Bin numerical data into intervals.</li>
<li>as category embedding</li>
<li>add vicinal info</li>
</ul>
<h2 id="Key-–-Value-Based"><a href="#Key-–-Value-Based" class="headerlink" title="Key – Value Based"></a>Key – Value Based</h2><ul>
<li>df[A].groupby(B).aggregate(func) Func can be anything, F(subgroupby(A)) -&gt; x</li>
<li>Common aggregate func: mean, std, skew, kurt, entropy, min, max, median, frequency, size etc.</li>
<li>Residues and weights of Least-squares fitting works well in sequential value.</li>
<li>High order interaction, groupby().groupby()</li>
</ul>
<h2 id="Latent-Representation"><a href="#Latent-Representation" class="headerlink" title="Latent Representation"></a>Latent Representation</h2><p>Use decomposition algorithm to extract latent features.  Works better in neural network than GBDT.  Just fit all handcrafted feature with different decomposition algorithm.</p>
<ul>
<li>Topic Models (LDA, SVD, NMF etc).</li>
<li>Manifold Learning (T-SNE)</li>
</ul>
<h2 id="Symbolic-Learning"><a href="#Symbolic-Learning" class="headerlink" title="Symbolic Learning"></a>Symbolic Learning</h2><p>Using symbolics ( + - * / cos sin tan ^ log min max neg…) as functions set to represent a relationship.</p>
<ul>
<li>Total different interaction feature.</li>
<li>Automated feature learning.</li>
<li>Works in many natural scene, such as astronomical, geological, biological tasks.</li>
</ul>
<h2 id="Auto-Encoders"><a href="#Auto-Encoders" class="headerlink" title="Auto Encoders"></a>Auto Encoders</h2><ul>
<li>Learn to construct clean samples from corrupted ones. (denoise)</li>
<li>Regularization of the latent space to match a prior. (Always multivariate normal distribution)</li>
</ul>
<h2 id="Graphs"><a href="#Graphs" class="headerlink" title="Graphs"></a>Graphs</h2><ul>
<li>FeatureA – FeatureB – aggregate. Everything can construct graph.</li>
<li>Maximize the likelihood of preserving newwork neighborhoods of nodes.</li>
<li>Unsupervised learning methods: Deepwalk, node2vec.</li>
<li>Semi or supervised learning: GCN.</li>
<li>Deepwalk, node2vec work in many tasks, like CTR, Fraud detection</li>
</ul>
<h2 id="End2End-model"><a href="#End2End-model" class="headerlink" title="End2End model"></a>End2End model</h2><ul>
<li>Online  learning models, FTRL, mini-batch fm, etc.</li>
<li>DeepFM, DeepFFM,xDeepFM etc.</li>
<li>Attention based interaction model.</li>
</ul>
<h2 id="Target-mean-Encoding"><a href="#Target-mean-Encoding" class="headerlink" title="Target mean Encoding"></a>Target mean Encoding</h2><ul>
<li>Leave-one-out /K-Folds / history slide window schema to avoid overfitting</li>
<li>Post-prior schema</li>
</ul>
<h2 id="Model-based-target-encoding"><a href="#Model-based-target-encoding" class="headerlink" title="Model based target encoding"></a>Model based target encoding</h2><ul>
<li>Base model to predict target, with k–folds, user prob as features.</li>
<li>Base model to predict residuals (target - probs)</li>
<li>Sometimes, user level-N stacking probs works better.</li>
</ul>
<h2 id="Training-amp-Optimization-Tricks"><a href="#Training-amp-Optimization-Tricks" class="headerlink" title="Training &amp; Optimization Tricks"></a>Training &amp; Optimization Tricks</h2><h5 id="Common-Tricks"><a href="#Common-Tricks" class="headerlink" title="Common Tricks"></a>Common Tricks</h5><ul>
<li>Learning rate schedule. SGD lr=0.1, BS=256, lr * bs / 256= 0.1 * 256</li>
<li>Label smoothing.</li>
<li>No bias decay.</li>
<li>Bias initial with prior distribution.</li>
<li>Focal loss is a good metric for estimating models’capabilities.</li>
<li>Combine multi loss let the model converge rapidly towards better performance.</li>
<li>User standart scaler transform First.</li>
<li>Sometimes userful, but all depends on data.</li>
</ul>
<h5 id="NLP-task-traning-Tricks"><a href="#NLP-task-traning-Tricks" class="headerlink" title="NLP task traning Tricks"></a>NLP task traning Tricks</h5><ul>
<li>Diverse pre-trained embeddings</li>
<li>more than 90% of a model’s complexity resides the embedding layer.</li>
<li>Contextual level embedding always helps.</li>
<li>But now we have BERT.</li>
<li>Dynamic Padding boosts training speed and model performance.</li>
<li>Add test oov vocabs to embedding matrix.</li>
<li>OOV replaced with“something”</li>
<li>Adamw really works in many NLP tasks.</li>
<li>Spatial Droupout, shuflle noise in classification task.</li>
<li>Translation Aumentation. En-Fr, En-Fr-En.</li>
</ul>
<h5 id="CV-task-training-Tricks"><a href="#CV-task-training-Tricks" class="headerlink" title="CV task training Tricks"></a>CV task training Tricks</h5><ul>
<li>ResNet is always a good baseline model to do experiments.</li>
<li>Warm-up learning rate + cos learnging rate decay.</li>
<li>Manual Learning rate schedule.</li>
<li>Batch with different loss accumulate.</li>
<li>User more shape relevance augmentation.</li>
<li>CNN tend to learn texture information but not shape info.</li>
<li>Auto augmentation.</li>
</ul>
<h5 id="Pseudo-Learning"><a href="#Pseudo-Learning" class="headerlink" title="Pseudo Learning"></a>Pseudo Learning</h5><ul>
<li>Filtering hard examples in training dataset use OOF prediction.</li>
<li>Global variance info leakage.</li>
<li>Add the most confident test predictions to the training dataset.<br>Test distribution leakage, data extend<br>Simple in Batch Traning Models.<br>User single model to label test dataset.<br>Retrain model with each batch add 10~30% test pseudo labeled data.</li>
</ul>
<h2 id="TIPS"><a href="#TIPS" class="headerlink" title="TIPS"></a>TIPS</h2><ul>
<li>逻辑性（代码架构清晰、自动化框架、特征工程MECE）</li>
<li>代码可复用（不重复造轮子）</li>
<li>基于业务的分析（bad case 分析、Kernels、Discussion、相似比赛）</li>
<li>反思总结</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/01/11/Tricks%20in%20Data%20Mining%20Competitions/" data-id="ckix721hs000wuddl8vsd0ovt" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Word2Vec" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/11/Word2Vec/" class="article-date">
  <time datetime="2020-01-11T12:11:00.000Z" itemprop="datePublished">2020-01-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/11/Word2Vec/">Word2Vec</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="Core-idea"><a href="#Core-idea" class="headerlink" title="Core idea"></a>Core idea</h2><p>How to define a word (presentation) meaning ? A word’s meaning is given by the words that frequently appear close-by.</p>
<center>“You shall know a word by the company it keeps” (J. R. Firth 1957: 11).</center>

<p>One of the most successful ideas of modern statistical NLP!</p>
<div style="text-align: center;">
<img alt="" src="https://i.loli.net/2020/09/20/Ca1YdUvotWESinT.jpg" style="display: inline-block;"/>
</div>


<h5 id="Training-samples-Construction"><a href="#Training-samples-Construction" class="headerlink" title="Training samples Construction"></a>Training samples Construction</h5><div style="text-align: center;">
<img alt="" src="https://i.loli.net/2020/09/20/eMyuIgNJtYCKA4T.jpg" style="display: inline-block;"/>
</div>

<h5 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h5><div style="text-align: center;">
<img alt="" src="https://s1.ax1x.com/2020/09/20/wo7n76.jpg" style="display: inline-block;"/>
</div>

<p>和都是生成的向量，每个词均有2个向量表示；通常将距离中心词较近的Matrix作为词向量；</p>
<h5 id="Where-is-word-presentation"><a href="#Where-is-word-presentation" class="headerlink" title="Where is word presentation?"></a>Where is word presentation?</h5><h5 id="Subsampling-of-Frequent-Words"><a href="#Subsampling-of-Frequent-Words" class="headerlink" title="Subsampling of Frequent Words"></a>Subsampling of Frequent Words</h5><p>“in”, “the”, “a”  这类词出现很多但是几乎无法提供信息，但是常和语料中关键词共现；<br>上述高频词在构造训练样本中占据了很大比例；</p>
<p>Subsampling: 对于每个词训练时以如下概率丢弃，其中为词在语料中出现频率；t为超参数（Mikolov论文设置为10-5）；</p>
<p>即相当于以如下概率保留样本；</p>
<p>上述函数具有以下性质</p>
<p> (100% chance of being kept) when . This means that only words which represent more than 0.26% of the total words will be subsampled.<br> (50% chance of being kept) when .<br> (3.3% chance of being kept) .</p>
<p>Negative Sampling<br>目标函数将替换为如下函数，核心思想是从某个noise distribution   进行负采样，然后通过logistic regression转化为二分类问题；</p>
<p>是每次采样中负样本的个数：在较小的数据集上设置为[5, 20]之间；在较大数据集上设置为[2, 5]之间;<br>可采用unigram distribution，公式如下所示，本质上是对负样本根据词频进行加权，即词频更高的词更有可能被选择为负样本；</p>
<p>Mikolov修改该采样分布为3/4次幂时比原始的unigram distribution效果提升明显；</p>
<p>negative sampling的效果：显著提升训练速度，同时也提升了词向量质量；<br>为什么以unigram distribution的方式采样会提升词向量的质量？因为语言本身有个规律：句子中越高频的词(例如it, and, …,)这些词对于刻画句子的含义作用越小；</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/01/11/Word2Vec/" data-id="ckix721hu0011uddlgj2r8oim" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Yoshua Bengio对ML研究者的建议" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/11/Yoshua%20Bengio%E5%AF%B9ML%E7%A0%94%E7%A9%B6%E8%80%85%E7%9A%84%E5%BB%BA%E8%AE%AE/" class="article-date">
  <time datetime="2020-01-11T12:11:00.000Z" itemprop="datePublished">2020-01-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Always-Review/">Always Review</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/11/Yoshua%20Bengio%E5%AF%B9ML%E7%A0%94%E7%A9%B6%E8%80%85%E7%9A%84%E5%BB%BA%E8%AE%AE/">Yoshua Bengio对ML研究者的建议</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      	<!-- Table of Contents -->
		
        <hr>
<h2 id="Quora原始问题"><a href="#Quora原始问题" class="headerlink" title="Quora原始问题"></a>Quora原始问题</h2><p>Originally Answered: Yoshua Bengio: What advise do you have for young researchers entering the field of Machine Learning?</p>
<p>对话Yoshua Bengio：对年轻机器学习领域的研究者的建议？</p>
<p>Make sure you have a strong training in math and computer science (including the practical part, i.e., programming). Read books and (lots of) papers but that is not enough: you need to develop your intuitions by </p>
<p>(a) programming a bunch of learning algorithms yourself, e.g., trying to reproduce existing papers, and </p>
<p>(b) by learning to tune hyper-parameters and explore variants (in architecture, objective function, etc.), e.g., by participating in competitions or trying to improve on published results once you have been able to reproduce them. Then, find collaborators with whom you can brainstorm about ideas and share the workload involved in exploring and testing new ideas.  Working with an existing group is ideal, of course, or recruit your own students to work on this with you, if you are a faculty.</p>
<p>四个核心能力</p>
<ul>
<li>确保数学和计算机学科扎实训练（包括实践部分，即编程)</li>
<li>阅读书籍和很多论文</li>
<li>在阅读论文的基础上（a）自己编写一堆学习算法来发展自己的直觉，例如尝试复制现有的论文，（b）通过学习调参和探索变体（在模型结构，目标函数等），例如通过参与竞赛或尝试改进已发表的结果，一旦你能够重现它们。</li>
<li>找到小伙伴与他们头脑风暴，分享探索和测试新想法的相关工作。如果是教师，与现有团队合作当然是很理想的，或者招募学生与一起工作。</li>
</ul>
<h2 id="Preference"><a href="#Preference" class="headerlink" title="Preference"></a>Preference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.quora.com/bookmarked_answers">https://www.quora.com/bookmarked_answers</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/01/11/Yoshua%20Bengio%E5%AF%B9ML%E7%A0%94%E7%A9%B6%E8%80%85%E7%9A%84%E5%BB%BA%E8%AE%AE/" data-id="ckix721hv0015uddl6t7h6q77" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Always-Review/">Always Review</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-Data/">Programming & Data</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading/">Reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recsys-Ads-Tech/">Recsys & Ads Tech</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/12/20/Shoe%20Dog/">Shoe Dog</a>
          </li>
        
          <li>
            <a href="/2020/12/16/Bert/">Bert</a>
          </li>
        
          <li>
            <a href="/2020/11/08/The%20essence%20of%20the%20RecSys/">The essence of the RecSys</a>
          </li>
        
          <li>
            <a href="/2020/10/01/Hexo%E6%90%AD%E5%BB%BA/">Hexo搭建</a>
          </li>
        
          <li>
            <a href="/2020/09/14/Item-Based%20CF/">Item-Based CF</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Gold and Rabbit<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>