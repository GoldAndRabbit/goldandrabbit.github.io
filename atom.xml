<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gold and Rabbit&#39;s Blog</title>
  
  <subtitle>Always ambitious</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-02-18T10:30:48.138Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Gold and Rabbit</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020 Annual Summary</title>
    <link href="http://example.com/2020/12/31/2020%20Annual%20Summary/"/>
    <id>http://example.com/2020/12/31/2020%20Annual%20Summary/</id>
    <published>2020-12-31T15:11:00.000Z</published>
    <updated>2021-02-18T10:30:48.138Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="年度关键词"><a href="#年度关键词" class="headerlink" title="年度关键词"></a>年度关键词</h2></blockquote><ul><li>今年给自己的年度关键词是: <br/><center><font color="red" size="6"><strong>拥抱长期主义</strong></font></center></li></ul><blockquote><h2 id="工作内容总结"><a href="#工作内容总结" class="headerlink" title="工作内容总结"></a>工作内容总结</h2></blockquote><h5 id="Guide-Purchase-CVR"><a href="#Guide-Purchase-CVR" class="headerlink" title="Guide Purchase CVR"></a>Guide Purchase CVR</h5><ul><li>通用场景的CVR预估的意义. CVR模型在排序阶段<strong>会将全用户平均意义上发生转化可能性最高的物料排序</strong>, 而不是对于某个用户来说某个实时性兴趣的一种.</li><li>CVR平衡机制<ol><li>CTR是CVR的一种基础保证.在导购或者以内容为主的场景的一道基础保证, 如果缺少CTR或者其他相关性保证机制, 那么高转化无从谈起.</li><li>转化只是一种结果, 而用户的体感却是方方面面的. CVR模型上线之后, 可能线上会有直接的提升, 但是还需要辅助改进一些用户体验上的策略. 比如很多高CVR的商品其实都是十分低价的, 仔细想想, 这是我们场景现阶段所需要的吗? 简而言之, 提升场景效果实则不难; 改善用户体验确实长久需要做的事情.</li></ol></li></ul><h5 id="1111-TLJ-Delay-FeedBack-amp-Presale"><a href="#1111-TLJ-Delay-FeedBack-amp-Presale" class="headerlink" title="1111 TLJ Delay FeedBack &amp; Presale"></a>1111 TLJ Delay FeedBack &amp; Presale</h5><p>大促时期是用户兴趣得到充分释放的购物时间区间, 我们需要把整个场景优化目标放眼到整个大促周期, 从”预热期”=&gt;”预售期”=&gt;”爆发期”三个阶段存在一定的相对独立性, 因此建模需要具备时间全局意识, 让每个时期的特点更鲜明的凸显出来, 就是算法核心的目标</p><ul><li><strong>预售</strong>: 种草商品的集中体现, 商品的成交笔单价有显著的爆发, 尤其在大家电/电子设备/鞋类/高价美妆类目, 这些商品的折扣额度在这个时期内是”全年最低”, 因此推荐结果一定要从充分打满商品品牌和档次, 通过提升客单价的路径来契合用户的种草以久带来的爆发.</li><li><strong>爆发</strong>: 爆发期相对来说混合了较多的因素, 包括平台最后时刻释放的临时红包、活动以及很多场外因素(比如晚会临时带来的冲动行消费), 在爆发期具备时间短, 行为密集而丰富的主要特点, 因此这里我的设计原则是基于重定向的思路召回, 重点关注实时性需求的召回, 模型复杂度要求反而可能不需要太高. 在效果实施上有一个注意点在于一定要锁定爆发期的开始时间, 在流量迎来峰值的时候具备稳妥的变更方案, 保证线上稳定的过渡.</li><li><strong>预热</strong>: 相对于以上两个时期, 用户剁手心智会极度热化, 因此需要将推荐结果和以上两类行为进行显式的区分, 具体的形式需要结合产品的形式来确定, 以抽奖、做任务的形式来看, 需要结合相应的权益形式或者营销形式制定算法方案. </li></ul><h5 id="User-Growth"><a href="#User-Growth" class="headerlink" title="User Growth"></a>User Growth</h5><ul><li>增长和留存: 在很多场景是同一个目标的不同表达. 用户增长第一验证指标: 第n天留存率和n天留存率, 其中n可以等于1, 7, 14, 30. 其中最重要的是n=30.</li><li>由于项目还没有正式开始需要进一步深耕, 沉淀更有价值的方法论.</li></ul><blockquote><h2 id="技术总结"><a href="#技术总结" class="headerlink" title="技术总结"></a>技术总结</h2></blockquote><h5 id="Deep-Rank"><a href="#Deep-Rank" class="headerlink" title="Deep-Rank"></a>Deep-Rank</h5><ul><li>完成总结feature interaction模型的相关体系, 并且用tf.estimator结合functional api实现. </li><li>deep ctr边际效应递减. 但对于一个推荐系统来说, deep model 只能影响到排序的一个因素, 在大流量场景可能比较明显. 但在很多时候deep ctr模型边际效应较小且递减明显. 尤其在小流量没有形成业务闭环的场景, 突破重点更需要一种产品或者运营的思维去看待, 简而言之, 快刀斩乱麻当断则断, 投入到边际效应更大的目标上, 带着ROI去推进目标.</li></ul><h5 id="Match-MTL-Graph"><a href="#Match-MTL-Graph" class="headerlink" title="Match/MTL/Graph"></a>Match/MTL/Graph</h5><ul><li>加深item-cf理解. 在梳理和落地一些match方法并且看了大量的bad case之后, 尤其加深了item-cf这种方法的理解, 虽然现在进入深度召回时代, 但能够彻底地item-cf的理解我认为是提升对推荐系统理解相当有效的路径, 宏观来看, 方法本身没有对错高低之分, 关键还是在于用方法的人. </li><li>Graph. Graph从一开始就为我们建立起来一种理解数据的视角, 这点当然也是follow学术界的一个正常反应.<br>但从目前的落地的情况来, 可能还是定位于一种锦上添花的位置, 并非一种可以拿来即用的屠龙之术. 因为实际业务使用图学习的场景是动态且复杂的. 例如图的实时更新, 选择怎样的几组关系来进行抽象, 工程师需要经过比较高的迭代成本去迭代和试错. 个人感觉ROI不一定很高. 希望在2021有更多能够提升落地层次的技术应用出现. </li></ul><h5 id="Pretrain-Model"><a href="#Pretrain-Model" class="headerlink" title="Pretrain Model"></a>Pretrain Model</h5><ul><li>跟进预训练模型的进展, 这部分也仅停留在bert这个项目上面, 没有进行太多的技术扩展. </li><li>表示学习, 绝对可扩展到更大的场景. 从业务场景的角度来看, 预训练模型在很多业务场景上还是广阔天地大有可为, 比如2020 tencent ad比赛. 本质还是一个表示学习/文本分类问题. 结合用户画像业务, 理论上可以实现比很多传统方法更优的突破. </li></ul><blockquote><h2 id="工作方法论"><a href="#工作方法论" class="headerlink" title="工作方法论"></a>工作方法论</h2></blockquote><h5 id="搭建体系"><a href="#搭建体系" class="headerlink" title="搭建体系"></a>搭建体系</h5><ul><li>搭建体系是今年方法论上的最重要的认识改变, 一个体系可以是工作或者生活中的任意一个内容, 先不用考虑太复杂的细节, 明确主要做的3-4个方面. <ol><li>比如从推荐系统的技术角度来看, 首先有一个大的pipeline体系, 分为物料/召回/排序/策略, 每个部分由可单独拆分一个体系去展开来优化. </li><li>扩大到对于一个产品的视角, 比如对于某个tk产品来说 用户增长/导购属性/工具属性, 可能是重点的三个发展的体系.</li></ol></li></ul><h5 id="质量保证体系"><a href="#质量保证体系" class="headerlink" title="质量保证体系"></a>质量保证体系</h5><ul><li>建立一种不以人的意志为依赖的质量保证体系. 简单来说分为两部分. <ol><li>重点项目指定唯一负责PM, PM全局精通, 对所有问题负责.</li><li>维护checklist, 跟进checklist在项目迭代周期内所有的状态.</li></ol></li></ul><blockquote><h2 id="2021年工作-学习需要改进-深入的点"><a href="#2021年工作-学习需要改进-深入的点" class="headerlink" title="2021年工作/学习需要改进/深入的点"></a>2021年工作/学习需要改进/深入的点</h2></blockquote><ul><li><strong>站出来, 往前冲</strong>. 在工作中, 仍然需要具备更强的owner意识, 这种意识2020年有一定提升, 但是还不够. 需要更主动的承担对问题把握. </li><li><strong>插一句, 再插一句</strong>. 在开会讨论/团队讨论中, 需要更多发表对问题的见解, 在2021年尤其需要关注的是同伴的工作, 同事的工作, 和自己kpi无关的一些内容, 大胆质疑和挑战, 提供自己的见解, 不怕在意的眼光. 只要自己不尴尬, 尴尬的就不是自己. </li><li><strong>Serving服务待打通</strong>. 由于ali基建和造轮子的能力, 日常工作中对于技术的底层原理往往是不求甚解, 会用就行. 在2021年需要对底层技术提出更深入的要求；从目标来看需要建设好如何把推荐系统从打通数据闭环到提供线上稳定Serving的pipeline.</li></ul><blockquote><h2 id="年度宏观思考"><a href="#年度宏观思考" class="headerlink" title="年度宏观思考"></a>年度宏观思考</h2></blockquote><h3 id="用50-的感性决策"><a href="#用50-的感性决策" class="headerlink" title="用50%的感性决策"></a>用50%的感性决策</h3><ul><li>至少用50%的感性做关键决策: 大胆地用感性来决策. 世界上大多数东西看起来似乎都是能被量化继而决策的, 尤其是对算法工程师或者数据从业者而言. 我们总是遵循这种模式 数据反应A群体具备B占比的C行为特点, 因此我们针对C采用方案D指导A, 这种模式听起来是没什么错误的, 但是现实中很多问题并不是通过历史统计, 或着某一种分析框架下就能解决的. </li></ul><p>例如</p><ol><li><p>2020武汉封城. cov19是新病毒历史上从来没有出现过, 当时谁也没有有效的治疗手段, 也不知道扩散速度和这种病的致病因子到底是什么. 每个人都在一种恐慌的情绪下, 那么问题来了?如何应对? 如果按照上述模式, 简单来说: 完全没有ABC的定义, 请出台方案D.  但是大胆选择了封城、全员戴口罩、严查出入、健康码、钉钉上课这样的一些列手段, 事实证明是非常正确的, 那么深入想想有个问题, 当时你100%充分的理由去解决该问题吗? 我认为没有, 因为上述决策, 并不完全是现有的、可以拿来就用的方案、而是我们根据一种相对模糊的经验、一种并不能完全看清全局的判断做出的决定. </p></li><li><p>抗美援朝战役. 不同于抗日战争. 对手是美国人, 且作战的主场在朝鲜. 美国并没有深入的打到你的领土上, 我们也没有和美国人在战场上证明打过, 美国兵是可以战胜的吗?他们的武器那么先进经过二战洗礼, 海陆空配合一体, 我们能打得过吗?有多少胜算?输了怎么办?ROI如何. 当时的领导人我认为也无法在做完决策时之前就说出答案. 上述这些考虑在当时的情况下是不可能完全看清的. 但仍然选择了义无反顾的抗美援朝, 保家卫国. 事实证明又一次采取了正确的选择. </p></li><li><p>选择学校和工作. 从以往的经历来看, 我在选择学校和工作时理由往往是十分充分的, 能列出很多理由说服自己为什么选择bupt和ali, 但事实上很多选择的坑在当时是看不出来的, 也是不可能看出来的, 只是在我当时视野下找到一个局部最优, 而不是全局最优. </p></li></ol><p>结论:  在今后的决策中, <strong>我打算放弃类似全盘理性（其实根本做不到, 因为好多变量你无法提前计算）, 而是采取50%感性的50%理性的方式去做关键决策.</strong> 说起来这个思想有点类似于dropout, 我为了拟合一个确定的目标, 在我已有的样本上疯狂的降低loss, 也只是在你那已有的样本上做这件事情, 不妨想想样本真的够吗? 样本质量真的够吗?样本多样性真的够吗?</p><h3 id="对等价值量化与长期主义"><a href="#对等价值量化与长期主义" class="headerlink" title="对等价值量化与长期主义"></a>对等价值量化与长期主义</h3><h5 id="懂得对等价值才会影响做事的方式"><a href="#懂得对等价值才会影响做事的方式" class="headerlink" title="懂得对等价值才会影响做事的方式."></a>懂得对等价值才会影响做事的方式.</h5><ol><li>平时周末花200元去吃火锅. 假如以余额宝这种货币基金2.5%平均年化计算, 需要10k存一年才能达到. 这个钱允许被我花掉, 但是也得想清楚, 这同时意味着用多少钱在别人的手里保存, 承担一定的风险才能达到同等的收益.</li><li>12月份我用了前3周, 每周跑量平均20km, 体重基本上稳定下降了1kg, 60km的跑量虽然也算不上太多, 但是相比这个1kg, 却是一种对等的概念. 同时每次跑6-8km的热量, 等同于也就是一个汉堡王天椒皇堡的概念.  但为了消耗这一个汉堡, 我得在跑步机上用45-55min时间对抗无聊感. 即使目前我对跑步这项运动非常有兴趣, 但是仍然这个时间是需要用毅力支撑并非全盘享受的.</li></ol><h5 id="长期主义"><a href="#长期主义" class="headerlink" title="长期主义"></a>长期主义</h5><p>量化一种时间价值. 2020另一个思考是通过学习货币金融学课程非常详细的理解了利率、复利等金融概念, 这些概念其实在日常生活中也是经常听, 但是并没有把这种概念变成自己做事、长期做事的一种日常化的态度和本能行为动机. 在2021年, 需要将长期主义这种态度融入到日常的工作、理财和做事当中去. 包括但不限于以下几个方面  </p><ol><li>推荐系统技术打通. 推荐系统从小里面说可以实现一个几十行的cf也是, 但是到一个成熟的工业级推荐系统需要非常完善的技术体系和质量保证体系.<br>比如数据生产, 实时特征和实时样本, 多维度评价体系, 复杂模型线上Serving, 实时监控以及效果保证.</li><li>长期跑步. 2020年5月, 跑奥森南远跑了2.3km已经完全跑不动了.9月份开始重启跑步, 10月份42分钟完成南园5km, 12月初36min完成北园5km, 12月底1h34min完成了南北10km(状态不佳实际速度比这个快), 等到2021年12月31日, 我想看看自己能达到怎样的水平.<div align="center"><img src="https://s3.ax1x.com/2021/01/03/s9qBzd.jpg" width="700" /></div>     <center> 图 奥森10km完赛 </center></li><li>理财. 目前还是个小白, 暂不赘述.   </li><li>Blog &amp; Github. 作为一个技术人, 对于技术的积累和业务的思考是一直有的，通过Blog和Github将所有的东西归纳在一起, 回过头来看自己还是很有意义. 今年完成了初步的搭建和主要的体系梳理, 2021年需要在重点方向上深入, 养成长期总结和汇总的好习惯.<div align="center"><img src="https://s3.ax1x.com/2021/01/03/s9O5bq.png" width="700" /></div><center> 图 Blog </center>    </li></ol><div align="center"><img src="https://s3.ax1x.com/2021/01/03/s9O4rn.png" width="700" /></div> <center> 图 Github </center>    <h5 id="另一种获得成功的价值认同方式-时间-gt-距离-gt-效率"><a href="#另一种获得成功的价值认同方式-时间-gt-距离-gt-效率" class="headerlink" title="另一种获得成功的价值认同方式: 时间&gt;=距离&gt;=效率"></a>另一种获得成功的价值认同方式: 时间&gt;=距离&gt;=效率</h5><ol><li>Hiit. 因为疫情影响, 年初一个人在家办公经历了太孤独的一段时间, 商店不开门, 门口超市都不太敢进去. 尝试了下HiiT L3难度的站立挑战, 发现全套动作认真做起来还是真不容易. 但是Hiit对于场地完全没有要求, 燃脂效率也非常高, 是一项值得长期做的运动. 并且发现了一种适合自己的方法, 以L3站立挑战来说, 1组13min保证100%完成效果远远不如重复做2组每组80%的效果好.</li><li>ASICS. 入坑跑步, 有一次跑完之后发现脚疼, 缓了1个多星期才恢复. 于是狠下心来买了价值1390不打折的ASICS kayano 27于是狠下心来买了价值1390不打折的ASICS KAYANO 27. 品牌名来自拉丁语格言”anima sana in corpore sano”的首字母缩写, 意为<strong>健全的精神寓于强健的体魄</strong>, 这种理念随着年龄增长我是越来越认同的.  我是不相信一个男性没有野性、没有粗犷的体魄(至少认同这种体魄的价值)只靠着聪明的大脑能够干出丰功伟绩. 尤其是看了广马施一公首次完赛, 以及日本本土马拉松惊人的水平, 决定向优秀的日本跑者看齐.</li></ol><div align="center"><img src="https://s3.ax1x.com/2020/12/28/r7VLoF.png" width="700" /></div><center> 图 KAYANO 27 </center>    <h5 id="智者务其实-愚者图虚名"><a href="#智者务其实-愚者图虚名" class="headerlink" title="智者务其实, 愚者图虚名"></a>智者务其实, 愚者图虚名</h5><ul><li>这句话是新三国中司马懿的一段话, 新版三国我最佩服的还是司马懿, 孔明在三国演义中是顶峰存在，但是仍然在关键时候存在过于谨慎的特点, 比如子午谷不敢出兵, 司马懿策略层面略逊于孔明，但是在宏观韬略上面却要胜出孔明一筹.<div align="center"><img src="https://s3.ax1x.com/2021/01/03/s9qdiD.png" width="700" /></div><center> 图 司马懿告诫曹丕 </center></li></ul><blockquote><h2 id="足球"><a href="#足球" class="headerlink" title="足球"></a>足球</h2></blockquote><h5 id="拜仁8-2巴萨"><a href="#拜仁8-2巴萨" class="headerlink" title="拜仁8-2巴萨"></a>拜仁8-2巴萨</h5><ul><li><strong>光环掩盖问题</strong>. 看球以来可能是最让人难受的一场结局, 刚得知新闻一瞬间感觉梅西真应该去年踢完欧冠就退役. 但仔细想想, 还是由于巴萨本身实力不足, 加上梅西个人具有光环的表现使得整个团队看起来是成功的团队, 掩盖了过多的问题. </li><li><strong>三个投资, 一个成功</strong>. 从拜仁的成功来看, 有一个因素是青年人才快速崛起, 印象比较深的阿方索戴维斯, 格雷茨卡, 格纳布里, 似乎三条线上都有了非常强的人才补强. 反观巴萨和皇马就不尽如人意了, 不过巴萨皇马真的是前几年没有布局吗?我认为绝对不是, 巴萨皇马在15年之后都有非常充分的人才培养, 每个年轻球员至少是在当时是绝对看起来充满潜力的. 但是为什么失败呢?我认为这涉及到了足球的一点本质: 球员还是会归结于一种投资, 这种投资本身是带有失败的概率. 只不过在很多因素作用下这种失败或者说不成功没有被放大凸显出来, 欧冠拜仁巴萨这场将这种失败完全地放大了. 因此足球本质上还是一笔非常难做的生意. </li></ul><blockquote><h2 id="旅游"><a href="#旅游" class="headerlink" title="旅游"></a>旅游</h2><p>今年没有太多旅游, 一方面是疫情影响, 一方面是有意识地控制储蓄. </p></blockquote><h5 id="古北水镇"><a href="#古北水镇" class="headerlink" title="古北水镇"></a>古北水镇</h5><ul><li>古北这边游客比较多, 整体的风景在北方已经算令人眼前一亮了, 印象中墙面和墙体都有各种绿色植物, 集体拍照效果也很不错.</li><li>司马台长城. 司马台长城前面一段垂直角度很大, 一口气爬上去费不少力气, 爬到一半中间有点恐惧但也没有回头路. 但爬上去之后感觉, 哦, 也不过如此. </li></ul><div align="center"><img src="https://s3.ax1x.com/2021/01/03/s9bU3j.jpg" width="700" /></div><center> 图 古北水镇1 </center><div align="center"><img src="https://s3.ax1x.com/2021/01/03/s9bY4g.jpg" width="700" /></div><center> 图 古北水镇2 </center><h5 id="小浪底"><a href="#小浪底" class="headerlink" title="小浪底"></a>小浪底</h5><ul><li>黄河边上各种工程百废待兴, 和长江相比, 黄河本身不太容易打造成风景秀丽的景观. 除了那种波乱壮阔的风景, 平静地黄河景观看起来是缺少一些亮眼的点.  但是黄河确实是塑造了山西淳朴的民风, 一种纯真善良的人的品格. 这一点深信不疑. <div align="center"><img src="https://s3.ax1x.com/2021/01/03/s9qceP.jpg" width="700" /></div><center> 图 运城小浪底水库 </center></li></ul><blockquote><h2 id="请我姥姥吃波士顿龙虾"><a href="#请我姥姥吃波士顿龙虾" class="headerlink" title="请我姥姥吃波士顿龙虾"></a>请我姥姥吃波士顿龙虾</h2></blockquote><ul><li>实现了之前的一个目标<div align="center"><img src="https://s3.ax1x.com/2021/01/03/s9q8MR.jpg" width="700" /></div></li></ul><center> 图 请姥姥吃波士顿 </center>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;h2 id=&quot;年度关键词&quot;&gt;&lt;a href=&quot;#年度关键词&quot; class=&quot;headerlink&quot; title=&quot;年度关键词&quot;&gt;&lt;/a&gt;年度关键词&lt;/h2&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;今年给自己的年度关键词是: &lt;br/&gt;
&lt;cent</summary>
      
    
    
    
    <category term="Always Review" scheme="http://example.com/categories/Always-Review/"/>
    
    
  </entry>
  
  <entry>
    <title>Shoe Dog</title>
    <link href="http://example.com/2020/12/20/Shoe%20Dog/"/>
    <id>http://example.com/2020/12/20/Shoe%20Dog/</id>
    <published>2020-12-20T10:53:00.000Z</published>
    <updated>2021-02-18T10:38:01.148Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="Cover-Page"><a href="#Cover-Page" class="headerlink" title="Cover Page"></a>Cover Page</h2></blockquote><div align="center"><img src="https://s3.ax1x.com/2020/12/20/raRXHU.png" width="400"/></div><blockquote><h2 id="Phil-Knight-菲尔·奈特"><a href="#Phil-Knight-菲尔·奈特" class="headerlink" title="Phil Knight 菲尔·奈特"></a>Phil Knight 菲尔·奈特</h2></blockquote><ul><li>Phil Kight出生于俄勒冈州 (State of Oregon) , 俄勒冈州: 位于美国太平洋沿岸, 面为加利福尼亚州, 北面为华盛顿州, 气候温和, 季节分明. <div align="center"><img src="https://s3.ax1x.com/2020/12/20/raWBrV.png" width="666"/></div></li></ul><blockquote><h2 id="体感"><a href="#体感" class="headerlink" title="体感"></a>体感</h2></blockquote><blockquote><h5 id="环游世界的意义"><a href="#环游世界的意义" class="headerlink" title="环游世界的意义"></a>环游世界的意义</h5></blockquote><ul><li>在第一份工作没开始的时候, phil先环游了一次世界, 对各国文化有着相当深厚的理解. 例如”NIKE”这个词的由来, 虽然文中写到也是经过各种犹豫不定才确定下来, 但是也是在心里埋下了一个种子渴望”胜利”.</li></ul><blockquote><h5 id="疯狂的跑步信徒们"><a href="#疯狂的跑步信徒们" class="headerlink" title="疯狂的跑步信徒们"></a>疯狂的跑步信徒们</h5></blockquote><ul><li>Knight是一个跑步极度爱好者, 是一个疯狂的跑步信徒, 且打心眼相信”跑步让世界能变美好”. 平时跑10km, 遇到压力大的时候跑20km. </li><li>他的合伙人鲍尔曼是美国田径知名教练, 带出来很多奥运会奖牌得主, 而且这个教练非常喜欢鼓捣鞋子, 经常拆开尝试自制鞋子. 得力干将约翰逊也是个疯狂的跑步爱好者. </li></ul><blockquote><h5 id="时代赋予的意义-属于Phil的顺势而为"><a href="#时代赋予的意义-属于Phil的顺势而为" class="headerlink" title="时代赋予的意义: 属于Phil的顺势而为"></a>时代赋予的意义: 属于Phil的顺势而为</h5></blockquote><ul><li>紧紧抓住每一届奥运会和其他赛事. 每一届奥运会都是NIKE提升品牌价值的重要契机, 他的团队紧紧抓住这些重要的赛事, 推出新的产品, 签约更符合自己心目中品牌形象的运动员.</li><li>品牌营销的教科书： NIKE所代表的不是一款产品, 而是将NIKE上升为一种美国人的精神, 一种自由精神的寄托. </li></ul><blockquote><h5 id="艰难时刻"><a href="#艰难时刻" class="headerlink" title="艰难时刻"></a>艰难时刻</h5></blockquote><ul><li>偷情报: 在观察到自己不受到鬼冢信任时, 偷了北见包里面的文件来看, 本质上是及其不道德的商业行为, 但是却为自己掌握了更多情报.</li></ul><blockquote><h5 id="美式幽默的写作风格"><a href="#美式幽默的写作风格" class="headerlink" title="美式幽默的写作风格"></a>美式幽默的写作风格</h5></blockquote><ul><li>和山姆沃尔顿完全迥异的写作风格, 三句正常文字表达之后就加一句美式幽默. </li></ul><blockquote><h5 id="1962"><a href="#1962" class="headerlink" title="1962"></a>1962</h5></blockquote><p>It was one of my final classes, a seminar on entrepreneurship.<br>I’d written a research paper about shoes, and the paper had evolved from a run-of-the-mill assignment to an all-out obsession.<br>Being a runner, I knew something about running shoes.<br>Being a business buff, I knew that Japanese cameras had made deep cuts into the camera market, which had once been dominated by Germans.<br>Thus, I argued in my paper that Japanese running shoes might do the same thing.<br>The idea interested me, then inspired me, then captivated me.<br>It seemed so obvious, so simple, so potentially huge.</p><blockquote><h5 id="1964"><a href="#1964" class="headerlink" title="1964"></a>1964</h5></blockquote><p>Driving back to Portland I’d puzzle over my sudden success at selling.<br>I’d been unable to sell encyclopedias, and I’d despised it to boot.<br>I’d been slightly better at selling mutual funds, but I’d felt dead inside.<br>So why was selling shoes so different?<br>Because, I realized, it wasn’t selling.<br>I believed in running.<br>I believed that if people got out and ran a few miles every day, the world would be a better place, and I believed these shoes were better to run in.<br>People, sensing my belief, wanted some of that belief for themselves.<br>Belief, I decided. Belief is irresistible.</p><blockquote><h5 id="1965"><a href="#1965" class="headerlink" title="1965"></a>1965</h5></blockquote><p>In fact, in 1965, running wasn’t even a sport.<br>It wasn’t popular, it wasn’t unpopular—it just was.<br>To go out for a three-mile run was something weirdos did, presumably to burn off manic energy.<br>Running for pleasure, running for exercise, running for endorphins (内啡肽) , running to live better and longer—these things were unheard of People often went out of their way to mock runners.<br>Drivers would slowdown and honk their horns.<br>“Get a horse!” they’d yell, throwing a beer orsoda at the runner’s head.<br>Johnson had been drenched by many a Pepsi.<br>He wanted to change all this.<br>He wanted to help all the oppressed runners of the world, to bring them into the light, enfold them in a community.<br>So maybe he was a social worker after all.<br>He just wanted to socialize exclusively with runners.</p><blockquote><h5 id="1966"><a href="#1966" class="headerlink" title="1966"></a>1966</h5></blockquote><p>I look back now and wonder if I was truly being myself, or if I was emulating Bowerman, or my father, or both.<br>Was I adopting their man-of-few-words demeanor? Was I maybe modeling all the men I admired?<br>At the time I was reading everything I could get my hands on about generals, samurai, shoguns, along with biographies of my three main heroes—Churchill, Kennedy, and Tolstoy.<br>I had no love of violence, but I was fascinated by leadership, or lack thereof, under extreme conditions.<br>War is the most extreme of conditions. But business has its warlike parallels.<br>Someone somewhere once said that business is war without bullets, and I tended to agree.  </p><p>To Johnson’s credit, though he craved more communication, he never let the lack of it discourage him.<br>On the contrary, it motivated him. He was anal, he recognized that I was not, and though he enjoyed complaining (to me, to<br>my sister, to mutual friends), he saw that my managerial style gave him freedom.<br>Left to do as he pleased, he responded with boundless creativity and energy.<br>He worked seven days a week, selling and promoting Blue Ribbon, and when he wasn’t selling, he was beaverishly building up his customer data files.</p><p>Each new customer got his or her own index card, and each index card contained that customer’s personal information, shoe size, and shoe preferences.<br>This database enabled Johnson to keep in touch with all his customers, at all times, and to keep them all feeling special. He sent them Christmas cards.<br>He sent them birthday cards. He sent them notes of congratulation after they completed a big race or marathon.<br>Whenever I got a letter from Johnson I knew it was one of dozens he’d carried down to the mailbox that day.<br>He had hundreds and hundreds of customer correspondents, all along the spectrum of humanity, from high school track stars to octogenarian weekend joggers.<br>Many, upon pulling yet another Johnson letter from their mailboxes, must have thought the same thing I did:<br>“Where does this guy find the time?”</p><p>Bowerman told the writer from <em>Sports Illustrated</em> that Pre was the fastest middle-distance runner alive.<br>I’d never heard such unbridled enthusiasm frommy stolid coach.<br>In the days ahead, in other articles I clipped, Bowerman waseven more effusive, calling Pre “the best runner I’ve ever had.”<br>Bowerman’s assistant, Bill Dellinger, said Pre’s secret weapon was his confidence, whichwas as freakish as his lung capacity.<br>“Usually,” Dellinger said, “it takes ourguys twelve years to build confidence in themselves, and here’s a young manwho has the right attitude naturally.”<br>Yes, I thought. Confidence. More than equity, more than liquidity, that’s what a man needs.<br>I wished I had more. I wished I could borrow some.<br>But confidence was cash. You had to have some to get some. And people were loath to give it to you.</p><blockquote><h5 id="1971"><a href="#1971" class="headerlink" title="1971"></a>1971</h5></blockquote><p>I’d never heard of Senter, but Sumeragi assured me the man was a genuine, head-to-toe shoe dog.<br>I’d heard this phrase a few times.<br>Shoe dogs were people who devoted themselves wholly to the making, selling, buying, or designing of shoes.<br>Lifers used the phrase cheerfully to describe other lifers, men and women who had toiled so long and hard in the shoe trade, they thought and talked about nothing else.<br>It was an all-consuming mania, a recognizable psychological disorder, to care so much about insoles and outsoles, linings and welts, rivets and vamps.<br>But I understood.<br>The average person takes seventy-five hundred steps a day, 274 million steps over the course of a long life, the equivalent of six times around the globe—shoe dogs, it seemed to me, simply wanted to be part of that journey.<br>Shoes were their way of connecting with humanity.<br>What better way of connecting, shoe dogs thought, than by refining the hinge that joins each person to the world’s surface?<br>I felt an unusual sympathy for such sad cases.<br>I wondered how many I might have met in my travels.</p><blockquote><h5 id="1972"><a href="#1972" class="headerlink" title="1972"></a>1972</h5></blockquote><p>They started to barrage us with questions. Hey—what IS this?<br>That’s a Nike.<br>The hell’s a Nike?<br>It’s the Greek goddess of victory.<br>Greek what now?<br>Goddess of vic—<br>And what’s THIS?<br>That’s a swoosh.<br>The hell’s a swoosh?<br>The answer flew out of me: It’s the sound of someone going past you.<br>They liked that. Oh, they liked it a whole lot.  </p><p>“This is—the moment,” I said. “This is the moment we’ve been waiting for. Our moment.<br>No more selling someone else’s brand.<br>No more working for someone else.<br>Onitsuka has been holding us down for years.<br>Their late deliveries, their mixed-up orders, their refusal to hear and implement our design ideas—who among us isn’t sick of dealing with all that?<br>It’s time we faced facts: If we’re going to succeed, or fail, we should do so on our own terms, with our own ideas—our own brand.<br>We posted two million in sales last year . . . none of which had anything to do with Onitsuka.<br>That number was a testament to our ingenuity and hard work. Let’s not look at this as a crisis.<br>Let’s look at this as our liberation. Our Independence Day.</p><p><em>The cowards never started and the weak died along the way—that leaves us.</em></p><blockquote><h5 id="1973"><a href="#1973" class="headerlink" title="1973"></a>1973</h5></blockquote><center><font size=5 color="Red">MacArthur came to mind. <i>Go fast or die.</i></font></center><ul><li><font color="blue"><strong>Supply and demand is always the root problem in business.</strong></font> It’s been true since Phoenician traders raced to bring Rome the coveted purple dye that colored the clothing of royals and rich people; there was never enough purple to go around. It’s hard enough to invent and manufacture and market a product, but then the logistics, the mechanics, the hydraulics of getting it to the people who want it, when they want it—this is how companies die, how ulcers are born.  </li></ul><center><font size=5 color="Red"><i>You are remembered for the rules you break.</i></font></center><blockquote><h5 id="1976"><a href="#1976" class="headerlink" title="1976"></a>1976</h5></blockquote><p>Watching that shoe evolve in 1976 from popular accessory to cultural artifact, I had a thought.<br>People might start wearing this thing to class.<br>And the office.<br>And the grocery store.<br>And throughout their everyday lives.<br>It was a rather grandiose idea. Adidas had had limited success converting athletic shoes to everyday wear, with the Stan Smith tennis shoe and the<br>Country running shoe.<br>But neither was nearly as distinctive, or popular, as the waffle trainer.<br>So I ordered our factories to start making the waffle trainer in blue, which would go better with jeans, and that’s when it really took off.<br>We couldn’t make enough. Retailers and sales reps were on their knees, pleading for all the waffle trainers we could ship.<br>The soaring pair counts were transforming our company, not to mention the industry.<br>We were seeing numbers that redefined our long-term goals, because they gave us something we’d always lacked—an identity.<br>More than a brand, Nike was now becoming a household word, to such an extent that we would have to change the company name.<br>Blue Ribbon, we decided, had run its course. We would have to incorporate as Nike, Inc.</p><blockquote><h5 id="1980"><a href="#1980" class="headerlink" title="1980"></a>1980</h5></blockquote><p>I notified everyone who was going on the trip to get their papers and passports and affairs in order.<br>Then I spent the days leading up to our departure reading, cramming on Chinese history.<br>The Boxer Rebellion. The Great Wall. Opium Wars. Ming dynasty. Confucius. Mao.</p><p>Just before getting on the plane home we signed deals with two Chinese factories, and officially became the first American shoemaker in twenty-five years to be allowed to do business in China.<br>It seems wrong to call it “business.”<br>It seems wrong to throw all those hectic days and sleepless nights, all those magnificent triumphs and desperate struggles, under that bland, generic banner: business.   </p><p>What we were doing felt like so much more.<br>Each new day brought fifty new problems, fifty tough decisions that needed to be made, right now, and we were always acutely aware that one rash move, one wrong decision could be the end.<br>The margin for error was forever getting narrower, while the stakes were forever creeping higher—and none of us wavered in the belief that “stakes” didn’t mean “money.”<br>For some, I realize, business is the all-out pursuit of profits, period, full stop, but for us business was no more about making money than being human is about making blood.<br>Yes, the human body needs blood.<br>It needs to manufacture red and white cells and platelets and redistribute them evenly, smoothly, to all the right places, on time, or else.<br>But that day-to-day business of the human body isn’t our mission as human beings.<br>It’s a basic process that enables our higher aims, and life always strives to transcend the basic processes of living—and at some point in the late 1970s, I did, too.<br>I redefined winning, expanded it beyond my original definition of not losing, of merely staying alive.<br>That was no longer enough to sustain me, or my company.<br>We wanted, as all great businesses do, to create, to contribute, and we dared to say so aloud.<br>When you make something, when you improve something, when you deliver something, when you add some new thing or service to the lives of strangers, making them happier, or healthier, or safer, or better, and when you do it all crisply and efficiently, smartly, the way everything should be done but so seldom is—you’re participating more fully in the whole grand human drama.<br>More than simply alive, you’re helping others to live more fully, and if that’s business, all right, call me a businessman.<br>Maybe it will grow on me.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;h2 id=&quot;Cover-Page&quot;&gt;&lt;a href=&quot;#Cover-Page&quot; class=&quot;headerlink&quot; title=&quot;Cover Page&quot;&gt;&lt;/a&gt;Cover Page&lt;/h2&gt;&lt;/blockquote&gt;
&lt;div align=&quot;ce</summary>
      
    
    
    
    <category term="Reading" scheme="http://example.com/categories/Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>Bert</title>
    <link href="http://example.com/2020/12/16/Bert/"/>
    <id>http://example.com/2020/12/16/Bert/</id>
    <published>2020-12-16T13:56:00.000Z</published>
    <updated>2021-02-18T11:12:51.788Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h2></blockquote><ul><li>BERT is basically a trained Transformer Encoder stack.  </li><li>Both BERT model sizes have a large number of encoder layers (which the paper calls Transformer Blocks) – twelve for the Base version, and twenty four for the Large version. These also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the default configuration in the reference implementation of the Transformer in the initial paper (6 encoder layers, 512 hidden units, and 8 attention heads).<div align="center"><img  src="https://s3.ax1x.com/2020/12/16/r1tWy4.png"  width="666" /></div></li></ul><hr><div align="center"><img  src="https://s3.ax1x.com/2020/12/16/r1tcWT.png"  width="666" /></div><hr><div align="center"><img  src="https://s3.ax1x.com/2020/12/16/r1tfOJ.png"  width="666" /></div><hr><div align="center"><img  src="https://s3.ax1x.com/2020/12/16/r1tRlF.png"  width="666" /></div><hr><div align="center"><img  src="https://s3.ax1x.com/2020/12/16/r1t2SU.png"  width="666" /></div><hr><div align="center"><img  src="https://s3.ax1x.com/2020/12/16/r1tx0A.png"  width="666" /></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://jalammar.github.io/illustrated-bert/">https://jalammar.github.io/illustrated-bert/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;h2 id=&quot;Bert&quot;&gt;&lt;a href=&quot;#Bert&quot; class=&quot;headerlink&quot; title=&quot;Bert&quot;&gt;&lt;/a&gt;Bert&lt;/h2&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;BERT is basically a trained T</summary>
      
    
    
    
    <category term="RecSys &amp; Machine Learning" scheme="http://example.com/categories/RecSys-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>The Essence of RecSys</title>
    <link href="http://example.com/2020/11/08/The%20Essence%20of%20RecSys/"/>
    <id>http://example.com/2020/11/08/The%20Essence%20of%20RecSys/</id>
    <published>2020-11-08T03:56:00.000Z</published>
    <updated>2021-02-18T10:42:11.906Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="推荐系统的真正价值"><a href="#推荐系统的真正价值" class="headerlink" title="推荐系统的真正价值"></a>推荐系统的真正价值</h2></blockquote><h5 id="理解推荐的三个层次"><a href="#理解推荐的三个层次" class="headerlink" title="理解推荐的三个层次"></a>理解推荐的三个层次</h5><p>理解工业界推荐系统，需要依次理解到如下三个层次</p><ol><li>解放人力.  </li><li>聚焦目标.  </li><li>平衡体验. 推荐系统不仅是一种流量的遥控器，从产品角度来看, 平衡平台方的业务目标和用户的用户体验十分重要. </li></ol><p>综上, 推荐系统可被拆分为两件事情:  </p><ol><li>确立一种分发的机制  </li><li>基于设计好的分发机制，相依地调整体验平衡机制</li></ol><h4 id="搜索-v-s-推荐-v-s-广告"><a href="#搜索-v-s-推荐-v-s-广告" class="headerlink" title="搜索 v.s. 推荐 v.s. 广告"></a>搜索 v.s. 推荐 v.s. 广告</h4><p>三者在业务目标、技术实现上有如下差异</p><div style="text-align: center;"><img src="https://s3.ax1x.com/2020/12/13/rm3V9x.png" style="display: inline-block;" width="450"/></div><blockquote><h2 id="架构大图v1-0"><a href="#架构大图v1-0" class="headerlink" title="架构大图v1.0"></a>架构大图v1.0</h2></blockquote><div style="text-align: center;"><img src="https://s3.ax1x.com/2020/11/24/DU98pQ.png" style="display: inline-block;" width="666"/></div><hr><h2 id="召回-Match"><a href="#召回-Match" class="headerlink" title="召回 Match"></a>召回 Match</h2><h4 id="召回的目标"><a href="#召回的目标" class="headerlink" title="召回的目标"></a>召回的目标</h4><p>召回决定了推荐系统效果的天花板, 召回的目标是丰富且有效, 召回环节质量过低则排序无从谈起; 值得注意的是, 召回的结果是丰富和有效之间的一种trade off.</p><ul><li>丰富: 丰富有两个层面的目标 <ol><li>召回宽度, 算法”见世面”的能力.</li><li>召回深度, 算法”以点带面”的能力.</li></ol></li><li>有效: 在保证丰富性的情况下, 相关性不能太低  </li></ul><h4 id="召回的层次与融合"><a href="#召回的层次与融合" class="headerlink" title="召回的层次与融合"></a>召回的层次与融合</h4><p>召回结果的相关性和发现性是另一对需要被平衡的trade off.</p><ul><li>相关性：      U2I         &gt;= U2I2I &gt;= U2X2I/other  </li><li>新颖性/发现性: U2X2I/other &gt;= U2I2I &gt;= U2I</li></ul><h4 id="item-based-CF的四宗罪和不放弃它的原因"><a href="#item-based-CF的四宗罪和不放弃它的原因" class="headerlink" title="item-based CF的四宗罪和不放弃它的原因"></a>item-based CF的四宗罪和不放弃它的原因</h4><p>原始的item-based CF存在以下问题</p><ol><li>聚集性: 哈利波特天天见<br>建模物品相关关系时, 向热门商品或者具有高频行为的用户聚集, 故需要显示地进行热门物品或者热门用户的打压.</li><li>无向性: 从你到我 != 从我到你.<br>co-currence建模不具备方向性, 因此对任意关系统计, 两个item之间的作用相等，但在实际业务中往往物品体系之间存在一定逻辑结构.<br>例如存在推荐结果: trigger: “iphone12” =&gt; rec_result: “iphone12手机壳”</li><li>有偏性: 存在即合理，不存在就不合理? 实际业务中存在较多的</li><li>失准性: 脑洞</li></ol><p>不放弃item-CF的原因  </p><ul><li>co-currence始终是一种朴素而有效关系建模方式, 具备实现简单服务效率高的特点(离线索引), 众多实验证明swing等改进版item-CF是一个不错的Baseline.</li><li>item-CF优化最佳实践: 其公式如下</li></ul><hr><h2 id="排序-Rank"><a href="#排序-Rank" class="headerlink" title="排序 Rank"></a>排序 Rank</h2><ul><li>排序战略地位: 排序在整个推荐链路处于倒数第二个阶段，在召回侧足够丰富且有效的情况下，排序侧是直面用户的最重要的阶段。  </li><li>排序公式设计原则:   <ol><li>对齐业务目标</li><li>足够简单</li><li>权值化</li></ol></li></ul><h3 id="排序公式"><a href="#排序公式" class="headerlink" title="排序公式"></a>排序公式</h3><h4 id="推荐场景"><a href="#推荐场景" class="headerlink" title="推荐场景"></a>推荐场景</h4><ul><li>猜你喜欢 : <em>rank_score = pctr<sup>α</sup> x pcvr<sup>β</sup></em></li><li>营销/权益场景 : <em>rank_score = pctr<sup>α</sup> x pcvr<sup>β</sup> x rights<sup>γ</sup></em><br>其中, <em>rights<sup>γ</sup></em> 为各类优惠券的折扣公式   </li></ul><h4 id="商业化场景"><a href="#商业化场景" class="headerlink" title="商业化场景"></a>商业化场景</h4><ul><li>商业化 (CPM) : <em>ecpm = bid_show</em> </li><li>商业化 (CPC) :  <em>ecpm = pctr<sup>α</sup> x bid_click</em>  </li><li>商业化 (oCPM) : <em>ecpm = pctr<sup>α</sup> x pcvr<sup>β</sup> x bid_convert</em></li><li>商业化 (oCPC) : <em>ecpm = pctr<sup>α</sup> x pcvr<sup>β</sup> x bid_convert</em></li><li>商业化 (CPS) : <em>rank_score = pctr<sup>α</sup> x pcvr<sup>β</sup> x price<sup>γ</sup> x commiRate<sup>δ</sup></em><br>其中, <em>bid_show</em>: 曝光出价, <em>bid_click</em>: 点击出价, <em>bid_convert</em>: 转化出价</li></ul><h4 id="CTR预估的核心问题"><a href="#CTR预估的核心问题" class="headerlink" title="CTR预估的核心问题"></a>CTR预估的核心问题</h4><h5 id="Feature-Interaction"><a href="#Feature-Interaction" class="headerlink" title="Feature Interaction"></a>Feature Interaction</h5><p>  特征交互（特征交叉）是提升CTR预估准度的关键问题. 从建模方式上常见分两种: GBDT和DNN(详情对比见), 简言之<br>  GBDT: 依赖人工特征抽取, 但在数据量不过大, id类dim不太高的Tabular Data场景具备很强的Fit能力和一定的解释性<br>  DNN: 通过NN自动捕捉Feature Interaction, 且具备generalization能力. 对于Feature Interaction的优化主要是通过不同的网络设计实现(FM, Attention), 研究见下图</p><div style="text-align: center;"><img src="https://s3.ax1x.com/2020/12/13/rme34S.png" style="display: inline-block;" width="666"/></div><h5 id="Sparsity"><a href="#Sparsity" class="headerlink" title="Sparsity."></a>Sparsity.</h5><p>  稀疏性和场景紧密相关。</p><h5 id="Sequence-Modeling"><a href="#Sequence-Modeling" class="headerlink" title="Sequence Modeling."></a>Sequence Modeling.</h5><p>  序列建模主要对时序信息进行单独建模, 但个人认为序列建模适用场景有限，偏向于Session Based推荐场景, 即具备显著如下特点<br>  Local Activation (局部激活): 在预估时，Sequence item和Target item之间具备较强的相关性. 因此对于用户相对独立的、离散的或者跨时间较长的范围甚至有负向作用.</p><h4 id="CVR预估的三大挑战"><a href="#CVR预估的三大挑战" class="headerlink" title="CVR预估的三大挑战"></a>CVR预估的三大挑战</h4><ol><li><p>Sample Sparsity 样本稀疏性</p></li><li><p>Sample Selection Bias 样本选择偏差</p><div style="text-align: center;"><img src="https://s3.ax1x.com/2020/12/13/rm3aDg.png" style="display: inline-block;" width="400"/></div></li><li><p>Delay FeedBack 延迟反馈</p></li></ol><h4 id="多任务-多目标学习-Multi-Task-Multi-Objective"><a href="#多任务-多目标学习-Multi-Task-Multi-Objective" class="headerlink" title="多任务/多目标学习 Multi-Task/Multi-Objective"></a>多任务/多目标学习 Multi-Task/Multi-Objective</h4><ul><li>相关性</li></ul><blockquote><h2 id="冷启动-Cold-Start"><a href="#冷启动-Cold-Start" class="headerlink" title="冷启动 Cold Start"></a>冷启动 Cold Start</h2></blockquote><h2 id="质量保证-Quality"><a href="#质量保证-Quality" class="headerlink" title="质量保证 Quality"></a>质量保证 Quality</h2><h4 id="核心项目必须指定唯一PM"><a href="#核心项目必须指定唯一PM" class="headerlink" title="核心项目必须指定唯一PM"></a>核心项目必须指定唯一PM</h4><ul><li>唯一PM对所有线上问题负责，必须熟悉项目背景和方案实现细节, 时刻把控项目迭代过程中的维护CheckList;</li></ul><h4 id="CheckList维护"><a href="#CheckList维护" class="headerlink" title="CheckList维护"></a>CheckList维护</h4><ul><li>为什么CheckList? 质量保证的作用在于: 去除软件系统对于“工程师对于质量的细心程度和对于系统的熟悉程度”依赖, 换句话说: 通过检查机制实现项目质量, 永远不依赖工程师的”意识”、”责任感”、”经验”或者”细心习惯”检查问题；</li><li>CheckList的迭代过程: 从需求文档开始构建v1.0版本CheckList, 测试开始之前进行更新，直到上线前持续更新;</li><li>CheckList样例模板(更新中…)</li></ul><table><thead><tr><th>id</th><th>Check项</th><th>负责人</th><th>实现方法</th><th>latest check time</th></tr></thead><tbody><tr><td>1</td><td>过滤虚拟类目商品</td><td>张三</td><td>1.投放禁用虚拟类目配置id:9527  2.实时召回过滤逻辑,code:<a href="http://git@gitlab.com/">http://git@gitlab.com</a></td><td>2020-10-10-15:30</td></tr><tr><td>2</td><td>过滤预售商品</td><td>李四</td><td>1.投放禁用预售配置id:9528  2.实时召回过滤逻辑,code:<a href="http://git@gitlab.com/">http://git@gitlab.com</a></td><td>2020-10-10-16:30</td></tr><tr><td>3</td><td>过滤黑名单商品</td><td>李四</td><td>1.投放禁用黑名单配置id:9529  2.实时召回过滤逻辑,code:<a href="http://git@gitlab.com/">http://git@gitlab.com</a></td><td>2020-10-10-16:50</td></tr></tbody></table><h4 id="足够及时的Review"><a href="#足够及时的Review" class="headerlink" title="足够及时的Review"></a>足够及时的Review</h4><ul><li>Review的进行间不能超过3天，一般为线上问题发生当天之后第二天最合适。当天或当天后一天需要专注于处理线上问题保证问题第一时间得到解决，及时当天问题得到解决，也需要给工程师一个缓冲的时间冷静下来思考为什么会产生这样的原因。</li><li>Review先要对问题进行分类，是在哪个环节出了问题？在测试过程中哪个环节没有测试到？</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;h2 id=&quot;推荐系统的真正价值&quot;&gt;&lt;a href=&quot;#推荐系统的真正价值&quot; class=&quot;headerlink&quot; title=&quot;推荐系统的真正价值&quot;&gt;&lt;/a&gt;推荐系统的真正价值&lt;/h2&gt;&lt;/blockquote&gt;
&lt;h5 id=&quot;理解推荐的三个层次&quot;</summary>
      
    
    
    
    <category term="RecSys &amp; Machine Learning" scheme="http://example.com/categories/RecSys-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Principles of Economics</title>
    <link href="http://example.com/2020/10/03/Principles%20of%20Economics/"/>
    <id>http://example.com/2020/10/03/Principles%20of%20Economics/</id>
    <published>2020-10-03T07:03:00.000Z</published>
    <updated>2021-01-08T09:12:41.385Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="Cover-Page"><a href="#Cover-Page" class="headerlink" title="Cover Page"></a>Cover Page</h2><div align="center"><img  src="https://s3.ax1x.com/2021/01/08/suOjvd.png"  width="400" /></div><hr><h2 id="Ten-Principles-of-Economics"><a href="#Ten-Principles-of-Economics" class="headerlink" title="Ten Principles of Economics"></a>Ten Principles of Economics</h2><div align="center"><img  src="https://s3.ax1x.com/2021/01/08/suOXgH.png"/></div>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;Cover-Page&quot;&gt;&lt;a href=&quot;#Cover-Page&quot; class=&quot;headerlink&quot; title=&quot;Cover Page&quot;&gt;&lt;/a&gt;Cover Page&lt;/h2&gt;&lt;div align=&quot;center&quot;&gt;
&lt;img  src=&quot;http</summary>
      
    
    
    
    <category term="Reading" scheme="http://example.com/categories/Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>Hexo搭建</title>
    <link href="http://example.com/2020/10/01/Hexo%E6%90%AD%E5%BB%BA/"/>
    <id>http://example.com/2020/10/01/Hexo%E6%90%AD%E5%BB%BA/</id>
    <published>2020-10-01T12:11:00.000Z</published>
    <updated>2021-02-18T10:35:50.707Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="安装-Node-js和npm"><a href="#安装-Node-js和npm" class="headerlink" title="安装 Node.js和npm"></a>安装 Node.js和npm</h2></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt install nodejs -y</span><br><span class="line">sudo apt install npm -y</span><br><span class="line"><span class="comment"># 升级npm为最新版</span></span><br><span class="line">sudo npm install npm@latest -g</span><br><span class="line"><span class="comment"># 安装用于管理node的模块n</span></span><br><span class="line">sudo npm install -g n</span><br><span class="line"><span class="comment"># 使用n模块来安装node版本</span></span><br><span class="line">sudo n latest <span class="comment"># 最新版本</span></span><br><span class="line">sudo n stable <span class="comment"># 最新稳定版本</span></span><br><span class="line"><span class="comment"># 查看安装好的版本</span></span><br><span class="line">node -v</span><br><span class="line">v12<span class="number">.4</span><span class="number">.0</span></span><br><span class="line">npm -v</span><br><span class="line"><span class="number">6.9</span><span class="number">.0</span></span><br></pre></td></tr></table></figure><blockquote><h2 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h2></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir hexo &amp; cd hexo</span><br><span class="line">sudo npm install -g hexo-cli</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure><p>若报错执行代码,不报错忽略</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo npm config set user 0</span><br><span class="line">sudo npm config set unsafe-perm true</span><br><span class="line">sudo npm install -g hexo-cli</span><br></pre></td></tr></table></figure><p>安装插件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-archive --save</span><br><span class="line">npm install hexo-generator-category --save</span><br><span class="line">npm install hexo-generator-tag --save</span><br><span class="line">npm install hexo-server --save</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line">npm install hexo-deployer-heroku --save</span><br><span class="line">npm install hexo-deployer-rsync --save</span><br><span class="line">npm install hexo-deployer-openshift --save</span><br><span class="line">npm install hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-stylus --save</span><br><span class="line">npm install hexo-generator-feed --save</span><br><span class="line">npm install hexo-generator-sitemap --save</span><br></pre></td></tr></table></figure><p>测试安装成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure><p>得到成功提示, 浏览器输入 <a href="http://0.0.0.0:4000/">http://0.0.0.0:4000</a> 访问首页</p><blockquote><h2 id="站点信息设置"><a href="#站点信息设置" class="headerlink" title="站点信息设置"></a>站点信息设置</h2><p>初步修改Hexo配置文件hexo/_config.yml, 后续会换用主题, 这里暂时不设置也可以</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">title: Gold and Rabbit&#39;s Blog </span><br><span class="line">subtitle: &#39;Always ambitious&#39;</span><br><span class="line">description: &#39;having fun coding&#39;</span><br><span class="line">keywords: Recommending System</span><br><span class="line">author: Gold and Rabbit </span><br><span class="line">language: en</span><br><span class="line"># language: zh-CN</span><br><span class="line">timezone: &#39;&#39;</span><br></pre></td></tr></table></figure><blockquote><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>修改Hexo配置文件hexo/_config.yml, github创建一个新项目，项目名称为 yourname.github.io</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@github.com:GoldAndRabbit&#x2F;GoldAndRabbit.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><blockquote><h2 id="Hexo命令"><a href="#Hexo命令" class="headerlink" title="Hexo命令"></a>Hexo命令</h2></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建文章(会在 hexo/source/_post 下生成对应.md 文件)</span></span><br><span class="line">hexo n “文章名称”</span><br><span class="line"><span class="comment"># 生成静态文件(位于 hexo/public 目录)</span></span><br><span class="line">hexo g</span><br><span class="line"><span class="comment"># 启动 Hexo 预览</span></span><br><span class="line">hexo s</span><br><span class="line"><span class="comment"># 提交部署(需要相关配置)</span></span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><blockquote><h2 id="上传"><a href="#上传" class="headerlink" title="上传"></a>上传</h2></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>访问yourname.github.io</p><blockquote><h2 id="一个坑"><a href="#一个坑" class="headerlink" title="一个坑"></a>一个坑</h2><p>表现提示如下</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">extends includes&#x2F;layout.pug block content #recent-posts.recent-posts include includes&#x2F;recent-posts.pug include includes&#x2F;pagination.pug </span><br></pre></td></tr></table></figure><h5 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h5><ul><li><a href="https://zhuanlan.zhihu.com/p/137946156">https://zhuanlan.zhihu.com/p/137946156</a></li></ul><blockquote><h2 id="Hexo主题配置-butterfly"><a href="#Hexo主题配置-butterfly" class="headerlink" title="Hexo主题配置-butterfly"></a>Hexo主题配置-butterfly</h2></blockquote><ul><li><a href="http://www.c-hasel.cn/2019/12/01/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E4%BA%8C/#%E4%B8%BB%E9%A2%98%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%8D%87%E7%BA%A7">http://www.c-hasel.cn/2019/12/01/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E4%BA%8C/#%E4%B8%BB%E9%A2%98%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%8D%87%E7%BA%A7</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;h2 id=&quot;安装-Node-js和npm&quot;&gt;&lt;a href=&quot;#安装-Node-js和npm&quot; class=&quot;headerlink&quot; title=&quot;安装 Node.js和npm&quot;&gt;&lt;/a&gt;安装 Node.js和npm&lt;/h2&gt;&lt;/blockquote</summary>
      
    
    
    
    <category term="Programming &amp; Data" scheme="http://example.com/categories/Programming-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>Item-Based CF</title>
    <link href="http://example.com/2020/09/14/Item-Based%20CF/"/>
    <id>http://example.com/2020/09/14/Item-Based%20CF/</id>
    <published>2020-09-14T14:56:00.000Z</published>
    <updated>2021-01-08T09:33:15.993Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="Graph-Distance-Metrics"><a href="#Graph-Distance-Metrics" class="headerlink" title="Graph Distance Metrics"></a>Graph Distance Metrics</h2><ul><li>度量距离的方式<div style="text-align: center;"><img alt="" src="https://i.loli.net/2020/09/14/C2k7zgqBLOYeH9a.png" style="display: inline-block;"/></div></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;Graph-Distance-Metrics&quot;&gt;&lt;a href=&quot;#Graph-Distance-Metrics&quot; class=&quot;headerlink&quot; title=&quot;Graph Distance Metrics&quot;&gt;&lt;/a&gt;Graph Distance </summary>
      
    
    
    
    <category term="RecSys &amp; Machine Learning" scheme="http://example.com/categories/RecSys-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Deep CTR Model</title>
    <link href="http://example.com/2020/08/15/Deep%20CTR%20Model/"/>
    <id>http://example.com/2020/08/15/Deep%20CTR%20Model/</id>
    <published>2020-08-15T12:11:00.000Z</published>
    <updated>2021-04-10T15:04:27.488Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="Why-Deep-CTR-model"><a href="#Why-Deep-CTR-model" class="headerlink" title="Why Deep CTR model?"></a>Why Deep CTR model?</h2></blockquote><ul><li><strong>Auto Feature Interaction</strong>. 深度学习用于CTR预估问题, 主要优势是通过网络设计达到自动学习特征交互Feature Interaction的目的. 本文中涉及到的模型均是解决feature interaction的不同网络设计.</li><li><strong>Better Sparse ids presentation Support</strong>. 相比GBDT模型, DNN对稀疏id类特征有更好的表示学习能力. 业务需求中往往存在海量且稀疏id类特征, 通过embedding支持对海量id类特征具备较强的表示学习能力.</li><li><strong>Memorization &amp; Generalization</strong>. 记忆性和泛化性是推荐系统重要的两类能力, 这两类目标通过Wide &amp; Deep Learning结构同时学得, wide part采用FTRL实现, 目的是使得对id类特征具有memorization(记忆性); DNN结构具有generalization的特性(泛化性); </li><li>整理实现. 封装在gold-deep-rank这个项目中, repo地址: <a href="https://github.com/GoldAndRabbit/gold-deep-rank">https://github.com/GoldAndRabbit/gold-deep-rank</a> 主要参考作者源码以及开源库.</li></ul><blockquote><h2 id="Deep-Model-Framework"><a href="#Deep-Model-Framework" class="headerlink" title="Deep Model Framework"></a>Deep Model Framework</h2></blockquote><div align="center"><img alt="" src="https://z3.ax1x.com/2021/04/10/cdTUkF.png" /></div><blockquote><h2 id="Wide-amp-Deep-Learning"><a href="#Wide-amp-Deep-Learning" class="headerlink" title="Wide &amp; Deep Learning"></a>Wide &amp; Deep Learning</h2></blockquote><div align="center"><img alt="" src="https://s3.ax1x.com/2020/12/15/rMbxQU.png" /></div><ul><li>Google提出将线性层和DNN同时优化的一般结构, 在此基础上对DNN部分做优化/定制.  </li><li>泛化性和记忆性是推荐系统的重要的两类基础能力.</li></ul><blockquote><h2 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h2></blockquote><div align="center"><img alt="" src="https://s3.ax1x.com/2020/12/15/rMqKwd.png" width="400"/></div><center>   <p><img src="http://latex.codecogs.com/png.latex?y_%7Bfm%7D=w_%7B0%7D+%5Csum_%7Bi=1%7D%5E%7Bn%7Dw_%7Bi%7Dx_%7Bi%7D+%5Csum_%7Bi=1%7D%5E%7Bn%7D%5Csum_%7Bj=i+1%7D%5E%7Bn%7D%3Cv_%7Bi%7D%5Codot%7Bv_%7Bj%7D%7D%3Ex_%7Bi%7D%7Bx_%7Bj%7D%7D" alt="deepfm"></p></center><ul><li>fm是二阶特征交互的基础方法, 可作为一般Baseline. </li><li>fm复杂度降低实现推导，将复杂度从O(kn^2)降低到O(kn), 简单记忆方法: sum_square-square_sum, 不要忘了前面还有1/2常系数.</li></ul><div align="center"><img src="https://s3.ax1x.com/2020/12/28/r7PVKS.png" width="400" /></div><blockquote><h2 id="PNN-Inner-Outer-product"><a href="#PNN-Inner-Outer-product" class="headerlink" title="PNN: Inner/Outer product"></a>PNN: Inner/Outer product</h2></blockquote><div align="center"><img alt="" src="https://s3.ax1x.com/2020/12/15/rMqJl8.png"  width="400"/></div><ul><li>向量的内积和外积可以定义两种vec的交叉方式, 很朴素的feature interaction思想.</li></ul><blockquote><h2 id="DCN-Cross-Network"><a href="#DCN-Cross-Network" class="headerlink" title="DCN: Cross Network"></a>DCN: Cross Network</h2></blockquote><div align="center"><img alt="" src="https://s3.ax1x.com/2020/12/15/rMbzyF.png"  width="400"/></div><ul><li>思想是实现<strong>多项式形式</strong>的feature interaction，其实和一般意义上的特征交叉有所区别.<center></li></ul><p><img src="http://latex.codecogs.com/png.latex?x_%7Bl+1%7D=x_%7B0%7Dx_%7Bl%7D%5E%7BT%7Dx_%7Bl%7D+b_%7Bl%7D+x_%7Bl%7D" alt="dcn"></p></center><blockquote><h2 id="xDeepFM-CIN"><a href="#xDeepFM-CIN" class="headerlink" title="xDeepFM: CIN"></a>xDeepFM: CIN</h2></blockquote><div align="center"><img alt="" src="https://s3.ax1x.com/2020/12/15/rMqFF1.png" /></div><ul><li>引入vector-wise feature交叉, 而不是bit-wise.</li><li>CIN的结构不建议理解公式(形式化复杂), 结合图和源码看比较容易理解.</li></ul><blockquote><h2 id="AFM-FM-based-attention"><a href="#AFM-FM-based-attention" class="headerlink" title="AFM: FM based attention"></a>AFM: FM based attention</h2></blockquote><div align="center"><img alt="" src="https://s3.ax1x.com/2020/12/15/rMqPoR.png"  width="666"/></div><ul><li>在原有deepfm基础上, 加一层attention layer<center></li></ul><p><img src="http://latex.codecogs.com/png.latex?y_%7Bfm%7D%5E=w_%7B0%7D+%5Csum_%7Bi=1%7D%5E%7Bn%7Dw_%7Bi%7Dx_%7Bi%7D+%5Cmathbf%7Bp%5E%7BT%7D%7D%5Calpha_%7Bij%7D%5Csum_%7Bi=1%7D%5E%7Bn%7D%5Csum_%7Bj=i+1%7D%5E%7Bn%7D%3Cv_%7Bi%7D%5Codot%7Bv_%7Bj%7D%7D%3Ex_%7Bi%7D%7Bx_%7Bj%7D%7D" alt="afm"></p></center><blockquote><h2 id="AutoInt-Multi-head-attention"><a href="#AutoInt-Multi-head-attention" class="headerlink" title="AutoInt: Multi-head attention"></a>AutoInt: Multi-head attention</h2></blockquote><div align="center"><img alt="" src="https://s3.ax1x.com/2020/12/15/rMxVSI.png"  width="400"/></div><ul><li>引入multi-head self attention学习feature interaction, 关于multi-head self attention查看transformer原理</li></ul><blockquote><h2 id="FiBiNet-SENET-amp-Bi-linear-interaction"><a href="#FiBiNet-SENET-amp-Bi-linear-interaction" class="headerlink" title="FiBiNet: SENET &amp; Bi-linear interaction"></a>FiBiNet: SENET &amp; Bi-linear interaction</h2></blockquote><div align="center"><img alt="" src="https://s3.ax1x.com/2020/12/15/rMqY6S.png"  width="666"/></div><ul><li>引入SENET学习feature interaction</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;h2 id=&quot;Why-Deep-CTR-model&quot;&gt;&lt;a href=&quot;#Why-Deep-CTR-model&quot; class=&quot;headerlink&quot; title=&quot;Why Deep CTR model?&quot;&gt;&lt;/a&gt;Why Deep CTR model</summary>
      
    
    
    
    <category term="RecSys &amp; Machine Learning" scheme="http://example.com/categories/RecSys-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Why Alibaba</title>
    <link href="http://example.com/2020/06/05/Why%20Alibaba%20/"/>
    <id>http://example.com/2020/06/05/Why%20Alibaba%20/</id>
    <published>2020-06-05T14:56:00.000Z</published>
    <updated>2021-02-18T11:01:49.427Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="发展驱动力：因实际问题需解决"><a href="#发展驱动力：因实际问题需解决" class="headerlink" title="发展驱动力：因实际问题需解决"></a>发展驱动力：因实际问题需解决</h2><h5 id="C端商家和用户之间交易安全-amp-信任的问题。"><a href="#C端商家和用户之间交易安全-amp-信任的问题。" class="headerlink" title="C端商家和用户之间交易安全&amp;信任的问题。"></a>C端商家和用户之间交易安全&amp;信任的问题。</h5><p>淘宝初期发展时期，需要解决C端商家和用户之间交易安全&amp;信任的问题。卖家先发货还是买家先付款？如果双方有一方抵赖怎么办？双方都是陌生人互不相识如何建立信任？如果解决不了同时让卖家、买家同时放心的交易的问题，电商平台一定是做不起来的。当时阿里巴巴是希望第三方（银行）去完成这件事情，但是由于交易复杂性（例如，一笔交易可能就是几块钱几毛钱，相比银行的主营业务，银行最少的服务费都比这个来的多）银行不愿意完成这件事情，逼不得已使得阿里在支付技术上进行大规模推进。<br>淘宝客服系统: 购物时带来一种安全感；想买一台PC，但是这个PC不是一件预算较低商品，无论详情页写的多么具体，购物体验远远低于真人客服；</p><h5 id="技术面临大促期间的流量洪峰问题。"><a href="#技术面临大促期间的流量洪峰问题。" class="headerlink" title="技术面临大促期间的流量洪峰问题。"></a>技术面临大促期间的流量洪峰问题。</h5><p>在面对双11大促流量洪峰，双11当天客户不能流畅的进行交易，对客户而言不流畅的购物体验会对平台造成非常致命的影响（类比大学选课系统，找到自己心仪课程之后一旦点击就会卡死，等到网络恢复正常发现课程已经被选满；类比早期12306在过年期间抢不到票，大量乘客面临“过年回不了家”这种问题），如果C端客户在大促期间体验非常差也会让B端卖家失去对于平台的信任。为解决这个问题，催生云计算（海量数据处理技术、数据库底层技术）业务的发展。</p><h5 id="双11的包裹堆积配送效率低下的问题。"><a href="#双11的包裹堆积配送效率低下的问题。" class="headerlink" title="双11的包裹堆积配送效率低下的问题。"></a>双11的包裹堆积配送效率低下的问题。</h5><p>大一大二时，双十一期间快递爆仓。各种排队。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;发展驱动力：因实际问题需解决&quot;&gt;&lt;a href=&quot;#发展驱动力：因实际问题需解决&quot; class=&quot;headerlink&quot; title=&quot;发展驱动力：因实际问题需解决&quot;&gt;&lt;/a&gt;发展驱动力：因实际问题需解决&lt;/h2&gt;&lt;h5 id=&quot;C端商家和用户之间交易安</summary>
      
    
    
    
    <category term="Always Review" scheme="http://example.com/categories/Always-Review/"/>
    
    
  </entry>
  
  <entry>
    <title>Status as a Service</title>
    <link href="http://example.com/2020/04/13/Status%20as%20a%20Service/"/>
    <id>http://example.com/2020/04/13/Status%20as%20a%20Service/</id>
    <published>2020-04-13T12:13:00.000Z</published>
    <updated>2020-12-30T02:45:28.530Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="Social-network-Status-as-a-Service-社交网络：地位即服务"><a href="#Social-network-Status-as-a-Service-社交网络：地位即服务" class="headerlink" title="Social network: Status as a Service 社交网络：地位即服务"></a>Social network: Status as a Service 社交网络：地位即服务</h2><p>本文对社交网络进行研究，解释以下几个问题(问题之间并不互斥，说的是同一种观点)</p><ol><li>为什么社交软件总会过时;</li><li>为什么有些社交软件会成功，有些社交软件会失败;</li><li>抛开其他所有因素，单从社会资本/社会地位这个角度分析，如何解释问题1和问题2；</li></ol><hr><h2 id="Status-Seeking-Monkeys-人是追求社会地位的猴子"><a href="#Status-Seeking-Monkeys-人是追求社会地位的猴子" class="headerlink" title="Status-Seeking Monkeys 人是追求社会地位的猴子"></a>Status-Seeking Monkeys 人是追求社会地位的猴子</h2><center><font size="5" color="red" ><i>"It is a truth universally acknowledged, that a person in possession of little fortune, must be in want of more social capital."</i></font></center><p>一条真理：穷人、没钱的人，总是无形中更多地需要一点社会地位。</p><p>Let’s begin with two principles:  </p><ul><li><strong>People are status-seeking monkeys.</strong> 人是什么？人是追求社会地位的猴子.</li><li><strong>People seek out the most efficient path to maximizing social capital.</strong> 人，这种追求社会地位的猴子，总是在沿着最高效的路径来追求社会地位。</li></ul><p>In the past few years, much progress has been made analyzing Software as a Service (SaaS) businesses. Not as much has been made on social networks. Analysis of social networks still strikes me as being like economic growth theory long before Paul Romer’s paper on endogenous technological change. However, we can start to demystify social networks if we also think of them as SaaS businesses, but instead of software, they provide status. This post is a deep dive into what I refer to as Status as a Service (StaaS) businesses.   </p><p>过去几年中，“软件即服务”（SaaS）这种思想盛行，因此涉及到分析社交网络的问题，可以通过这种框架来思考。但是社交网络（想想facebook/twitter/instgram/snapcat/微博/微信）最本质的地方：提供一种人的地位。这种提出的思路被称为“地位即服务”（StaaS）。</p><hr><h2 id="Traditional-Network-Effects-Model-of-Social-Networks-社交网络的传统网络效应模型"><a href="#Traditional-Network-Effects-Model-of-Social-Networks-社交网络的传统网络效应模型" class="headerlink" title="Traditional Network Effects Model of Social Networks 社交网络的传统网络效应模型"></a>Traditional Network Effects Model of Social Networks 社交网络的传统网络效应模型</h2><p>One of the fundamental lessons of successful social networks is that they must first appeal to people when they have few users. Typically this is done through some form of single-user utility. This is the classic cold start problem of social.  </p><p>经典社交冷启动：成功的社交网络，有个最基本的经验，就是当刚开始没有用户的时候，必须吸引住人。典型做法是：以单个用户（利用软件）完成某个实用功能。</p><p>The second fundamental lessons is that social networks must have strong network effects so that as more and more users come aboard, the network enters a positive flywheel of growth, a compounding value from positive network effects that leads to hockey stick growth that puts dollar signs in the eyes of investors and employees alike. </p><center><font size="5" color="red"><i>"Come for the tool, stay for the network."</i></font></center><p>wrote Chris Dixon, in perhaps the most memorable maxim for how this works.  </p><p>具备强网络效应：让更多的用户入坑。进来网络的人，好比启动了一个高速转动的飞轮，然后由于积极网络效应带来的综合价值，导致投资人和员工眼里满满都是人民币。有句话叫做“为工具而来，为网络而留”，就是说的这个意思。</p><p>Even before social networks, we had Metcalfe’s Law on telecommunications networks: The value of a telecommunications network is proportional to the square of the number of connected users of the system (n^2). This ported over to social networks cleanly. It is intuitive, and it includes that tantalizing math formula that explains why growth curves for social networks bends up sharply at the ankle of the classic growth S-curve.  </p><p>在互联网社交软件出现之前，就有个叫做梅特卡夫定律，本意说的是电信网络（就是打电话），很早就是说过这个本质。电信用户的价值，正比于用户数量的平方。</p><p>But dig deeper and many many questions remain. Why do some large social networks suddenly fade away, or lose out to new tiny networks? Why do some new social networks with great single-player tools fail to transform into networks, while others with seemingly frivolous purposes make the leap? Why do some networks sometimes lose value when they add more users? What determines why different networks stall out at different user base sizes? Why do some networks cross international borders easily while others stay locked within specific countries? Why, if Metcalfe’s Law holds, do many of Facebook’s clones of other social network features fail, while some succeed, like Instagram Stories?  </p><p>但是传统网络效应模型有很多不能解释的问题。  </p><ol><li>为什么有些大型的社交网络突然留不住用户？或者被一些新出现的小型社交网络替代？</li><li>为什么有些社交网络工具不能形成网络效应，但是另一些看起来没用的却实现了质变？</li><li>为什么有些社交网络能够全世界都很流行，然而另一些却只能在本国流行出不去？</li><li>Facebook克隆（抄袭？）了很多社交软件的功能，但是一些失败了，有一些成功了（例如Instagram Stories）？</li></ol><hr><h2 id="Utility-vs-Social-Capital-Framework-实用性-v-s-社会资本"><a href="#Utility-vs-Social-Capital-Framework-实用性-v-s-社会资本" class="headerlink" title="Utility vs. Social Capital Framework 实用性 v.s. 社会资本"></a>Utility vs. Social Capital Framework 实用性 v.s. 社会资本</h2><p>事实上，社交网络应该用三跳轴线去解读。但是“娱乐性”这个维度比较复杂，简化起见采用两轴解释。  </p><p>A social network like Facebook allows me to reach lots of people I would otherwise have a harder time tracking down, and that is useful. A messaging app like WhatsApp allows me to communicate with people all over the world without paying texting or incremental data fees, which is useful. Quora and Reddit and Discord and most every social network offer some forms of utility.  </p><p>facebook：社交网络的实用性不难理解。例如Facebook能让我们接触到很多人，没有它就完全接触不到。whatsapp支持即时通讯，不用在额外花发短信钱。Quora，Reddit，Discord同理也是具备某种形式的实用性。</p><p>The other axis is, for a lack of a more precise term, the social capital axis, or the status axis. Can I use the social network to accumulate social capital? What forms? How is it measured? And how do I earn that status?   </p><p>此外谈谈社会地位这根轴。这根轴没有精确的定义。可以叫社会资本，也可以叫社会地位。那么，人可以利用社交网络积累社会资本吗？以什么形式积累社会资本？如何去衡量一个人积累了多少社会资本？另外如何通过社交网络获得社会地位？</p><p>The creation of a successful status game is so mysterious that it often smacks of alchemy.  </p><p>创造成功地位的这种局（这种游戏），实在是太迷，和炼丹差不多。</p><p>With the rise of Instagram, with its focus on photos and filters, and Snapchat, with its ephemeral messaging, and Vine, with its 6-second video limit, for a while there was a thought that new social networks would be built on some new modality of communications. That’s a piece of it, but it’s not the complete picture, and not for the reasons many people think, which is why we have seen a whole bunch of strange failed experiments in just about every odd combinations of features and filters and artificial constraints in how we communicate with each other through our phones. Remember Facebook’s Snapchat competitor Slingshot, in which you had to unlock any messages you received by responding with a message? It felt like product design by mad libs.  </p><p>社交网络的崛起， Instagram专注照片和滤镜的 ，Snapchat专注阅后即焚的 。人们一度认为新的社交网络必须建立在新的通讯形态上。但这是管中窥豹，而且搞错了因果。这也就是为什么我们会在手机通讯领域，看到一大堆失败的奇怪实验品，几乎涵盖了所有的奇特功能、滤镜特效和人为约束。还记得为了与 Snapchat 竞争，Facebook 推出的 Slingshot 吗？你必须通过回复一条消息来解锁你刚收到的消息。</p><p>When modeling how successful social networks create a status game worth playing, a useful metaphor is one of the trendiest technologies: cryptocurrency.  </p><p>如何认识：社交网络创造一种社会地位（这种看起来很迷很迷的局）用什么样的视角，用什么样的建模思路去解读呢。作者脑洞大开：用当前最时髦的技术之一：加密货币。作者认为，这种创造社会地位的玩法，和加密货币有很强的内在联系。  </p><hr><h2 id="Social-Networks-as-ICO’s-社交网络是一种ICO行为"><a href="#Social-Networks-as-ICO’s-社交网络是一种ICO行为" class="headerlink" title="Social Networks as ICO’s 社交网络是一种ICO行为"></a>Social Networks as ICO’s 社交网络是一种ICO行为</h2><p>ICO，Initial Coin Offering，首次币发行，是区块链行业术语，是一种为加密数字货币/区块链项目筹措资金的常用方式，早期参与者可以从中获得初始产生的加密数字货币作为回报。<br>How is a new social network analogous to an ICO? 社交网络和ICO有啥关系？  </p><ol><li>Each new social network issues a new form of social capital, a token. 每个社交网络都会发行一种新形态的社会资本，一种代币。</li><li>You must show proof of work to earn the token. 你必须证明自己的工作量，才能获得代币。  </li><li>Over time it becomes harder and harder to mine new tokens on each social network, creating built-in scarcity. 随着时间推移，每个社交网络上，挖掘新代币变得越来越难，从而创造出一种内在的稀缺性。  </li><li>Many people, especially older folks, scoff at both social networks and cryptocurrencies. 很多人，尤其是老人，同时看不起社交网络和加密货币。（很有意思的论据有木有）  </li></ol><p>然后作者分享了一个案例，又一次在朋友家，他朋友高中的女儿在楼上叮呤咣啷，声音吵杂，嘻嘻哈哈，不时地跺脚，还放音乐。一问咋回事。拍了Musical.ly的视频。排练非常卖力，累的上气不接下气，满脸是汗水，拍了很多遍。这是什么？请把她们的行为理解成一种工作量证明。Proof of work。  </p><p>status games of adults are already well covered by the existing media, from literature to film. Children’s status games, once familiar to us, begin to fade from our memory as time passes, and its modern forms have been drastically altered by social media. </p><p>然后作者就思考了成年人的证明社会地位的游戏，现有主流媒体把这种成年人证明社会地位的游戏都已经做的烂大街了，不管是文字还是电影。（我理解这句话。想想刷到的抖音、想想自己的朋友圈，里面一切成年人带有一点装逼的情节（装逼这里不贬义）、或者彰显自己才华、或者自己特殊才艺、特殊兴趣爱好的情节、秀自己身材样貌、搞笑天赋、科研实力、旅游过程中的心灵体验、宗教信仰、美食鸡汤，仔细想想这个是不是符合每一条，几乎每一条都是一种status）。但是儿童地位的游戏，我们曾经是熟知的那些，已经渐渐消失了。（想想我小时候有四驱车、玩具枪、整箱整箱的玩具、小霸王游戏机、地沟油零食、甚至是老师发的小红花，这些不都是小时候证明我们自己“地位”的东西吗，我们小时候就是每天都在玩这种“地位”的游戏）。但是现代的社会形态和科技形态，已经使得这种形式通过不同的社交媒体发生了变化。比如王者农药的皮肤。这不就是现在儿童的“地位”吗？</p><p>Other examples abound. Perhaps you’ve read a long and thoughtful response by a random person on Quora or Reddit, or watched YouTube vloggers publishing night after night, or heard about popular Vine stars living in houses together, helping each other shoot and edit 6-second videos. While you can outsource Bitcoin mining to a computer, people still mine for social capital on social networks largely through their own blood, sweat, and tears. </p><p>继续拓宽思维思维，还有各种五光十色、光怪陆离、屡见不鲜的社会资本/社会地位追求形式。比如Quora或者Reddit上的帖子。（想想知乎上：谢邀。人在美国，刚下飞机，国内Top2高效，cs在读，人工智能劝退，熟人太多，匿了匿了。c++ primer 强势审校。）youtube上vlog博主连夜做视频。这个时代，你可以把挖矿的工作交给一台计算机，但是人们会乐此不疲，呕心沥血的追求自己的社会资本。</p><p>Perhaps, if you’ve spent time around today’s youth, you’ve watched with a mixture of horror and fascination as a teen snaps dozens of selfies before publishing the most flattering one to Instagram, only to pull it down if it doesn’t accumulate enough likes within the first hour. It’s another example of proof of work, or at least vigorous market research. </p><p>你如果和年轻人相处过一段时间，你会发现。年轻人是一种恐惧和着迷的集合体。他们会拍10张自拍然后把自己最满意的发在instgram上；如果一不小心没人点赞，他们就偷偷删掉。（捂脸。我真干过这种事）这是“工作量证明”的另一个例子。</p><p>If you’ve ever joined one of these social networks early enough, you know that, on a relative basis, getting ahead of others in terms of social capital (followers, likes, etc.) is easier in the early days. Some people who were featured on recommended follower lists in the early days of Twitter have follower counts in the 7-figures, just as early masters of Musical.ly and Vine were accumulated massive and compounding follower counts. The more people who follow you, the more followers you gain because of leaderboards and recommended follower algorithms and other such common discovery mechanisms.  </p><p>另外社交网络这玩意有个特点，越早加入的人，社交资本（比如说量化成粉丝量、点赞数量）就更容易领先他人。这是由于目前的分发机制：不管是人工排行榜、或者推荐算法，都会基于历史统计决策。想想电商运营选品还是机器学习工程师抽统计特征，都是会引入历史统计特征。</p><p>It’s true that as more people join a network, more social capital is up for grabs in the aggregate. However, in general, if you come to a social network later, unless you bring incredible exogenous social capital (Taylor Swift can join any social network on the planet and collect a massive following immediately), the competition for attention is going to be more intense than it was in the beginning. Everyone has more of an understanding of how the game works so the competition is stiffer.  </p><p>这就带来一个问题：来的越晚的人，在自然情况下入局就更难（火起来越难）。我说下个人理解：回顾想想字节跳动长久发展的核心策略，是不是坚持一直对新的创造者人为的倾斜流量？我认为是的。这不就是一种典型通过流量调控达到的整个生态持久繁荣的目的的case吗？脑洞再开一下，上升到经济学的角度来看，个人感觉这就好比早期的自由市场，靠看不见的手调控整个生态，但是为了长久发展，政府的宏观调控使得整个经济形态健康发展而不会出现过于严重的垄断。</p><p>但是有一种例外，叫做外生社会资本（exogenous social captial），假如现在有个新的社交平台火起来。不管你多早加入，你也无法改变一个事实，泰勒多晚加入她都比你影响力大多了。  </p><hr><h2 id="Why-Proof-of-Work-Matters-为什么社交网络需要工作量证明"><a href="#Why-Proof-of-Work-Matters-为什么社交网络需要工作量证明" class="headerlink" title="Why Proof of Work Matters? 为什么社交网络需要工作量证明?"></a>Why Proof of Work Matters? 为什么社交网络需要工作量证明?</h2><p>As with cryptocurrency, if it were so easy, it wouldn’t be worth anything. Value is tied to scarcity, and scarcity on social networks derives from proof of work. Status isn’t worth much if there’s no skill and effort required to mine it. It’s not that a social network that makes it easy for lots of users to perform well can’t be a useful one, but competition for relative status still motivates humans. Recall our first tenet: humans are status-seeking monkeys. Status is a relative ladder. By definition, if everyone can achieve a certain type of status, it’s no status at all, it’s a participation trophy.  </p><p>接下来请以一个社交网络app的设计者的出发点来思考问题，为什么需要负担很重的工作量证明呢？人们如果要最大化社会资本，不应该弄简单点吗？和加密货币原理一样，容易实现的东西就是没有价值。价值就是稀缺性，社交网络的稀缺性就是工作量证明。如果不需要用户掌握任何技能，付出努力，这种地位也就自然不值钱。这里注意，这不是说社交网络让用户用起来简单点是完全没用的。但是人们会为相对地位而竞争：人是追求社会地位的猴子。社会地位是什么？是一种相对性的等级阶梯。皇帝还是平民，富人还是穷人，老师还是学生。在这种定义情况下，试想一下如果每个人都能达到某种地位，那也就没有地位这么一个概念了，因为每个人都有了，这种地位就变成了人手一份的“参与奖”没有任何意义了。  </p><p>Twitter启动Favstar和Favrd两个功能，写出全球最受欢迎推文的人，获得奖牌，瞬间让更多优质内容浮出水面。  </p><p>Thirst for status is potential energy. It is the lifeblood of a Status as a Service business. To succeed at carving out unique space in the market, social networks offer their own unique form of status token, earned through some distinctive proof of work.<br>对地位的渴望，是一股潜在能量。它是“地位即服务”的业务命脉。要想开辟一个独特的市场，社交网络必须提供一种独特的地位的代币。同时，要想争取这些代币，必须付出独特的工作量证明。  </p><p>Conversely, let’s look at something like Prisma, a photo filter app which tried to pivot to become a social network. Prisma surged in popularity upon launch by making it trivial to turn one of your photos into a fine art painting with one of its many neural-network-powered filters. It worked well. Too well. Since almost any photo could, with one-click, be turned into a gorgeous painting, no single photo really stands out. The star is the filter, not the user, and so it didn’t really make sense to follow any one person over any other person. Without that element of skill, no framework for a status game or skill-based network existed. It was a utility that failed at becoming a Status as a Service business. In contrast, while Instagram filters, in its earliest days, improved upon the somewhat limited quality of smartphone photos at the time, the quality of those photos still depended for the most part on the photographer. The composition, the selection of subject matter, these still derived from the photographer’s craft, and no filter could elevate a poor photo into a masterpiece.  </p><p>看一个反例，Prisma。这个照片滤镜app曾经尝试转型为社交网络app。刚出的时候非常受欢迎，因为用神经网络添加滤镜效果让你的照片变成精美的艺术品。但是它真的是太好用了，过于好用了。因为每一张照片只需要点击一下就变成漂亮的艺术品。所以没有照片能脱颖而出。这款软件的主角实质上是这个强大的滤镜功能，而不是用户，这就太喧宾夺主了。大家也很少care其他人。因为大家水平都差不多。  </p><p>So, to answer an earlier question about how a new social network takes hold, let’s add this: a new Status as a Service business must devise some proof of work that depends on some actual skill to differentiate among users. If it does, then it creates, like an ICO, some new form of social capital currency of value to those users.   </p><p>因此一个全新的社交网络，该如何站稳脚跟？对于前面提到的这个问题，让我们再来补充一点：全新的 “社会地位即服务” 业务，必须设计出独特的工作量证明机制，它将要求用户们展示出真实技能。社交网络将以此为依据，来划分用户。如果社交网络能做到这一点，它就能像 ICO 一样，为这些用户创造一种全新的社会资本货币。</p><hr><h2 id="Facebook’s-Original-Proof-of-Work-Facebook的最早工作量体现"><a href="#Facebook’s-Original-Proof-of-Work-Facebook的最早工作量体现" class="headerlink" title="Facebook’s Original Proof of Work  Facebook的最早工作量体现"></a>Facebook’s Original Proof of Work  Facebook的最早工作量体现</h2><p>In fact, Facebook launched with one of the most famous proof of work hurdles in the world: you had to be a student at Harvard. By requiring a harvard.edu email address, Facebook drafted off of one of the most elite cultural filters in the world. It’s hard to think of many more powerful slingshots of elitism. By rolling out, first to Ivy League schools, then to colleges in general, Facebook scaled while maintaining a narrow age dispersion and exclusivity based around educational credentials. Layer that on top of the broader social status game of stalking attractive members of the other sex that animates much of college life and Facebook was a service that tapped into reserves of some of the most heated social capital competitions in the world.  </p><p>Facebook的最早工作量证明：你必须是哈佛学生。用harvard.edu邮箱注册。然后逐渐铺开到常春藤学校，然后是全美所有大学。FB保持用户年龄分布比较集中，且学历排他。然后，更能调动大学生积极性的地位游戏是 “stalk” 其他有吸引力的异性同学。加上前面两点，Facebook 成功利用了世界上最激烈的社会资本竞争。  </p><hr><h2 id="Social-Capital-ROI-关注社会资本回报率"><a href="#Social-Capital-ROI-关注社会资本回报率" class="headerlink" title="Social Capital ROI 关注社会资本回报率"></a>Social Capital ROI 关注社会资本回报率</h2><p>If a person posts something interesting to a platform, how quickly do they gain likes and comments and reactions and followers? The second tenet is that people seek out the most efficient path to maximize their social capital. To do so, they must have a sense for how different strategies vary in effectiveness. Most humans seem to excel at this. </p><p>思考一个问题。如果一个人在社交平台发布了一条内容，他获得点赞评论关注的速度有多快？多快其实还不是重点，应该思考的是，人们有多大程度上的意愿，要快速的追求社会资本。引出第二个原则：人们在追求社会地位的时候，一定会追求最有效的途径，使得最大化社会资本。怎么才是最有效呢?   </p><p>The gradient of your network’s social capital ROI can often govern your market share among different demographics. Young girls flocked to Musical.ly in its early days because they were uniquely good at the lip synch dance routine videos that were its bread and butter. In this age of neverending notifications, heavy social media users are hyper aware of differing status ROI among the apps they use.    </p><p>社交网络中，社会资本回报率的梯度，往往可以主导你在不同人群中的市场份额。比如，年轻女孩子特别擅长对嘴舞蹈表演，所以在 Musical.ly 的早期，她们就蜂拥而至，而这些表演视频，恰恰是 Musical.ly 赖以生存的饭碗。在这个时代，社交媒体的通知推送，永无休止。重度用户都非常清楚，这些应用中，哪些有更高的回报率，能更有效地获取社会地位。</p><p>TikTok is an interesting new player in social media because its default feed, For You, relies on a machine learning algorithm to determine what each user sees; the feed of content from by creators you follow, in contrast, is hidden one pane over. If you are new to TikTok and have just uploaded a great video, the selection algorithm promises to distribute your post much more quickly than if you were on sharing it on a network that relies on the size of your following, which most people have to build up over a long period of time. Conversely, if you come up with one great video but the rest of your work is mediocre, you can’t count on continued distribution on TikTok since your followers live mostly in a feed driven by the TikTok algorithm, not their follow graph.   </p><p>TikTok是一个新的有趣社交媒体，因为它默认就是 “推荐” 页。“推荐” 页会依赖机器学习算法来确定每个用户看到的内容。相比之下，你关注的创作者所提供的内容，会被藏在旁边另一个页面上。如果你是抖音新人，并刚上传了一个精彩的视频，那么推荐算法可以更快地分发你的帖子。而如果只靠自己在社交网络圈分享，大多数人都必须花很长一段时间，才能构建起足够大的圈子。但反过来说，如果你只有一个精彩的视频，而其余的作品都很平庸，你不能指望抖音会继续帮你分发内容。因为，你的关注者主要看的，是由抖音的推荐算法驱动的信息流，而不是由他们的关注图谱驱动的信息流。</p><p>The result is a feedback loop that is much more tightly wound that that of other social networks, both in the positive and negative direction. Theoretically, if the algorithm is accurate, the content in your feed should correlate most closely to quality of the work and its alignment with your personal interests rather than the drawing from the work of accounts you follow. At a time when Bytedance is spending tens (hundreds?) of millions of marketing dollars in a bid to acquire users in international markets, the rapid ROI on new creators’ work is a helpful quality in ensuring they stick around.  </p><p>这样做的结果是：抖音的反馈链路比其他社交网络更环环紧扣，无论是正反馈还是负反馈。从理论上讲，如果推荐算法是准确的，你的信息流里出现的内容，会和内容的质量，以及你的个人兴趣关系最大，而不会和你所关注的帐户产生太大关系。眼下，字节跳动公司在市场营销上花费数千万 (数亿？) 美金，以获取国际市场上的用户。作品见效快，回报率高，这是确保新创作者能持续呆下去的法宝。</p><p>This development is interesting for another reason: graph-based social capital allocation mechanisms can suffer from runaway winner-take-all effects. In essence, some networks reward those who gain a lot of followers early on with so much added exposure that they continue to gain more followers than other users, regardless of whether they’ve earned it through the quality of their posts. One hypothesis on why social networks tend to lose heat at scale is that this type of old money can’t be cleared out, and new money loses the incentive to play the game.  </p><p>社交网络的这种演化过程很有意思，还有另一个原因：基于 “社交图谱” 的社会资本分配机制，可能会因 ”赢家通吃” 效应而失控。一些社交网络，从本质上看，给那些在早期就获得大量关注者的人，奖励了过多的曝光度；以至于之后，无论帖子质量如何，他们都可以比其他用户获得更多的关注者。为什么随着社交网络规模的变大，会出现 “蒸发冷却效应”？一个假设是，如果老贵族 (old money) 不走，新贵族 (new money) 就会失去玩游戏的动力。</p><p>The same way many social networks track keystone metrics like time to X followers, they should track the ROI on posts for new users. It’s likely a leading metric that governs retention or churn. It’s useful as an investor, or even as a curious onlooker to test a social networks by posting varied content from test accounts to gauge the efficiency and fairness of the distribution algorithm.   </p><p>很多社交网络都会跟踪关键指标。比如，获得 X 个关注者需要花多少时间。同样，它们也应该跟踪新用户帖子的回报率 (ROI)。这可能是控制留存率、流失率的一个主要指标。作为一个投资者，甚至是一个好奇的旁观者，可以通过从测试账户发布各种内容，来衡量社交网络分发算法的效率性和公平性，这非常有价值。</p><p>Whatever the mechanisms, social networks must devote a lot of resources to market making between content and the right audience for that content so that users feel sufficient return on their work. Distribution is king, even when, or especially when it allocates social capital.   </p><p>无论采用何种机制，社交网络都必须投入大量的资源，来撮合市场上的内容及其正确受众，让用户感受到努力皆有回报。分发为王，哪怕 (或者说尤其) 是在分发社会资本时。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;Social-network-Status-as-a-Service-社交网络：地位即服务&quot;&gt;&lt;a href=&quot;#Social-network-Status-as-a-Service-社交网络：地位即服务&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="Reading" scheme="http://example.com/categories/Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>Git Tutorial</title>
    <link href="http://example.com/2020/04/05/Git%20Tutorial/"/>
    <id>http://example.com/2020/04/05/Git%20Tutorial/</id>
    <published>2020-04-05T14:56:00.000Z</published>
    <updated>2021-01-13T03:52:48.171Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;github的邮箱地址&quot;</span><br></pre></td></tr></table></figure><p>一路enter<br>（通过ls查看～/.ssh下面的所有内容查看）</p><p>将刚刚创建的ssh keys添加到github中</p><ol><li>利用gedit/cat命令，查看id_rsa.pub的内容</li><li>在GitHub中，依次点击Settings -&gt; SSH Keys -&gt; Add SSH Key，将id_rsa.pub文件中的字符串复制进去，注意字符串中没有换行和空格。</li></ol><p>再次检查SSH连接情况(在～/.ssh目录下):<br>输入如下命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>看到如下所示，则表示添加成功</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hi 你的用户名! You’ve successfully authenticated, but GitHub does <span class="keyword">not</span> provide shell access.</span><br></pre></td></tr></table></figure><p>即利用自己的用户名和email地址配置git</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --<span class="keyword">global</span> user.name <span class="string">&quot;你的github用户名&quot;</span></span><br><span class="line">git config --<span class="keyword">global</span> user.email <span class="string">&quot;你的github邮箱地址&quot;</span></span><br></pre></td></tr></table></figure><p>ps: 为什么配置好了提交代码到远程分之还是要输入密码? clone的时候选择http</p><hr><h2 id="clone-pull-push"><a href="#clone-pull-push" class="headerlink" title="clone, pull, push"></a>clone, pull, push</h2><ul><li><p>clone: 克隆远程分支到本地</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone -b my_branch &lt;远程仓库地址&gt;</span><br></pre></td></tr></table></figure></li><li><p>pull: 将远程代码pull[拉取]到本地 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin branch_name</span><br></pre></td></tr></table></figure></li><li><p>push: 将本地代码push[推送]到远程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add yourfilename</span><br><span class="line">git ci -m <span class="string">&quot;说明修改内容, 常见格式: fix xx bug/add xx function/remove xx file&quot;</span></span><br><span class="line">git push origin branch_name</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="merge-v-s-rebase"><a href="#merge-v-s-rebase" class="headerlink" title="merge v.s. rebase"></a>merge v.s. rebase</h2><ul><li>将分支my_branch合并到master分支<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">git merge my_branch</span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="远程仓库被重命名"><a href="#远程仓库被重命名" class="headerlink" title="远程仓库被重命名"></a>远程仓库被重命名</h2><ul><li>远程仓库被重命名</li></ul><p>Terminal</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin master</span><br></pre></td></tr></table></figure><p>显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Counting objects: 3, done.</span><br><span class="line">Delta compression using up to 4 threads.</span><br><span class="line">Compressing objects: 100% (3&#x2F;3), done.</span><br><span class="line">Writing objects: 100% (3&#x2F;3), 294 bytes | 294.00 KiB&#x2F;s, done.</span><br><span class="line">Total 3 (delta 2), reused 0 (delta 0)</span><br><span class="line">remote: Resolving deltas: 100% (2&#x2F;2), completed with 2 local objects.</span><br><span class="line">remote: This repository moved. Please use the new location:</span><br><span class="line">remote:   https:&#x2F;&#x2F;github.com&#x2F;tetsu-upstr&#x2F;JSblog.git</span><br><span class="line">To https:&#x2F;&#x2F;github.com&#x2F;tetsu-upstr&#x2F;blog.git</span><br><span class="line">   7468063..962d5cd  master -&gt; master</span><br></pre></td></tr></table></figure><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote -v</span><br></pre></td></tr></table></figure><p>显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">origin  https:&#x2F;&#x2F;github.com&#x2F;tetsu-upstr&#x2F;blog.git (fetch)</span><br><span class="line">origin  https:&#x2F;&#x2F;github.com&#x2F;tetsu-upstr&#x2F;blog.git (push)</span><br></pre></td></tr></table></figure><p>Terminal set-url</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote set-url origin https:&#x2F;&#x2F;github.com&#x2F;tetsu-upstr&#x2F;JSblog.git</span><br></pre></td></tr></table></figure><p>检查更改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote -v</span><br></pre></td></tr></table></figure><p>显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">origin  https:&#x2F;&#x2F;github.com&#x2F;tetsu-upstr&#x2F;JSblog.git (fetch)</span><br><span class="line">origin  https:&#x2F;&#x2F;github.com&#x2F;tetsu-upstr&#x2F;JSblog.git (push)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;配置&quot;&gt;&lt;a href=&quot;#配置&quot; class=&quot;headerlink&quot; title=&quot;配置&quot;&gt;&lt;/a&gt;配置&lt;/h2&gt;&lt;p&gt;执行&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt</summary>
      
    
    
    
    <category term="Programming &amp; Data" scheme="http://example.com/categories/Programming-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>Faiss</title>
    <link href="http://example.com/2020/04/04/Faiss/"/>
    <id>http://example.com/2020/04/04/Faiss/</id>
    <published>2020-04-04T12:11:00.000Z</published>
    <updated>2021-01-08T09:33:15.977Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>A library for efficient similarity search and clustering of dense vectors.Faiss：高效稠密向量相似Top检索库;</li><li>Faiss contains several methods for similarity search. It assumes that the instances are represented as vectors and are identified by an integer, and that the vectors can be compared with L2 (Euclidean) distances or dot products. Vectors that are similar to a query vector are those that have the lowest L2 distance or the highest dot product with the query vector. It also supports cosine similarity, since this is a dot product on normalized vectors. Faiss支持多种相似度检索度量：L2距离，点积和余弦相似度。</li></ul><h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 更新conda</span></span><br><span class="line">conda update conda</span><br><span class="line"><span class="meta">#</span><span class="bash"> 先安装mkl</span></span><br><span class="line">conda install mkl</span><br><span class="line"><span class="meta">#</span><span class="bash"> gpu版本 -- 记得根据自己安装的cuda版本安装对应的faiss版本，不然会出异常</span></span><br><span class="line">conda install faiss-gpu cudatoolkit=10.0 -c pytorch # For CUDA10</span><br></pre></td></tr></table></figure><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line">d = <span class="number">64</span>                           <span class="comment"># dimension</span></span><br><span class="line">nb = <span class="number">100000</span>                      <span class="comment"># database size</span></span><br><span class="line">nq = <span class="number">10000</span>                       <span class="comment"># nb of queries</span></span><br><span class="line">np.random.seed(<span class="number">1234</span>)             <span class="comment"># make reproducible</span></span><br><span class="line">xb = np.random.random((nb, d)).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">xq = np.random.random((nq, d)).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">xb[:, <span class="number">0</span>] += np.arange(nb) / <span class="number">1000.</span></span><br><span class="line">xq[:, <span class="number">0</span>] += np.arange(nq) / <span class="number">1000.</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Faiss is built around the Index object.</span></span><br><span class="line"><span class="string">It encapsulates the set of database vectors, and optionally preprocesses them to make searching efficient.</span></span><br><span class="line"><span class="string">There are many types of indexes,</span></span><br><span class="line"><span class="string">we are going to use the simplest version that just performs brute-force L2 distance search on them: IndexFlatL2.</span></span><br><span class="line"><span class="string">All indexes need to know when they are built which is the dimensionality of the vectors they operate on, d in our case.</span></span><br><span class="line"><span class="string">Then, most of the indexes also require a training phase, to analyze the distribution of the vectors. For IndexFlatL2, we can skip this operation.</span></span><br><span class="line"><span class="string">When the index is built and trained, two operations can be performed on the index: add and search.</span></span><br><span class="line"><span class="string">To add elements to the index, we call add on xb.</span></span><br><span class="line"><span class="string">We can also display the two state variables of the index: is_trained, a boolean that indicates whether training is required and ntotal, the number of indexed vectors.</span></span><br><span class="line"><span class="string">Some indexes can also store integer IDs corresponding to each of the vectors (but not IndexFlatL2).</span></span><br><span class="line"><span class="string">If no IDs are provided, add just uses the vector ordinal as the id, ie. the first vector gets 0, the second 1, etc.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">index = faiss.IndexFlatL2(d)     <span class="comment"># build the index</span></span><br><span class="line">print(index.is_trained)          <span class="comment"># log: True</span></span><br><span class="line">index.add(xb)                    <span class="comment"># add vectors to the index</span></span><br><span class="line">print(index.ntotal)              <span class="comment"># log: 100000</span></span><br><span class="line">​</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">The basic search operation that can be performed on an index is the k-nearest-neighbor search,</span></span><br><span class="line"><span class="string">ie. for each query vector, find its k nearest neighbors in the database.</span></span><br><span class="line"><span class="string">The result of this operation can be conveniently stored in an integer matrix of size nq-by-k,</span></span><br><span class="line"><span class="string">where row i contains the IDs of the neighbors of query vector i, sorted by increasing distance.</span></span><br><span class="line"><span class="string">In addition to this matrix, the search operation returns a nq-by-k floating-point matrix with the corresponding squared distances.</span></span><br><span class="line"><span class="string">As a sanity check, we can first search a few database vectors, to make sure the nearest neighbor is indeed the vector itself.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">k = <span class="number">4</span>                          <span class="comment"># we want to see 4 nearest neighbors</span></span><br><span class="line">D, I = index.search(xb[:<span class="number">5</span>], k) <span class="comment"># sanity check</span></span><br><span class="line">print(I)</span><br><span class="line">print(D)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[  0 393 363  78]</span></span><br><span class="line"><span class="string"> [  1 555 277 364]</span></span><br><span class="line"><span class="string"> [  2 304 101  13]</span></span><br><span class="line"><span class="string"> [  3 173  18 182]</span></span><br><span class="line"><span class="string"> [  4 288 370 531]]</span></span><br><span class="line"><span class="string">[[0.        7.1751738 7.20763   7.2511625]</span></span><br><span class="line"><span class="string"> [0.        6.3235645 6.684581  6.799946 ]</span></span><br><span class="line"><span class="string"> [0.        5.7964087 6.391736  7.2815123]</span></span><br><span class="line"><span class="string"> [0.        7.2779055 7.527987  7.6628466]</span></span><br><span class="line"><span class="string"> [0.        6.7638035 7.2951202 7.3688145]]</span></span><br><span class="line"><span class="string">the nearest neighbor of each query is indeed the index of the vector,</span></span><br><span class="line"><span class="string">and the corresponding distance is 0. And within a row, distances are increasing.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">D, I = index.search(xq, k)     <span class="comment"># actual search</span></span><br><span class="line">print(I[:<span class="number">5</span>])                   <span class="comment"># neighbors of the 5 first queries</span></span><br><span class="line">print(I[<span class="number">-5</span>:])                  <span class="comment"># neighbors of the 5 last queries</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[ 381  207  210  477]</span></span><br><span class="line"><span class="string"> [ 526  911  142   72]</span></span><br><span class="line"><span class="string"> [ 838  527 1290  425]</span></span><br><span class="line"><span class="string"> [ 196  184  164  359]</span></span><br><span class="line"><span class="string"> [ 526  377  120  425]]</span></span><br><span class="line"><span class="string">[[ 9900 10500  9309  9831]</span></span><br><span class="line"><span class="string"> [11055 10895 10812 11321]</span></span><br><span class="line"><span class="string"> [11353 11103 10164  9787]</span></span><br><span class="line"><span class="string"> [10571 10664 10632  9638]</span></span><br><span class="line"><span class="string"> [ 9628  9554 10036  9582]]</span></span><br><span class="line"><span class="string">Because of the value added to the first component of the vectors,</span></span><br><span class="line"><span class="string">the dataset is smeared along the first axis in d-dim space. </span></span><br><span class="line"><span class="string">So the neighbors of the first few vectors are around the beginning of the dataset,</span></span><br><span class="line"><span class="string">and the ones of the vectors around ~10000 are also around index 10000 in the dataset.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">To speed up the search, it is possible to segment the dataset into pieces.</span></span><br><span class="line"><span class="string">We define Voronoi cells in the d-dimensional space, and each database vector falls in one of the cells.</span></span><br><span class="line"><span class="string">At search time, only the database vectors y contained in the cell the query x falls in and a few neighboring ones are compared against the query vector.</span></span><br><span class="line"><span class="string">This is done via the IndexIVFFlat index.</span></span><br><span class="line"><span class="string">This type of index requires a training stage, that can be performed on any collection of vectors that has the same distribution as the database vectors.</span></span><br><span class="line"><span class="string">In this case we just use the database vectors themselves.</span></span><br><span class="line"><span class="string">The IndexIVFFlat also requires another index, the quantizer, that assigns vectors to Voronoi cells.</span></span><br><span class="line"><span class="string">Each cell is defined by a centroid, and finding the Voronoi cell a vector falls in consists in finding the nearest neighbor of the vector in the set of centroids.</span></span><br><span class="line"><span class="string">This is the task of the other index, which is typically an IndexFlatL2.</span></span><br><span class="line"><span class="string">There are two parameters to the search method: nlist, the number of cells, and nprobe, the number of cells (out of nlist) that are visited to perform a search.</span></span><br><span class="line"><span class="string">The search time roughly increases linearly with the number of probes plus some constant due to the quantization.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">nlist = <span class="number">100</span>                       <span class="comment"># 聚类中心个数</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)  <span class="comment"># the other index</span></span><br><span class="line">index = faiss.IndexIVFFlat(quantizer, d, nlist)</span><br><span class="line"><span class="keyword">assert</span> <span class="keyword">not</span> index.is_trained</span><br><span class="line">index.train(xb)</span><br><span class="line"><span class="keyword">assert</span> index.is_trained</span><br><span class="line"></span><br><span class="line">index.add(xb)                  <span class="comment"># add may be a bit slower as well</span></span><br><span class="line">D, I = index.search(xq, k)     <span class="comment"># actual search</span></span><br><span class="line">print(I[<span class="number">-5</span>:])                  <span class="comment"># neighbors of the 5 last queries</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[ 9900  9309  9810 10048]</span></span><br><span class="line"><span class="string"> [11055 10895 10812 11321]</span></span><br><span class="line"><span class="string"> [11353 10164  9787 10719]</span></span><br><span class="line"><span class="string"> [10571 10664 10632 10203]</span></span><br><span class="line"><span class="string"> [ 9628  9554  9582 10304]]</span></span><br><span class="line"><span class="string">The values are similar, but not exactly the same as for the brute-force search (see above).</span></span><br><span class="line"><span class="string">This is because some of the results were not in the exact same Voronoi cell.</span></span><br><span class="line"><span class="string">Therefore, visiting a few more cells may prove useful. &#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">index.nprobe = <span class="number">10</span>              <span class="comment"># default nprobe is 1, try a few more 默认聚类中心个数1, 尝试调大中心数10</span></span><br><span class="line">D, I = index.search(xq, k)</span><br><span class="line">print(I[<span class="number">-5</span>:])                  <span class="comment"># neighbors of the 5 last queries</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[ 9900 10500  9309  9831]</span></span><br><span class="line"><span class="string"> [11055 10895 10812 11321]</span></span><br><span class="line"><span class="string"> [11353 11103 10164  9787]</span></span><br><span class="line"><span class="string"> [10571 10664 10632  9638]</span></span><br><span class="line"><span class="string"> [ 9628  9554 10036  9582]]</span></span><br><span class="line"><span class="string">which is the correct result.</span></span><br><span class="line"><span class="string">Note that getting a perfect result in this case is merely an artifact of the data distribution,</span></span><br><span class="line"><span class="string">as it is has a strong component on the x-axis which makes it easier to handle.</span></span><br><span class="line"><span class="string">The nprobe parameter is always a way of adjusting the tradeoff between speed and accuracy of the result.</span></span><br><span class="line"><span class="string">Setting nprobe = nlist gives the same result as the brute-force search (but slower).</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ngpus = faiss.get_num_gpus()</span><br><span class="line">print(<span class="string">&#x27;number of GPUS:&#x27;</span>, ngpus)</span><br><span class="line">res = faiss.StandardGpuResources()</span><br><span class="line"><span class="comment"># build a flat (CPU) index</span></span><br><span class="line">index_flat = faiss.IndexFlatL2(d)</span><br><span class="line"><span class="comment"># make it into a gpu index</span></span><br><span class="line">gpu_index_flat = faiss.index_cpu_to_gpu(res, <span class="number">0</span>, index_flat)</span><br><span class="line">gpu_index_flat.add(xb)</span><br><span class="line">print(gpu_index_flat.ntotal)</span><br><span class="line">D, I = gpu_index_flat.search(xq, k)     <span class="comment"># actual search</span></span><br><span class="line">print(I[:<span class="number">5</span>])                            <span class="comment"># neighbors of the 5 first queries</span></span><br><span class="line">print(I[<span class="number">-5</span>:])                           <span class="comment"># neighbors of the 5 last queries</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># add self defined index</span></span><br><span class="line">index = faiss.IndexFlatL2(xb.shape[<span class="number">1</span>])</span><br><span class="line">ids = np.arange(xb.shape[<span class="number">0</span>]) + <span class="number">231</span></span><br><span class="line">index2 = faiss.IndexIDMap(index)</span><br><span class="line">index2.add_with_ids(xb, ids)</span><br><span class="line">k = <span class="number">4</span>                                   <span class="comment"># we want to see 4 nearest neighbors</span></span><br><span class="line">D, I = index2.search(xq, k)             <span class="comment"># actual search</span></span><br><span class="line">print(I[:<span class="number">5</span>])                            <span class="comment"># neighbors of the 5 first queries</span></span><br><span class="line">print(I[<span class="number">-5</span>:])                           <span class="comment"># neighbors of the 5 last queries</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;A library for efficient similarity search and clustering of</summary>
      
    
    
    
    <category term="RecSys &amp; Machine Learning" scheme="http://example.com/categories/RecSys-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>The Economics of Money, Banking, and Financial Markets</title>
    <link href="http://example.com/2020/04/04/The%20Economics%20of%20Money,%20Banking,%20and%20Financial%20Markets/"/>
    <id>http://example.com/2020/04/04/The%20Economics%20of%20Money,%20Banking,%20and%20Financial%20Markets/</id>
    <published>2020-04-04T12:11:00.000Z</published>
    <updated>2020-12-30T02:51:10.152Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="理解利率"><a href="#理解利率" class="headerlink" title="理解利率"></a>理解利率</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;理解利率&quot;&gt;&lt;a href=&quot;#理解利率&quot; class=&quot;headerlink&quot; title=&quot;理解利率&quot;&gt;&lt;/a&gt;理解利率&lt;/h2&gt;</summary>
      
    
    
    
    <category term="Reading" scheme="http://example.com/categories/Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>狼人杀如何致胜</title>
    <link href="http://example.com/2020/03/22/%E7%8B%BC%E4%BA%BA%E6%9D%80%E5%A6%82%E4%BD%95%E8%87%B4%E8%83%9C/"/>
    <id>http://example.com/2020/03/22/%E7%8B%BC%E4%BA%BA%E6%9D%80%E5%A6%82%E4%BD%95%E8%87%B4%E8%83%9C/</id>
    <published>2020-03-21T18:12:57.000Z</published>
    <updated>2020-12-13T12:15:24.165Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>到底是一个重点是靠盘逻辑的游戏？还是一个缺少逻辑、更多的是表演/发言/说谎的游戏？逻辑在获胜上占到多少比重？</li><li>表演是这个游戏里面重要的部分。如何正确的从自己的角色表演，达到获胜的目的。</li><li>作为游戏，均存在玩物丧志属性，总结游戏玩法也是符合让这个游戏对于我的边际效益最小化的目的，鼓励投入到新领域中去。</li><li>本文重点是9人国标局。3民3狼1预1巫1猎。12人局存在玩家质量参差不齐的局面。</li></ul><h2 id="获胜之道"><a href="#获胜之道" class="headerlink" title="获胜之道"></a>获胜之道</h2><ul><li>总结一下玩到现在如何获得胜利的方法，理论不一定严谨，整体目的是避免一些错误的发言和节奏，梳理一些关键性的要点，在整体上最大化胜率。重点解析预言家和狼人这两个角色。</li></ul><h5 id="预言家"><a href="#预言家" class="headerlink" title="预言家"></a>预言家</h5><ul><li>预言家是9人局胜率最低的角色，这是由于规则决定的（大概率双神起跳）。</li><li>作为预言家必须非常坦然接受你死的很快或者你死的很冤的事实。</li><li>发言的时候必须快速清晰的报出来查验，以免还没有说出来任何查验只透露自己身份然后被狼人自爆。预言家在遗言的时候。一定要多说。即使是非常冤死。但是真预言家的求生欲一定不能过低。</li></ul><h5 id="狼人"><a href="#狼人" class="headerlink" title="狼人"></a>狼人</h5><ul><li>狼人的胜利关键在于“顺势而为”，三只狼人需要各自独立地根据场上的形势因时制宜的选择悍跳/划水/引导/站边/跟票/冲票。</li><li>同时三狼要有一定程度的分工，整体态势太趋同（都怂或者都悍跳）风险的。</li></ul><h2 id="案例复盘"><a href="#案例复盘" class="headerlink" title="案例复盘"></a>案例复盘</h2><h4 id="失败的一局复盘"><a href="#失败的一局复盘" class="headerlink" title="失败的一局复盘"></a>失败的一局复盘</h4><div style="text-align: center;"><img alt="" src="https://i.loli.net/2020/09/14/QYTr83RcIsd4qWh.jpg" style="display: inline-block;" width="200"/></div><ul><li>图上这局是我玩的比较失败的一局。结果：作为好人阵营（猎人）我们输的非常彻底，被一只悍跳狼完全带了节奏，狼人预言家身份坐实，民整体没有视角做得出的选择也都是错误的，神职女巫和猎人完全齐齐的走向了错误的做法。</li></ul><p>这局的特殊在于 </p><ol><li>一局下来好人之间几乎没有形成提供有效的通讯（统计来看9人国标局好人输概率大于50%，但是好人们几乎完全没有相互之间的信息认可，这种是绝对不算多的案例）。</li><li>悍跳狼第一天就被投死，其余怂狼从头到尾也没有也没有太多行动。但是好人掉进圈套里完全出不来，后续决策是一连串错误，而且是走向那种温水煮青蛙的错误节奏。</li><li>整局游戏来看，好人绝对不是一定输，是肯定机会逆转的，但是整体上我认为好人如果想赢非常艰难，故做一下复盘。</li></ol><h4 id="重点历程"><a href="#重点历程" class="headerlink" title="重点历程"></a>重点历程</h4><ol start="0"><li><p>狼人刀3号女巫，3女巫自救。9预言家8金水。</p></li><li><p>第一天9号第一个发言，说自己预言家给8号的民发了金水，发言过程中没有明显语气或者逻辑上的问题。然后说由于7、8、9位置学，轻踩一下7号。接着顺次12345发言均没有有效信息，其中女巫跳出来身份说明是自救的。<br>8号是归票位，给8发出去金水，然后8又没有身份，我当时认为信服力不一定。<br>但是8不是神职，如果9是狼人发到8民身上，可能性也是存在的，发言上没有明显问题，当时我没有完全不信或者不信。</p></li><li><p>然后6号发言，给7号发了查杀。<br>从位置上来看，9预言家8号金水则7号位置有些尴尬。<br>6给7查杀，当时我是不太信的，然后7号是民，但7的角度而言，6和9对7都有一定攻击性。但6是查杀，他从自身身份出发只能从发言打死6。</p></li><li><p>第一天投票：我上票给了7号，6整体发言语气和逻辑都还好（6查验的是后位置的7有合理性，感觉9的力度不太好说，其实我这里操作有一定的赌博性，我是猎人从容错的角度出发可以这么投）。但是结果4，7，8，9投6，结果6出局。<br>这里狼人有个破绽，2和5选择了弃票。但当时在69发言都没有明显破绽的情况下，我对这个弃票没有太怀疑，思考的重点还是6/9谁是真预言家上。</p></li><li><p>6号发表遗言：<br>6发言整体比较激动，语气没问题像真预言家，用了4张加时卡，逻辑上看整体发言就表达一个他的逻辑：9预8金，则7是位置及其尴尬的一张牌。如果6是一只狼人，为什么不隐藏自己的身份，大概率把7打出去呢。为什么一定要站出来把7打死，铁铁的要打死这张7呢。<br>这里无非两种情况（当时我没有把这两种情况盘的非常清楚）：</p><ol><li>6是真预言家，验出来7是狼人，然后从到尾踩死这个7。 </li><li>6是狼人，这个狼人选择很冒险却收益很小的一步，冒着自己被投出去的风险杀死7这个民。他的风险在于大家可能不信他是真预言家（事实上第一轮投票大家真没相信）。因为9首位发言给归票位金水预言家面还是有一些的。<br>然后这里1我、3巫、还有4民这三个人无一例外心路历程都是上述第1种，相信了6是真预言家。当时怎么看觉得呢这个6非要踩死这个7，从他的狼人视角来看真的收益好小。</li></ol></li><li><p>第二天晚上9真预查验的是1猎（就是我）是好人，女巫因为是认定6真预言家，所以必然是要开毒，选择毒7。狼人杀了3女巫。<br>然后第二天我是最后发言的归票位。然后第二天白天发言2号没什么信息，3女巫被杀，4民5狼发言如下：<br>4民说了自己觉得信了6真预言家，理由就是感觉上一局6狼的话收益太小。且现在9还活着。表示想上票给9。<br>5狼发言表示和4的逻辑一样，同理觉得6狼的话收益太小。同想上票9。这时候我的心路历程和他俩一样一样的。<br>9真预言家发言，说查验了我是好人。但是9没有怎么盘6的问题。我没有信他的金水。 我觉得9的验人这个力度从9是好人说没问题，因为上一局我给他上了一票，是正确的验人选择。但是反过来从9狼人说也是存在可能性的，拉拢一个好人进来。另外我觉得2如果是真预言家在2天没死的情况下又2天验出来都是好人，内心当时确实存疑。其实从复盘看，他的验人选择真的就是单纯的真预言家好人的视角的做法没什么多想的，但是当时我和其他好人4、8真的没有认下来他的真预言家身份。<br>8民对于9第一天给他的金水不敢接。且他认为狼人第二天没杀9他心存疑惑。8表示要投票给9。<br>1猎人我，我逻辑和4和5一样也走在相信6的道路上。一方面是信了6，另一方面9的发言似乎没怎么针对过6号。这个我心存疑惑。然后我就balabala一直分析6如果是狼人第一天收益太小的事实，希望大家上票给9。</p></li><li><p>结果：大家上票给9。9出局，9没有太深入的盘逻辑。</p></li><li><p>第三天夜晚，狼人选择了杀8号。场上剩下四个人：1猎2狼4民5狼。双狼控场游戏结束。</p></li></ol><h4 id="要点总结"><a href="#要点总结" class="headerlink" title="要点总结"></a>要点总结</h4><ol><li><p>狼怎么玩的：<br>6狼（胜利方mvp）给7踩死的查杀。这种其实是根据位置学的反逻辑。用冒着自己的危险踩死7的视角，尝试自己的死换来的是强有力的预言家，表面看坐实的程度不低。<br>2号怂狼没有透出任何信息。<br>5狼略微引导了一下6是预言家的逻辑，但是1猎3巫4民确实是想到一块了（我当时确实从6悍跳预言家收益晓得角度认了6预言家，且在我之前发言的4民也是想到了且发言的），并不是因为跟了5的逻辑。</p></li><li><p>好人怎么玩的：<br>1猎，3巫，4民：其中这三个都是认为6作为狼人可能的收益很小的角度上，认定6可能是真预言家。后面女巫顺势开毒。其他均给9上票。<br>8民，第一天金水没有接也合理。因为6狼发言没有太大问题，9预言家的金水给8也不一定能坐实9。</p></li><li><p>这一局的独特性，为什么我总觉得这局好人难赢，为什么说这一局不容易太逆转。</p><ol><li>首先女巫自救没有银水。少验一个人。</li><li>预言家是首置位发言，给8民一个金水。8民是金水正好也是归票位的金水。在8是民的情况下。金水可接可不接。预言家可认可不认。（我觉得这里是在赌概率）。且第二天白天预言家这个发言我觉得有不足。</li><li>7号民牌，开不了视角。只能反打6，9号一开始根据位置学轻踩7，能说的大致就那么多。</li><li>2和5第一天的弃票是狼人一个重要的破绽，回过头来想想第一天好人正常逻辑大概率是不会弃票，肯定是6和7的局。弃票则说明真的狼面很大。但是为什么这个看起来暴露的弃票行为没有获得理论上足够的重视呢？我理解因为6最后遗言这一手。基本上把所有的思路都引入在真预言家身上=&gt;所有好人的当务之急，或者急需抿身份的思路都是【谁是真预言家】这个问题。如果真是6预9狼。9是要马上要投出去的牌。或者要干掉这个被6验出来的7号或者从位置学上被尴尬的7号。总之好人视角来看，7/9两张牌都有争议。无论是7/9出1狼或者7和9狼踩狼，【排7/9的身份或者狼坑】整体执行优先级都高于对于【好人对弃票的反馈】。</li></ol></li></ol><h4 id="为什么这局难以逆转"><a href="#为什么这局难以逆转" class="headerlink" title="为什么这局难以逆转"></a>为什么这局难以逆转</h4><p>如果发生以下条件，那么6号预言家难坐实，局势也不会这么发展。</p><ol><li>第一天狼人没有刀女巫。换来一个银水。</li><li>7号8号两个里面存在一个神。真实情况是7、8两位置连民，都整体缺少视角，7被毒冤死，另8缺少足够信息。</li><li>第一天9号真预言家不是第一个发言。且给的不是归票位验人。</li><li>真预言家在2天内至少验出来一匹狼。且最好第二天验出来狼。1/2/4/5的身份从头到尾巴除了2/5第一天弃票这个信息。基本上发言没有透露太多信息或者破绽。好人从头到位都有点缺乏信息。</li></ol><h4 id="这局如果好人想赢，怎么赢："><a href="#这局如果好人想赢，怎么赢：" class="headerlink" title="这局如果好人想赢，怎么赢："></a>这局如果好人想赢，怎么赢：</h4><ol><li><p>真预言家9，作为场上唯一有视野的人：</p><ol><li>努力去向全场好人去仔仔细细盘6号这匹狼的狼人意图。语态上始终坚持狠狠的打死6这只狼不动摇。（我认为这个不太容易让好人相信，但是是必要要尝试去做的不能不这么做。）</li><li>努力推进打平衡局的玩法。暂时保住7号。表示自己可以扛出，将7当做反向金水。一定建议让女巫不要毒7，哪怕毒自己，打一个平衡局，自己扛出。走1预1狼平衡局，真预言家出局后续走向纯盘逻辑的局。</li></ol></li><li><p>第二天的金水我接住。认同9号的验人逻辑。就是因为我第一天给他上票他验了我。结果是金水。就是这么回事没什么好说的。然后我转过来思维从剩余的人里面出。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;到底是一个重点是靠盘逻辑的游戏？还是一个缺少逻辑、更多的是表演/发言/说谎的游戏？逻辑在获胜上占到多少比重？&lt;/li&gt;</summary>
      
    
    
    
    <category term="Always Review" scheme="http://example.com/categories/Always-Review/"/>
    
    
  </entry>
  
  <entry>
    <title>David Stern, who turned NBA into powerhouse</title>
    <link href="http://example.com/2020/02/18/David%20Stern,%20who%20turned%20NBA%20into%20powerhouse/"/>
    <id>http://example.com/2020/02/18/David%20Stern,%20who%20turned%20NBA%20into%20powerhouse/</id>
    <published>2020-02-18T14:56:00.000Z</published>
    <updated>2020-12-30T03:43:40.003Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="NBA，一个世界联盟"><a href="#NBA，一个世界联盟" class="headerlink" title="NBA，一个世界联盟"></a>NBA，一个世界联盟</h2><ul><li>从小学时期到现在（2020），NBA贯穿了我的整个成长过程。今天，NBA是一个强大的商业联盟，是一种文化现象，但在早期是一个美国非常边缘的联赛。</li><li>大卫斯特恩，已故NBA总裁在30年间做对了几个关键决策。本文思考NBA如何从一个美国边缘联赛，成为一种席卷全球的文化现象。</li></ul><hr><h2 id="大卫·斯特恩留给NBA的遗产"><a href="#大卫·斯特恩留给NBA的遗产" class="headerlink" title="大卫·斯特恩留给NBA的遗产"></a>大卫·斯特恩留给NBA的遗产</h2><ul><li>一句话总结：斯特恩把NBA从”职业篮球比赛”变成了”现代体育秀”。NBA不是单纯的一种体育竞技，而是成了一种强大制星能力、强娱乐性的大众文化。有明星、有恩怨、有表演。</li><li>改变NBA的规则，让观众觉得不虚此行、看的过瘾。</li></ul><hr><h5 id="竞技平衡性的政策"><a href="#竞技平衡性的政策" class="headerlink" title="竞技平衡性的政策"></a>竞技平衡性的政策</h5><p>Why平衡性?</p><ul><li>NBA每支球队处于美国不同的城市，经济水平方差很大。大城市薪水高，小城市开不出薪水。实力越强的球员大概率就去大城市。所以强队越强，弱队越弱。是一种不够健康的发展态势。</li><li>从观众的角度来看，强队虐弱队，缺乏看点。水平旗鼓相当的对抗，最吸引人。因此问题转化为，如何让整体联盟每只球队实力差距不要太大。</li></ul><h5 id="出台”工资帽”制度。"><a href="#出台”工资帽”制度。" class="headerlink" title="出台”工资帽”制度。"></a>出台”工资帽”制度。</h5><ul><li>每个球队工资总和有个硬性上限，大幅度在整体上实力差距很小。（不是没有差距，是让这种差距在一种可以良性发展的范围内，可以接受的范围内）</li></ul><h5 id="调整选秀政策"><a href="#调整选秀政策" class="headerlink" title="调整选秀政策"></a>调整选秀政策</h5><ul><li>改变早期选秀制度的弊端。成绩越差的球队，下一年会有更好的顺位。因此有的球队在第一年肉眼可见拿不到好成绩的时候，就会选择”来年再战，破罐子破摔”这种策略。因此战绩惨不忍睹，观赏性下降。</li><li>排名垫底的若干只球队，都有概率获得状元签，而不是最差的那一只球队。个人感觉就是引入了一些噪声，使得最垫底的球队，自己的想法实现起来有巨大困难，迫使整体而言还是需要把名次向上提升。</li></ul><h5 id="政策鼓励球员得分"><a href="#政策鼓励球员得分" class="headerlink" title="政策鼓励球员得分"></a>政策鼓励球员得分</h5><ul><li>2004引入No Handcheck政策，防守者对于进攻者的身体接触，不能太大。说白了就是让球员的阻碍作用不能太狠，大大增加了更多上篮得分机会。典型的例子就是哈登这种，对于类似的规则相当收益。大大提升了外线球员爆炸性得分的可能性。</li><li>这种政策的效果就是对于造星的效果提升十分明显。有更多的球员有个人突出的表现。明星效应来的比之前强很多。</li></ul><h5 id="改变NBA的产品形式：从”比赛”到”球星”"><a href="#改变NBA的产品形式：从”比赛”到”球星”" class="headerlink" title="改变NBA的产品形式：从”比赛”到”球星”"></a>改变NBA的产品形式：从”比赛”到”球星”</h5><ul><li><p>相比于打造比赛，斯特恩更专注与把一个个球员打造成有故事的产品。你想起来一些球员，可能不是他的名字，而是外号。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">约翰逊、拉里伯德、乔丹、詹姆斯、杜兰特、贾巴尔、加内特、邓肯、罗斯、霍华德</span><br></pre></td></tr></table></figure><p>对比</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;魔术师&quot;、&quot;大鸟&quot;、&quot;飞人&quot;、&quot;小皇帝&quot;、&quot;死神&quot;、&quot;天沟&quot;、&quot;狼王&quot;、&quot;石佛&quot;、&quot;风城玫瑰&quot;、&quot;魔兽&quot;</span><br></pre></td></tr></table></figure><p>下面一行称号显然比上面那行更有吸引力、有故事感。</p></li><li><p>塑造豪门对决和恩怨情仇。例如，拉里伯德 v.s. 魔术师这两个时代巨星。凯尔特人 v.s. 湖人两支历史豪门对决</p></li><li><p>热火三巨头：巅峰老詹、巅峰韦德、龙王波什。三个当之无愧的球队老大放在一支球队，能否无敌？</p></li><li><p>各种”反叛”、”背叛”的文化。每段时间都会塑造一些争议人物。杜兰特投敌是对是错？给我的角度我认为追求总冠军是是没错的，同时对于俄克拉荷马也不欠什么，但是大多数球迷可能认为杜兰特背叛了俄克拉荷马。</p></li><li><p>造星推进手段1：全明星赛。NBA全明星赛是造星思想的最好体现。开场前的主持人依次介绍每个球员。还会请明星来演唱。绝对不仅仅是一场单纯的经济体育。在全明星赛上面基本上球员不怎么加强防守（多数只有比赛最后一节才认真），以各种夸张的扣篮、空接为主，即使是不爱看篮球的，看完都会很过瘾。</p></li></ul><p>拉文罚球线扣篮</p><ul><li>造星推进手段2：”圣诞大战” 。安排最强的几支队伍在圣诞节期间强强对话，几乎是额外增加一次重演季后赛决赛或者东西部半决赛的看点。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;NBA，一个世界联盟&quot;&gt;&lt;a href=&quot;#NBA，一个世界联盟&quot; class=&quot;headerlink&quot; title=&quot;NBA，一个世界联盟&quot;&gt;&lt;/a&gt;NBA，一个世界联盟&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;从小学时期到现在（2020），NBA贯穿了我的整个成长</summary>
      
    
    
    
    <category term="Reading" scheme="http://example.com/categories/Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux Commands</title>
    <link href="http://example.com/2020/02/18/Linux%20Commands/"/>
    <id>http://example.com/2020/02/18/Linux%20Commands/</id>
    <published>2020-02-18T14:56:00.000Z</published>
    <updated>2020-12-13T12:15:24.145Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="du"><a href="#du" class="headerlink" title="du"></a>du</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">du (disk usage) : 查看磁盘使用空间；</span><br><span class="line">du -sh filename/dirname</span><br></pre></td></tr></table></figure><h2 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h2><ul><li>When you execute a Unix job in the background ( using &amp;, bg command), and logout from the session, your process will get killed. You can avoid this using several methods — executing the job with nohup, or making it as batch job using at, batch or cron command. Nohup stands for no hang up, which can be executed as shown below. nohup syntax:<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup command &amp;</span><br></pre></td></tr></table></figure></li><li>Nohup is very helpful when you have to execute a shell-script or command that take a long time to finish. In that case, you don’t want to be connected to the shell and waiting for the command to complete. Instead, execute it with nohup, exit the shell and continue with your other work.<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -aux | grep charm</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;du&quot;&gt;&lt;a href=&quot;#du&quot; class=&quot;headerlink&quot; title=&quot;du&quot;&gt;&lt;/a&gt;du&lt;/h2&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;</summary>
      
    
    
    
    <category term="Programming &amp; Data" scheme="http://example.com/categories/Programming-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>Recurrent Neural Network</title>
    <link href="http://example.com/2020/02/13/Recurrent%20Neural%20Network/"/>
    <id>http://example.com/2020/02/13/Recurrent%20Neural%20Network/</id>
    <published>2020-02-13T14:56:00.000Z</published>
    <updated>2021-01-08T09:33:29.429Z</updated>
    
    <content type="html"><![CDATA[<hr>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
</summary>
      
    
    
    
    <category term="RecSys &amp; Machine Learning" scheme="http://example.com/categories/RecSys-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Tensorflow API</title>
    <link href="http://example.com/2020/02/13/Tensorflow%20API/"/>
    <id>http://example.com/2020/02/13/Tensorflow%20API/</id>
    <published>2020-02-13T14:56:00.000Z</published>
    <updated>2020-12-13T12:13:41.456Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="tf-nn-embedding-lookup"><a href="#tf-nn-embedding-lookup" class="headerlink" title="tf.nn.embedding_lookup()"></a>tf.nn.embedding_lookup()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup(</span><br><span class="line">    params,</span><br><span class="line">    ids,</span><br><span class="line">    partition_strategy=<span class="string">&#x27;mod&#x27;</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    validate_indices=<span class="literal">True</span>,</span><br><span class="line">    max_norm=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Looks up ids in a list of embedding tensors. This function is used to perform parallel lookups on the list of tensors in params. It is a generalization of tf.gather, where params is interpreted as a partitioning of a large embedding tensor. params may be a PartitionedVariable as returned by using tf.get_variable() with a partitioner. 按照ids顺序返回params中的第ids行。</p><ul><li>If len(params) &gt; 1, each element id of ids is partitioned between the elements of params according to the partition_strategy. In all strategies, if the id space does not evenly divide the number of partitions, each of the first (max_id + 1) % len(params) partitions will be assigned one more id.</li><li>If partition_strategy is “mod”, we assign each id to partition p = id % len(params). For instance, 13 ids are split across 5 partitions as: [[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]</li><li>If partition_strategy is “div”, we assign ids to partitions in a contiguous manner. In this case, 13 ids are split across 5 partitions as: [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]</li></ul><p>The results of the lookup are concatenated into a dense tensor. The returned tensor has shape shape(ids) + shape(params)[1:].</p><p>demo</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">a = np.array(</span><br><span class="line">    [[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>],</span><br><span class="line">     [<span class="number">1.1</span>, <span class="number">1.2</span>, <span class="number">1.3</span>],</span><br><span class="line">     [<span class="number">2.1</span>, <span class="number">2.2</span>, <span class="number">2.3</span>],</span><br><span class="line">     [<span class="number">3.1</span>, <span class="number">3.2</span>, <span class="number">3.3</span>],</span><br><span class="line">     [<span class="number">4.1</span>, <span class="number">4.2</span>, <span class="number">4.3</span>]])</span><br><span class="line">idx1 = tf.Variable([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>], tf.int32)</span><br><span class="line">idx2 = tf.Variable([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>]], tf.int32)</span><br><span class="line">out1 = tf.nn.embedding_lookup(a, idx1)</span><br><span class="line">out2 = tf.nn.embedding_lookup(a, idx2)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(a)</span><br><span class="line">    print(<span class="string">&#x27;=========&#x27;</span>)</span><br><span class="line">    print(sess.run(out1))</span><br><span class="line">    print(<span class="string">&#x27;=========&#x27;</span>)</span><br><span class="line">    print(sess.run(out2))</span><br></pre></td></tr></table></figure><p>result</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line"> [<span class="number">1.1</span> <span class="number">1.2</span> <span class="number">1.3</span>]</span><br><span class="line"> [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line"> [<span class="number">3.1</span> <span class="number">3.2</span> <span class="number">3.3</span>]</span><br><span class="line"> [<span class="number">4.1</span> <span class="number">4.2</span> <span class="number">4.3</span>]]</span><br><span class="line">=========</span><br><span class="line">[[<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line"> [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line"> [<span class="number">3.1</span> <span class="number">3.2</span> <span class="number">3.3</span>]</span><br><span class="line"> [<span class="number">1.1</span> <span class="number">1.2</span> <span class="number">1.3</span>]]</span><br><span class="line">=========</span><br><span class="line">[[[<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line">  [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line">  [<span class="number">3.1</span> <span class="number">3.2</span> <span class="number">3.3</span>]</span><br><span class="line">  [<span class="number">1.1</span> <span class="number">1.2</span> <span class="number">1.3</span>]]</span><br><span class="line"> [[<span class="number">4.1</span> <span class="number">4.2</span> <span class="number">4.3</span>]</span><br><span class="line">  [<span class="number">0.1</span> <span class="number">0.2</span> <span class="number">0.3</span>]</span><br><span class="line">  [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]</span><br><span class="line">  [<span class="number">2.1</span> <span class="number">2.2</span> <span class="number">2.3</span>]]]</span><br></pre></td></tr></table></figure><h2 id="tf-sequence-mask"><a href="#tf-sequence-mask" class="headerlink" title="tf.sequence_mask"></a>tf.sequence_mask</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.sequence_mask(</span><br><span class="line">    lengths,</span><br><span class="line">    maxlen=<span class="literal">None</span>,</span><br><span class="line">    dtype=tf.bool,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Returns a mask tensor representing the first N positions of each cell. If lengths has shape [d_1, d_2, …, d_n] the resulting tensor mask has dtype dtype and shape [d_1, d_2, …, d_n, maxlen], with<br>mask[i_1, i_2, …, i_n, j] = (j &lt; lengths[i_1, i_2, …, i_n])</p><p>Examples:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.sequence_mask([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>], <span class="number">5</span>) </span><br><span class="line"><span class="comment"># [[True, False, False, False, False], </span></span><br><span class="line"><span class="comment"># [True, True, True, False, False], </span></span><br><span class="line"><span class="comment"># [True, True, False, False, False]] </span></span><br></pre></td></tr></table></figure><p>另外</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.sequence_mask([[<span class="number">1</span>, <span class="number">3</span>],[<span class="number">2</span>,<span class="number">0</span>]]) </span><br><span class="line"><span class="comment"># [[[True, False, False], </span></span><br><span class="line"><span class="comment"># [True, True, True]], </span></span><br><span class="line"><span class="comment"># [[True, True, False], </span></span><br><span class="line"><span class="comment"># [False, False, False]]]</span></span><br></pre></td></tr></table></figure><h2 id="tf-expand-dims"><a href="#tf-expand-dims" class="headerlink" title="tf.expand_dims"></a>tf.expand_dims</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims( </span><br><span class="line">    input, </span><br><span class="line">    axis=<span class="literal">None</span>, </span><br><span class="line">    name=<span class="literal">None</span>, </span><br><span class="line">    dim=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>See the guide: Tensor Transformations &gt; Shapes and Shaping</p><p>Inserts a dimension of 1 into a tensor’s shape.</p><p>Given a tensor input, this operation inserts a dimension of 1 at the dimension index axis of input’s shape. The dimension index axis starts at zero; if you specify a negative number for axis it is counted backward from the end.</p><p>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape [height, width, channels], you can make it a batch of 1 image with expand_dims(image, 0), which will make the shape [1, height, width, channels].</p><p>Other examples:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &#x27;t&#x27; is a tensor of shape [2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">0</span>))  <span class="comment"># [1, 2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">-1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># &#x27;t2&#x27; is a tensor of shape [2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">0</span>))  <span class="comment"># [1, 2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">2</span>))  <span class="comment"># [2, 3, 1, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">3</span>))  <span class="comment"># [2, 3, 5, 1]</span></span><br></pre></td></tr></table></figure><p>This operation requires that:</p><p>-1-input.dims() &lt;= dim &lt;= input.dims()</p><p>This operation is related to squeeze(), which removes dimensions of size 1.</p><h2 id="tf-gather"><a href="#tf-gather" class="headerlink" title="tf.gather()"></a>tf.gather()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.gather( </span><br><span class="line">    params, </span><br><span class="line">    indices, </span><br><span class="line">    validate_indices=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>, </span><br><span class="line">    axis=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>indices must be an integer tensor of any dimension (usually 0-D or 1-D). Produces an output tensor with shape params.shape[:axis] + indices.shape + params.shape[axis + 1:] where:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.range(<span class="number">0</span>, <span class="number">10</span>)*<span class="number">10</span> + tf.constant(<span class="number">1</span>, shape=[<span class="number">10</span>])</span><br><span class="line">y = tf.gather(x, [<span class="number">1</span>, <span class="number">5</span>, <span class="number">9</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(x))</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure><p>result</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ <span class="number">1</span> <span class="number">11</span> <span class="number">21</span> <span class="number">31</span> <span class="number">41</span> <span class="number">51</span> <span class="number">61</span> <span class="number">71</span> <span class="number">81</span> <span class="number">91</span>]</span><br><span class="line">[<span class="number">11</span> <span class="number">51</span> <span class="number">91</span>]</span><br></pre></td></tr></table></figure><h2 id="tf-reshape"><a href="#tf-reshape" class="headerlink" title="tf.reshape()"></a>tf.reshape()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.reshape(</span><br><span class="line">    tensor,</span><br><span class="line">    shape,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Given tensor, this operation returns a tensor that has the same values as tensor with shape shape.</p><p>If one component of shape is the special value -1, the size of that dimension is computed so that the total size remains constant. In particular, a shape of [-1] flattens into 1-D. At most one component of shape can be -1.　If shape is 1-D or higher, then the operation returns a tensor with shape shape filled with the values of tensor. In this case, the number of elements implied by shape must be the same as the number of elements in tensor.</p><p>demo</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(np.array(list(range(<span class="number">1</span>, <span class="number">9</span>+<span class="number">1</span>, <span class="number">1</span>))))</span><br><span class="line">b = tf.reshape(a, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># t: a (3 x 2 x 3) tensor</span></span><br><span class="line">t = tf.constant(</span><br><span class="line">    [[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">      [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]],</span><br><span class="line">     [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">      [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]],</span><br><span class="line">     [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">      [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]]</span><br><span class="line">)</span><br><span class="line">c = tf.reshape(t, [<span class="number">2</span>, <span class="number">-1</span>])</span><br><span class="line">d = tf.reshape(t, [<span class="number">-1</span>, <span class="number">9</span>])</span><br><span class="line">e = tf.reshape(t, [<span class="number">2</span>, <span class="number">-1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(b.get_shape().as_list())</span><br><span class="line">    print(sess.run(t), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(t.get_shape().as_list())</span><br><span class="line">    print(sess.run(c), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(c.get_shape().as_list())</span><br><span class="line">    print(sess.run(d), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(d.get_shape().as_list())</span><br><span class="line">    print(sess.run(e), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(e.get_shape().as_list())</span><br></pre></td></tr></table></figure><p>result</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[[ <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line">  [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span>]</span><br><span class="line">  [<span class="number">10</span> <span class="number">11</span> <span class="number">12</span>]]]</span><br><span class="line"></span><br><span class="line">[<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">[[[ <span class="number">1</span>  <span class="number">4</span>]</span><br><span class="line">  [ <span class="number">2</span>  <span class="number">5</span>]</span><br><span class="line">  [ <span class="number">3</span>  <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7</span> <span class="number">10</span>]</span><br><span class="line">  [ <span class="number">8</span> <span class="number">11</span>]</span><br><span class="line">  [ <span class="number">9</span> <span class="number">12</span>]]]</span><br><span class="line"></span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure><h2 id="tf-split"><a href="#tf-split" class="headerlink" title="tf.split()"></a>tf.split()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.split(</span><br><span class="line">    value,</span><br><span class="line">    num_or_size_splits,</span><br><span class="line">    axis=<span class="number">0</span>,</span><br><span class="line">    num=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">&#x27;split&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Splits a tensor into sub tensors.</p><p>If num_or_size_splits is an integer type, num_split, then splits value along dimension axis into num_split smaller tensors. Requires that num_split evenly divides value.shape[axis]. If num_or_size_splits is not an integer type, it is presumed to be a Tensor size_splits, then splits value into len(size_splits) pieces. The shape of the i-th piece has the same size as the value except along dimension axis where the size is size_splits[i].</p><p>demo</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(np.reshape(list(range(<span class="number">1</span>, <span class="number">6</span>+<span class="number">1</span>, <span class="number">1</span>)), (<span class="number">2</span>, <span class="number">3</span>)))</span><br><span class="line">b1, b2, b3 = tf.split(a, <span class="number">3</span>, axis=<span class="number">1</span>)</span><br><span class="line">d1, d2, d3 = tf.split(a, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], axis=<span class="number">1</span>)</span><br><span class="line">c1, c2 = tf.split(a, [<span class="number">1</span>, <span class="number">2</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b1), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b2), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b3), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(d1), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(d2), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(d3), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(c1), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(c2), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">result</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>]</span><br><span class="line"> [<span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span>]</span><br><span class="line"> [<span class="number">5</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">3</span>]</span><br><span class="line"> [<span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>]</span><br><span class="line"> [<span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span>]</span><br><span class="line"> [<span class="number">5</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">3</span>]</span><br><span class="line"> [<span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>]</span><br><span class="line"> [<span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">6</span>]]</span><br></pre></td></tr></table></figure><h2 id="tf-transpose"><a href="#tf-transpose" class="headerlink" title="tf.transpose()"></a>tf.transpose()</h2><p>demo</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">     [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">)</span><br><span class="line">b = tf.transpose(a)</span><br><span class="line"><span class="comment"># equivaltently</span></span><br><span class="line">c = tf.transpose(a, [<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># x : 2 x 2 x 3</span></span><br><span class="line">x = tf.constant(</span><br><span class="line">    [[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">      [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]],</span><br><span class="line">     [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">      [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]]</span><br><span class="line">)</span><br><span class="line"><span class="comment"># perm is more useful for n-dimensional tersors, for n &gt; 2</span></span><br><span class="line"><span class="comment"># y : 2 x 3 x 2</span></span><br><span class="line">y = tf.transpose(x, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(b), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(c), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    print(sess.run(x), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(x.get_shape().as_list(), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(sess.run(y), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br><span class="line">    print(y.get_shape().as_list(), end=<span class="string">&#x27;\n\n&#x27;</span>)</span><br></pre></td></tr></table></figure><p>result</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">[[[ <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line">  [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span>]</span><br><span class="line">  [<span class="number">10</span> <span class="number">11</span> <span class="number">12</span>]]]</span><br><span class="line"></span><br><span class="line">[<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;tf-nn-embedding-lookup&quot;&gt;&lt;a href=&quot;#tf-nn-embedding-lookup&quot; class=&quot;headerlink&quot; title=&quot;tf.nn.embedding_lookup()&quot;&gt;&lt;/a&gt;tf.nn.embeddi</summary>
      
    
    
    
    <category term="Programming &amp; Data" scheme="http://example.com/categories/Programming-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>Graph Neural Network</title>
    <link href="http://example.com/2020/02/03/Graph%20Neural%20Network/"/>
    <id>http://example.com/2020/02/03/Graph%20Neural%20Network/</id>
    <published>2020-02-03T03:56:00.000Z</published>
    <updated>2021-01-08T09:33:15.961Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h2><p>Graph neural network/Graph embedding code.  </p><ul><li>GNN: gcn, graphSAGE</li><li>GE: node2vec</li></ul><h2 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h2><div align="center"><img src="https://s3.ax1x.com/2020/11/15/DFYfqe.png" /></div><p>GCN: learn a function of signals/features on a graph <em>G = (V, E)</em> which takes as input:  </p><ul><li>A feature description <em>x<sub>i</sub></em> for every node i summarized in a <em>N x D</em> feature matrix <em>X</em>. (<em>N</em> : number of nodes, <em>D</em> : number of input features)</li><li>A representative description of the graph structure in matrix form, eg: adjacency matrix <em>A</em>.</li></ul><p>and produces a node-level output <em>Z</em> (an <em>N x F</em> feature matrix, where <em>F</em> is the number of output features per node). Graph-level outputs can be modeled by producing some form of pooling operation.</p><p>Every neural network layer can then be written as a non-linear function, consider the following simple form of a layer-wise propagation rule:  </p><p><img src="http://latex.codecogs.com/png.latex?H%5E%7B(l+1)%7D=f%5Cleft(H%5E%7B(l)%7D,A%5Cright)=%5Csigma%5Cleft(AH%5E%7B(l)%7DW%5E%7B(l)%7D%5Cright)" alt="gnn">  </p><p>with <em>H<sup>(0)</sup>=X</em> and <em>H<sup>(L)</sup>=Z</em> (<em>z</em> for graph-level outputs), <em>L</em> being the number of layers. The specific models then differ only in how <img src="http://latex.codecogs.com/png.latex?f%5Cleft(%5Ccdot,%7B%5Ccdot%7D%5Cright)" alt="f(⋅,⋅)">  is chosen and parameterized.</p><h2 id="Preference"><a href="#Preference" class="headerlink" title="Preference"></a>Preference</h2><ul><li><p>Semi-Supervised Classification with Graph Convolutional Networks<br><a href="https://github.com/tkipf/gcn/">https://github.com/tkipf/gcn/</a> </p></li><li><p>Inductive Representation Learning on Large Graphs<br><a href="https://github.com/williamleif/graphsage-simple">https://github.com/williamleif/graphsage-simple</a></p></li><li><p>node2vec: Scalable Feature Learning for Networks<br><a href="https://github.com/aditya-grover/node2vec">https://github.com/aditya-grover/node2vec</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;GNN&quot;&gt;&lt;a href=&quot;#GNN&quot; class=&quot;headerlink&quot; title=&quot;GNN&quot;&gt;&lt;/a&gt;GNN&lt;/h2&gt;&lt;p&gt;Graph neural network/Graph embedding code.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GNN</summary>
      
    
    
    
    <category term="RecSys &amp; Machine Learning" scheme="http://example.com/categories/RecSys-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>The Illustrated Transformer</title>
    <link href="http://example.com/2020/02/03/The%20Illustrated%20Transformer/"/>
    <id>http://example.com/2020/02/03/The%20Illustrated%20Transformer/</id>
    <published>2020-02-03T03:56:00.000Z</published>
    <updated>2021-01-08T09:33:15.981Z</updated>
    
    <content type="html"><![CDATA[<hr><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>介绍博客链接关于Transformer以及论文《Attention is all you need》以加深对于transformer框架的理解；</li><li>Transformer在时序建模上效果在很多nlp任务上优于RNN，且不同于CNN卷积进行特征提取过程；</li></ul><h2 id="Transformer用于机器翻译任务"><a href="#Transformer用于机器翻译任务" class="headerlink" title="Transformer用于机器翻译任务"></a>Transformer用于机器翻译任务</h2><ul><li><p>Transformer的提出用于机器翻译任务；翻译句子时输入输出是两个句子，中间黑盒包括一个Encoder和一个Decoder，更一般的情况是多个Encoder和多个Decoder进行堆叠；</p></li><li><p>拆解开单个Encoder，其中是一个Self-Attention和一个Feed Forward层；拆解开单个Decoder，其中是一个Self-Attention、一个Enconder-Decoder Attention层和一个Feed Forward层；</p></li></ul><h2 id="Self-Attention-Layer-拆解"><a href="#Self-Attention-Layer-拆解" class="headerlink" title="Self-Attention Layer 拆解"></a>Self-Attention Layer 拆解</h2><ul><li><p>和常见NLP任务一样输入为单词的embedding，分别表示为；经过Self-Attention层后，分别输出为，再分别过各自的FFNN’</p></li><li><p>Attention过程首先对每个单词的embedding构造三个向量分别为Key、Query和Value；这三个向量是通过输入单词embedding分别乘以三个矩阵所得分别表示为，这三个矩阵在训练过程中得到；</p></li></ul><ul><li><p>Self-Attention过程如何实现？分解来看就是理解weight怎么得到，得到weight以后乘上去即可（Transformer里面是乘在value上），即理解如下过程；<br>  输入单词的embedding的query根据key点乘进行打分（维度归一化）</p><p>  =&gt; softmax之后得到一个weight</p><p>  =&gt; 对value乘以weight进行加权求和得到z</p></li></ul><ul><li>Self-Attention的计算过程可以用如下式子表示，其中理解Softmax(x)=weight</li></ul><p>上述过程描述了单个输入单词embedding生成self-attention后的表示的过程，在实际运算时通过矩阵运算来并行实现所有单词的self-attention过程；同时，上述实现了一个head的self-attention的过程，实际上可以设置多个head进行self-attention，再将多个head学习到的表示z进行concatenate，最后将这个宽表示乘以一个weight矩阵；</p><ul><li><p>截止目前Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子中词的顺序怎么打乱Transformer都会得到类似的结果，换句话说此时的Transformer只是一个功能更强大的词袋模型而已。为解决这个问题，Transformer编码词向量时引入了位置编码（Position Embedding）特征。那么如何编码位置信息呢？常见模式有：a. 根据数据学习；b. 自己设计编码规则。</p></li><li><p>另外Transformer在每个Encoder-Block里面，同时采用ResBlock的结构，另外采用了Layer Nomalization的方法进行配合使用；</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;介绍博客链接关于Transformer以及论文《Attention is all you need》以加深对于tran</summary>
      
    
    
    
    <category term="RecSys &amp; Machine Learning" scheme="http://example.com/categories/RecSys-Machine-Learning/"/>
    
    
  </entry>
  
</feed>
